üîù Retour au [Sommaire](/SOMMAIRE.md)

# 21.10 Tests de Failover

## Introduction

Vous avez construit un cluster multi-node hautement disponible avec toutes les protections possibles : redondance, backups, stockage distribu√©, load balancing. Mais **comment savoir si tout cela fonctionne vraiment ?**

C'est l√† qu'interviennent les **tests de failover**. Dans ce chapitre final sur le multi-node, nous allons apprendre √† :
- Valider que votre infrastructure est r√©ellement r√©siliente
- Planifier et ex√©cuter des tests de failover syst√©matiques
- Mesurer les temps de r√©cup√©ration r√©els
- Identifier les faiblesses avant qu'elles ne causent des probl√®mes en production
- Documenter et am√©liorer continuellement votre r√©silience

**Principe fondamental :** Vous ne savez pas si votre syst√®me est r√©silient tant que vous ne l'avez pas cass√© volontairement et vu qu'il se r√©pare tout seul.

**Citation de Netflix :** "The best way to avoid failure is to fail constantly." - Chaos Engineering

## Qu'est-ce qu'un Test de Failover ?

### D√©finition

Un **test de failover** (test de basculement) consiste √† simuler une panne d'un composant et √† v√©rifier que :
1. Le syst√®me d√©tecte la panne
2. Le syst√®me bascule automatiquement vers un composant de secours
3. Le service reste disponible (ou revient rapidement)
4. Les donn√©es restent int√®gres

**Analogie de l'ascenseur :**
Imaginez un immeuble avec 2 ascenseurs. Un test de failover consisterait √† :
- Arr√™ter l'ascenseur 1 volontairement
- V√©rifier que l'ascenseur 2 prend automatiquement le relais
- Mesurer combien de temps les gens doivent attendre
- S'assurer qu'aucun passager n'est bloqu√©

**Dans Kubernetes, c'est pareil :** On casse volontairement un composant et on v√©rifie que tout continue de fonctionner.

### Pourquoi Tester ?

**Raisons critiques :**

**1. Valider les hypoth√®ses**
Vous *pensez* que votre cluster est r√©silient. Les tests le *prouvent*.

**2. D√©couvrir les faiblesses cach√©es**
Souvent, des probl√®mes n'apparaissent que lors de pannes r√©elles. Mieux vaut les d√©couvrir lors d'un test contr√¥l√©.

**3. Mesurer les temps de r√©cup√©ration**
Votre SLA promet un RTO de 5 minutes. Les tests v√©rifient que c'est r√©aliste.

**4. Entra√Æner l'√©quipe**
Les tests pr√©parent votre √©quipe √† g√©rer de vraies crises.

**5. Am√©lioration continue**
Chaque test r√©v√®le des opportunit√©s d'am√©lioration.

**Statistique inqui√©tante :** 60% des entreprises qui d√©clarent avoir une infrastructure HA d√©couvrent lors d'une vraie panne que... √ßa ne fonctionne pas comme pr√©vu.

### Types de Tests

**1. Tests de composants individuels**
- Arr√™t d'un n≈ìud
- Crash d'un pod
- Saturation d'un disque

**2. Tests de cascade**
- Plusieurs composants tombent successivement
- Test de la limite de r√©silience

**3. Tests de charge sous panne**
- Panne pendant une p√©riode de forte charge
- V√©rifie que le failover fonctionne m√™me sous stress

**4. Tests de restauration**
- Apr√®s une panne, v√©rifier que tout revient √† la normale
- Y compris les donn√©es

## Planification des Tests de Failover

### Quand Tester ?

**Fr√©quence recommand√©e :**

| Type de test | Fr√©quence | Dur√©e typique |
|--------------|-----------|---------------|
| Test simple (1 n≈ìud) | Mensuel | 30 min |
| Test complet | Trimestriel | 2-4 heures |
| Test de restauration | Semestriel | 4-8 heures |
| Chaos engineering | Trimestriel | Variable |

**Moments opportuns :**
- Apr√®s l'ajout d'un nouveau n≈ìud
- Apr√®s une mise √† jour majeure
- Avant un d√©ploiement critique en production
- Lors des journ√©es "maintenance planifi√©e"

**√âviter :**
- Pendant les pics de charge
- Juste avant un weekend (si quelque chose casse...)
- Sans avoir pr√©venu l'√©quipe

### Pr√©paration

**Checklist pr√©-test :**

```markdown
# Checklist Test de Failover

## Avant le test
- [ ] Documentation du test pr√©par√©e
- [ ] Backup r√©cent v√©rifi√© (< 24h)
- [ ] √âquipe inform√©e (date/heure du test)
- [ ] Utilisateurs pr√©venus (si impact possible)
- [ ] Monitoring actif et visible
- [ ] Proc√©dure de rollback pr√™te
- [ ] Contact support disponible (si besoin)
- [ ] Fen√™tre de temps suffisante

## Environnement
- [ ] Cluster de test isol√© (recommand√©) OU
- [ ] Production en p√©riode de faible charge
- [ ] Snapshots de l'√©tat initial
- [ ] Logs activ√©s et stock√©s

## Outils
- [ ] Acc√®s SSH √† tous les n≈ìuds
- [ ] kubectl configur√©
- [ ] Scripts de test pr√™ts
- [ ] Grafana/Prometheus accessible
- [ ] Chronom√®tre (pour mesurer les temps)

## Plan B
- [ ] Proc√©dure d'annulation du test d√©finie
- [ ] Backup imm√©diatement accessible
- [ ] Contact escalade disponible
```

### Documentation du Test

**Template de plan de test :**

```markdown
# Plan de Test de Failover - [DATE]

## Objectif
Valider que le cluster tol√®re la panne de 1 n≈ìud worker sans interruption de service.

## P√©rim√®tre
- **Cluster** : Production (ou Test)
- **Composant test√©** : Worker2 (192.168.1.21)
- **Applications impact√©es** : Toutes (redistribution automatique)
- **Dur√©e estim√©e** : 30 minutes

## Hypoth√®se √† Valider
Le cluster redistribue automatiquement les pods de worker2 vers les autres n≈ìuds en moins de 2 minutes, sans perte de donn√©es.

## Proc√©dure
1. Prendre un snapshot de l'√©tat initial
2. Arr√™ter worker2 : `ssh worker2 'sudo shutdown -h now'`
3. Observer la redistribution des pods
4. V√©rifier l'accessibilit√© des applications
5. Red√©marrer worker2
6. V√©rifier le retour √† la normale

## Crit√®res de Succ√®s
- [ ] Aucune interruption de service > 30 secondes
- [ ] Tous les pods red√©marr√©s sur d'autres n≈ìuds
- [ ] Donn√©es int√®gres (v√©rification DB)
- [ ] Worker2 rejoint le cluster en < 5 minutes
- [ ] Aucune alerte critique non r√©solue

## Crit√®res d'√âchec / Rollback
- Interruption de service > 5 minutes ‚Üí Annuler et investiguer
- Perte de donn√©es ‚Üí Restaurer depuis backup
- Cluster instable ‚Üí Annuler le test

## √âquipe
- **Chef de test** : Jean Dupont
- **Observateur** : Marie Martin
- **Support** : Support Team (on-call)

## Communication
- Slack #ops : Notification d√©but/fin de test
- Email : Rapport complet sous 24h
```

## Tests de Failover D√©taill√©s

### Test 1 : Arr√™t d'un N≈ìud Worker

**Objectif :** Valider que la perte d'un worker n'impacte pas le service.

**Pr√©requis :**
- Au moins 2 workers dans le cluster
- Applications avec replicas ‚â• 2

**Proc√©dure :**

**1. Capturer l'√©tat initial**
```bash
# Lister tous les pods et leur n≈ìud
microk8s kubectl get pods --all-namespaces -o wide > /tmp/pods-before.txt

# Capturer les m√©triques
microk8s kubectl top nodes > /tmp/metrics-before.txt

# V√©rifier les applications critiques
curl -I http://myapp.example.com
```

**2. Arr√™ter le n≈ìud worker cibl√©**
```bash
# Sur worker2
sudo shutdown -h now

# Ou depuis le control plane
ssh worker2 'sudo shutdown -h now'
```

**D√©marrer le chronom√®tre ! ‚è±Ô∏è**

**3. Observer le comportement**
```bash
# Surveiller le statut du n≈ìud
watch -n 1 'microk8s kubectl get nodes'

# Observer la redistribution des pods
watch -n 1 'microk8s kubectl get pods --all-namespaces -o wide'
```

**Ce que vous devriez voir :**
```
# Apr√®s 30-60 secondes
NAME      STATUS     ROLES    AGE
node1     Ready      <none>   10d
node2     Ready      <none>   10d
worker1   Ready      <none>   5d
worker2   NotReady   <none>   5d    ‚Üê Marqu√© NotReady

# Apr√®s 1-2 minutes
# Les pods de worker2 passent en "Terminating"
# Puis sont recr√©√©s sur node1, node2 ou worker1
```

**4. V√©rifier l'accessibilit√© des applications**
```bash
# Boucle de test (pendant 5 minutes)
for i in {1..100}; do
  curl -s -o /dev/null -w "%{http_code}\n" http://myapp.example.com
  sleep 3
done
```

**Attendu :** Tous les codes HTTP 200 (ou quelques 503 pendant max 30 secondes)

**5. V√©rifier l'int√©grit√© des donn√©es**
```bash
# Se connecter √† la base de donn√©es
POD=$(microk8s kubectl get pod -l app=postgres -o jsonpath="{.items[0].metadata.name}")
microk8s kubectl exec -it $POD -- psql -U postgres -c "SELECT COUNT(*) FROM users;"

# Comparer avec le nombre avant le test
```

**6. Red√©marrer le n≈ìud**
```bash
# Sur worker2 (apr√®s 5-10 minutes)
# Le n≈ìud red√©marre

# V√©rifier qu'il rejoint le cluster
microk8s kubectl get nodes
```

**7. Capturer l'√©tat final**
```bash
microk8s kubectl get pods --all-namespaces -o wide > /tmp/pods-after.txt
microk8s kubectl top nodes > /tmp/metrics-after.txt
```

**8. Mesurer les temps**
```
T0 : Arr√™t de worker2
T1 : Worker2 marqu√© NotReady (attendre 30-60s)
T2 : Premier pod en Terminating (attendre 30s)
T3 : Premier pod recr√©√© Running (attendre 30-60s)
T4 : Tous les pods recr√©√©s (attendre 1-2 min)

Total : T4 - T0 = 2-4 minutes typiquement
```

**R√©sultats attendus :**
- ‚úÖ Tous les pods redistribu√©s
- ‚úÖ Applications accessibles pendant toute la dur√©e
- ‚úÖ Aucune perte de donn√©es
- ‚úÖ Worker2 rejoint le cluster automatiquement

**Documentation :**
```markdown
# R√©sultat Test Worker Failover - 2025-01-15

## R√©sum√©
‚úÖ SUCC√àS - Failover fonctionnel

## M√©triques
- Temps avant NotReady : 45 secondes
- Temps de redistribution : 2 minutes 10 secondes
- Interruption de service : 0 seconde
- Pods perdus : 0
- Donn√©es perdues : 0

## Observations
- Pod postgres-0 a pris 3 min pour red√©marrer (normal, base volumineuse)
- Aucune alerte critique
- Load balancing a bien fonctionn√©

## Actions
- RAS - Syst√®me conforme aux attentes
```

### Test 2 : Arr√™t du N≈ìud Control Plane Leader

**Objectif :** Valider que la perte du leader d√©clenche une nouvelle √©lection et que le cluster reste fonctionnel.

**Difficult√© :** Avanc√© (impacte le control plane)

**Pr√©requis :**
- Cluster HA avec 3+ n≈ìuds control plane
- Backup r√©cent

**Proc√©dure :**

**1. Identifier le n≈ìud leader**
```bash
# Voir les leases pour trouver le leader
microk8s kubectl get lease -n kube-system -o wide

# Ou v√©rifier les logs Dqlite
sudo journalctl -u snap.microk8s.daemon-k8s-dqlite | grep -i "leader"
```

**2. Arr√™ter le leader**
```bash
# Sur le n≈ìud leader identifi√© (ex: node1)
sudo shutdown -h now
```

**D√©marrer le chronom√®tre ! ‚è±Ô∏è**

**3. Observer l'√©lection**
```bash
# Depuis un autre n≈ìud (node2 ou node3)
microk8s kubectl get nodes

# Essayer des commandes kubectl
microk8s kubectl get pods --all-namespaces
```

**Ce que vous devriez observer :**
- Pendant 5-10 secondes : kubectl peut √™tre lent ou timeout
- Apr√®s 10-15 secondes : une nouvelle √©lection est effectu√©e
- Apr√®s 20-30 secondes : kubectl fonctionne normalement depuis les autres n≈ìuds

**4. V√©rifier le quorum**
```bash
# Sur node2 ou node3
microk8s status
```

```
microk8s is running
high-availability: yes
  datastore master nodes: 192.168.1.11:19001 192.168.1.12:19001
  datastore standby nodes: none
```

Node1 (le leader tomb√©) n'appara√Æt plus, mais quorum maintenu (2/3). ‚úÖ

**5. V√©rifier les applications**
```bash
# Les pods continuent-ils de tourner ?
microk8s kubectl get pods --all-namespaces

# Les applications sont-elles accessibles ?
curl http://myapp.example.com
```

**6. Red√©marrer l'ancien leader**
```bash
# Apr√®s 5-10 minutes, rallumer node1
# Il rejoint automatiquement comme follower
```

**7. V√©rifier le retour √† la normale**
```bash
microk8s status
```

```
high-availability: yes
  datastore master nodes: 192.168.1.10:19001 192.168.1.11:19001 192.168.1.12:19001
```

Les 3 n≈ìuds sont de retour. ‚úÖ

**R√©sultats attendus :**
- ‚úÖ Nouvelle √©lection en < 15 secondes
- ‚úÖ kubectl fonctionne depuis les autres n≈ìuds
- ‚úÖ Applications continuent de tourner
- ‚úÖ Quorum maintenu (2/3)
- ‚úÖ Ancien leader rejoint comme follower

**Temps d'interruption kubectl :** 5-15 secondes (acceptable)
**Temps d'interruption applications :** 0 seconde ‚úÖ

### Test 3 : Perte de Quorum (2 N≈ìuds Control Plane)

**Objectif :** Comprendre ce qui se passe quand le quorum est perdu.

**‚ö†Ô∏è ATTENTION :** Test destructif ! √Ä faire UNIQUEMENT en environnement de test.

**Pr√©requis :**
- Cluster de test isol√© (PAS en production)
- Backup tr√®s r√©cent

**Proc√©dure :**

**1. √âtat initial : 3 n≈ìuds control plane**
```bash
microk8s kubectl get nodes
```

```
NAME    STATUS   ROLES
node1   Ready    <none>
node2   Ready    <none>
node3   Ready    <none>
```

**2. Arr√™ter 2 n≈ìuds simultan√©ment**
```bash
# Sur node1
sudo shutdown -h now &

# Sur node2 (imm√©diatement apr√®s)
sudo shutdown -h now &
```

**3. Observer depuis node3**
```bash
# Node3 d√©tecte la perte de quorum
microk8s status
```

```
microk8s is running
high-availability: no (QUORUM LOST)
  datastore master nodes: 192.168.1.12:19001
```

**4. Tenter des commandes kubectl**
```bash
# Lecture : fonctionne
microk8s kubectl get pods

# √âcriture : √âCHOUE
microk8s kubectl run test --image=nginx
```

```
Error: cluster is read-only (quorum lost)
```

**Le cluster est en mode READ-ONLY.** Les pods existants continuent de tourner, mais impossible de cr√©er/modifier/supprimer des ressources.

**5. Restaurer le quorum**
```bash
# Rallumer node1 OU node2 (1 suffit)
# Apr√®s ~2 minutes, le quorum est restaur√©

microk8s status
```

```
high-availability: yes
  datastore master nodes: 192.168.1.11:19001 192.168.1.12:19001
```

Quorum restaur√© (2/3). ‚úÖ Cluster redevient op√©rationnel.

**Le√ßons apprises :**
- ‚ùå Perte de quorum = cluster en lecture seule
- ‚úÖ Les applications continuent de tourner
- ‚úÖ Restauration possible en rallumant 1 n≈ìud
- üí° **Importance de toujours avoir 3+ n≈ìuds control plane**

### Test 4 : Saturation d'un Disque

**Objectif :** Valider que Kubernetes g√®re correctement un n≈ìud avec disque plein.

**Proc√©dure :**

**1. Cr√©er un gros fichier pour saturer le disque**
```bash
# Sur worker1
df -h /  # Noter l'espace libre

# Cr√©er un fichier pour remplir √† 90%
sudo fallocate -l 10G /tmp/bigfile

df -h /  # V√©rifier
```

**2. Observer le comportement de Kubernetes**
```bash
# Apr√®s quelques minutes, Kubernetes d√©tecte la pression disque
microk8s kubectl describe node worker1
```

```
Conditions:
  Type             Status
  ----             ------
  DiskPressure     True   ‚Üê Kubernetes a d√©tect√© le probl√®me
```

**3. V√©rifier que les nouveaux pods √©vitent ce n≈ìud**
```bash
# Cr√©er un nouveau deployment
microk8s kubectl create deployment test --image=nginx --replicas=5

# V√©rifier o√π les pods sont plac√©s
microk8s kubectl get pods -o wide
```

Les 5 pods devraient √™tre sur node1, node2, node3, worker2 - **AUCUN sur worker1**. ‚úÖ

**4. Nettoyer**
```bash
sudo rm /tmp/bigfile
```

Apr√®s quelques minutes, worker1 repasse en `DiskPressure: False`.

**R√©sultats attendus :**
- ‚úÖ Kubernetes d√©tecte la saturation en < 5 minutes
- ‚úÖ Nouveaux pods ne sont pas schedul√©s sur le n≈ìud plein
- ‚úÖ Pods existants continuent de tourner
- ‚úÖ Alertes d√©clench√©es (Prometheus)

### Test 5 : Crash d'un Pod

**Objectif :** Valider que Kubernetes red√©marre automatiquement les pods crash√©s.

**Proc√©dure :**

**1. Identifier un pod √† crasher**
```bash
microk8s kubectl get pods -l app=web
```

```
NAME                   READY   STATUS
web-7d8f9c5b4-abc123   1/1     Running
```

**2. Tuer le processus principal dans le pod**
```bash
microk8s kubectl exec web-7d8f9c5b4-abc123 -- killall -9 nginx
```

**D√©marrer le chronom√®tre ! ‚è±Ô∏è**

**3. Observer le red√©marrage**
```bash
watch -n 1 'microk8s kubectl get pods -l app=web'
```

```
# Imm√©diatement apr√®s le kill
NAME                   READY   STATUS    RESTARTS
web-7d8f9c5b4-abc123   0/1     Error     0

# Apr√®s quelques secondes
web-7d8f9c5b4-abc123   0/1     CrashLoopBackOff  1

# Puis
web-7d8f9c5b4-abc123   1/1     Running   1
```

**4. Mesurer le temps de red√©marrage**
Typiquement : 10-30 secondes

**5. V√©rifier que les autres replicas ont pris le relais**
```bash
# Si vous avez 3 replicas, les 2 autres ont continu√© √† servir le trafic
curl http://myapp.example.com  # Devrait fonctionner pendant toute la dur√©e
```

**R√©sultats attendus :**
- ‚úÖ Pod red√©marr√© automatiquement
- ‚úÖ RESTARTS incr√©ment√© (visible dans kubectl get pods)
- ‚úÖ Service non interrompu (gr√¢ce aux autres replicas)

### Test 6 : Panne R√©seau (Partition)

**Objectif :** Simuler une panne r√©seau et observer le comportement.

**‚ö†Ô∏è Test avanc√©** - Risque de casser la communication cluster

**Proc√©dure :**

**1. Bloquer le r√©seau sur un n≈ìud**
```bash
# Sur worker2, bloquer tout le trafic (sauf SSH pour garder l'acc√®s)
sudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT
sudo iptables -A OUTPUT -p tcp --sport 22 -j ACCEPT
sudo iptables -A INPUT -j DROP
sudo iptables -A OUTPUT -j DROP
```

**2. Observer depuis les autres n≈ìuds**
```bash
microk8s kubectl get nodes
```

Apr√®s ~1 minute, worker2 passe en `NotReady`.

**3. V√©rifier que les pods sont redistribu√©s**
M√™me comportement que le test 1 (arr√™t de n≈ìud).

**4. Restaurer le r√©seau**
```bash
# Sur worker2
sudo iptables -F  # Flush toutes les r√®gles
```

Worker2 redevient `Ready` et rejoint le cluster.

**R√©sultats attendus :**
- ‚úÖ N≈ìud isol√© marqu√© NotReady
- ‚úÖ Pods redistribu√©s
- ‚úÖ Restauration automatique une fois le r√©seau r√©tabli

### Test 7 : Restauration Compl√®te depuis Backup

**Objectif :** Valider que vous pouvez reconstruire le cluster depuis z√©ro.

**Dur√©e :** 4-8 heures (test complet)

**Proc√©dure :**

**1. Cr√©er un "snapshot" de l'√©tat actuel**
```bash
# Backup complet (vu au chapitre 21.8)
sudo /usr/local/bin/backup-cluster-manual.sh
```

**2. D√©truire le cluster**
```bash
# Sur chaque n≈ìud
microk8s stop
sudo snap remove microk8s --purge
```

**Cluster compl√®tement d√©truit ! üí•**

**3. Reconstruire depuis z√©ro**
```bash
# Sur chaque n≈ìud
sudo snap install microk8s --classic --channel=1.28/stable

# Former le cluster HA
# (voir chapitre 21.4)
```

**4. Restaurer depuis le backup**
```bash
# Copier le backup
scp backup-20250115.tar.gz node1:/tmp/

# Extraire et restaurer
# (voir chapitre 21.8 - section Restauration)
```

**5. Valider la restauration**
```bash
# Toutes les applications sont-elles revenues ?
microk8s kubectl get all --all-namespaces

# Les donn√©es sont-elles int√®gres ?
# V√©rifier la base de donn√©es

# Les Ingress fonctionnent-ils ?
curl http://myapp.example.com
```

**6. Mesurer le temps total de restauration**
```
T0 : D√©but de la reconstruction
T1 : Cluster HA form√© (45 min)
T2 : Addons restaur√©s (15 min)
T3 : Applications restaur√©es (30 min)
T4 : Donn√©es restaur√©es (1-2h)
T5 : Validation compl√®te (30 min)

Total : 3-4 heures
```

**R√©sultats attendus :**
- ‚úÖ Cluster reconstruit fonctionnel
- ‚úÖ Toutes les applications restaur√©es
- ‚úÖ Donn√©es int√®gres
- ‚úÖ RTO respect√© (si < 4h √©tait l'objectif)

**Document les √©carts :**
- Quoi n'a pas √©t√© restaur√© ?
- Quelles difficult√©s rencontr√©es ?
- Comment am√©liorer la proc√©dure ?

## M√©triques de Failover

### Temps de R√©cup√©ration (RTO)

**Recovery Time Objective = Temps maximum acceptable pour restaurer le service**

**Mesures √† prendre :**
```
RTO_n≈ìud = Temps pour que le cluster redistribue les pods apr√®s panne d'un n≈ìud
RTO_pod = Temps pour red√©marrer un pod crash√©
RTO_leader = Temps pour √©lire un nouveau leader Dqlite
RTO_complet = Temps pour restaurer depuis backup complet
```

**Benchmarks typiques pour un cluster MicroK8s 3 n≈ìuds :**
| √âv√©nement | RTO typique | RTO cible |
|-----------|-------------|-----------|
| Crash d'un pod | 10-30s | < 1min |
| Panne d'un worker | 1-2 min | < 3min |
| Panne leader Dqlite | 10-20s | < 30s |
| Saturation disque (d√©tection) | 3-5 min | < 5min |
| Restauration depuis backup | 3-4h | < 4h |

### Perte de Donn√©es (RPO)

**Recovery Point Objective = Perte de donn√©es maximale acceptable**

**Mesures √† prendre :**
```
RPO = Fr√©quence des backups

Exemples :
- Backup quotidien ‚Üí RPO = 24h
- Backup horaire ‚Üí RPO = 1h
- R√©plication synchrone ‚Üí RPO = 0 (aucune perte)
```

**Pour Longhorn avec 3 replicas :** RPO = 0 (r√©plication synchrone)
**Pour backups Velero :** RPO = fr√©quence du backup (ex: 1h)

### Taux de Disponibilit√© (Uptime)

**Formule :**
```
Uptime (%) = ((Temps total - Downtime) / Temps total) √ó 100
```

**Calcul apr√®s tests :**
```
P√©riode de test : 1 mois (720 heures)
Downtime cumul√© :
- Test failover worker : 0s (service maintenu)
- Test failover leader : 15s
- Panne impr√©vue : 5 minutes = 300s
- Maintenance : 10 minutes = 600s

Total downtime : 915 secondes = 15.25 minutes

Uptime = ((720√ó60 - 15.25) / (720√ó60)) √ó 100 = 99.96%
```

**SLA standards :**
- 99% = 7h20min/mois
- 99.9% = 43min/mois
- **99.95% = 21min/mois** ‚Üê Votre r√©sultat
- 99.99% = 4min/mois

### Tableau de Bord des Tests

**Maintenir un suivi :**

```markdown
# Historique Tests de Failover - Q1 2025

| Date | Type | Dur√©e | Downtime | Succ√®s | Notes |
|------|------|-------|----------|--------|-------|
| 2025-01-15 | Worker failover | 30min | 0s | ‚úÖ | RAS |
| 2025-02-10 | Leader failover | 45min | 12s | ‚úÖ | L√©g√®re latence kubectl |
| 2025-03-05 | Disk saturation | 1h | 0s | ‚úÖ | Alerte bien d√©clench√©e |
| 2025-03-20 | Restauration compl√®te | 4h | N/A | ‚úÖ | Proc√©dure am√©lior√©e |

## Statistiques Q1
- Tests r√©alis√©s : 4
- Succ√®s : 4 (100%)
- Downtime cumul√© : 12s
- RTO moyen : 2min 30s
- Uptime Q1 : 99.98%
```

## Automatisation des Tests

### Scripts de Test

**Script de test automatique :**

```bash
#!/bin/bash
# test-failover-auto.sh

# Configuration
TARGET_NODE="worker2"
TEST_NAME="Worker Failover Test"
TIMESTAMP=$(date +%Y%m%d-%H%M%S)
REPORT_FILE="/tmp/failover-test-$TIMESTAMP.txt"

echo "=== $TEST_NAME - $TIMESTAMP ===" | tee $REPORT_FILE

# 1. √âtat initial
echo "√âtat initial :" | tee -a $REPORT_FILE
microk8s kubectl get nodes | tee -a $REPORT_FILE
microk8s kubectl get pods --all-namespaces -o wide > /tmp/pods-before-$TIMESTAMP.txt

# 2. V√©rifier l'accessibilit√© avant
echo "Test accessibilit√© avant panne..." | tee -a $REPORT_FILE
HTTP_BEFORE=$(curl -s -o /dev/null -w "%{http_code}" http://myapp.example.com)
echo "HTTP Status avant : $HTTP_BEFORE" | tee -a $REPORT_FILE

# 3. Arr√™ter le n≈ìud
echo "Arr√™t de $TARGET_NODE..." | tee -a $REPORT_FILE
START_TIME=$(date +%s)
ssh $TARGET_NODE 'sudo shutdown -h now' &

# 4. Surveiller la redistribution
echo "Surveillance de la redistribution..." | tee -a $REPORT_FILE
sleep 30  # Attendre que le n≈ìud soit NotReady

# Attendre que tous les pods soient Running
TIMEOUT=300  # 5 minutes max
ELAPSED=0
while [ $ELAPSED -lt $TIMEOUT ]; do
  PENDING=$(microk8s kubectl get pods --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded --no-headers | wc -l)

  if [ $PENDING -eq 0 ]; then
    echo "Tous les pods sont Running" | tee -a $REPORT_FILE
    break
  fi

  echo "Pods en attente : $PENDING" | tee -a $REPORT_FILE
  sleep 10
  ELAPSED=$((ELAPSED + 10))
done

END_TIME=$(date +%s)
RECOVERY_TIME=$((END_TIME - START_TIME))

echo "Temps de r√©cup√©ration : ${RECOVERY_TIME}s" | tee -a $REPORT_FILE

# 5. V√©rifier l'accessibilit√© apr√®s
echo "Test accessibilit√© apr√®s redistribution..." | tee -a $REPORT_FILE
HTTP_AFTER=$(curl -s -o /dev/null -w "%{http_code}" http://myapp.example.com)
echo "HTTP Status apr√®s : $HTTP_AFTER" | tee -a $REPORT_FILE

# 6. Comparer avant/apr√®s
microk8s kubectl get pods --all-namespaces -o wide > /tmp/pods-after-$TIMESTAMP.txt

# 7. G√©n√©rer le rapport
echo "" | tee -a $REPORT_FILE
echo "=== R√âSULTAT ===" | tee -a $REPORT_FILE

if [ "$HTTP_AFTER" == "200" ] && [ $RECOVERY_TIME -lt 180 ]; then
  echo "‚úÖ TEST R√âUSSI" | tee -a $REPORT_FILE
  echo "- Service accessible" | tee -a $REPORT_FILE
  echo "- R√©cup√©ration en ${RECOVERY_TIME}s (< 3min)" | tee -a $REPORT_FILE
else
  echo "‚ùå TEST √âCHOU√â" | tee -a $REPORT_FILE
  [ "$HTTP_AFTER" != "200" ] && echo "- Service inaccessible" | tee -a $REPORT_FILE
  [ $RECOVERY_TIME -ge 180 ] && echo "- R√©cup√©ration trop lente (${RECOVERY_TIME}s)" | tee -a $REPORT_FILE
fi

echo "" | tee -a $REPORT_FILE
echo "Rapport complet : $REPORT_FILE"

# 8. Envoyer notification
# curl -X POST https://hooks.slack.com/... -d "Test failover : voir $REPORT_FILE"
```

**Rendre ex√©cutable et tester :**
```bash
chmod +x test-failover-auto.sh
./test-failover-auto.sh
```

### Int√©gration CI/CD

**GitLab CI / GitHub Actions pour tests mensuels :**

```yaml
# .gitlab-ci.yml
failover-test:
  stage: test
  only:
    - schedules  # Ex√©cuter uniquement sur schedule mensuel
  script:
    - ./scripts/test-failover-auto.sh
  artifacts:
    paths:
      - /tmp/failover-test-*.txt
    expire_in: 3 months
  allow_failure: false  # Fail le pipeline si test √©choue
```

**Schedule mensuel dans GitLab :**
Settings ‚Üí CI/CD ‚Üí Schedules ‚Üí New Schedule (1er de chaque mois)

### Chaos Engineering avec Chaos Mesh

**Installation de Chaos Mesh :**

```bash
# Ajouter le repo Helm
microk8s helm3 repo add chaos-mesh https://charts.chaos-mesh.org

# Installer
microk8s helm3 install chaos-mesh chaos-mesh/chaos-mesh \
  --namespace chaos-mesh \
  --create-namespace \
  --set chaosDaemon.runtime=containerd \
  --set chaosDaemon.socketPath=/var/snap/microk8s/common/run/containerd.sock
```

**Exemple de Chaos Experiment - Tuer un pod al√©atoire :**

```yaml
# pod-kill-experiment.yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: pod-kill-test
  namespace: default
spec:
  action: pod-kill
  mode: one  # Tuer 1 pod al√©atoire
  selector:
    namespaces:
      - production
    labelSelectors:
      app: web
  scheduler:
    cron: '@every 1h'  # Toutes les heures
```

```bash
microk8s kubectl apply -f pod-kill-experiment.yaml
```

**Chaos Mesh tue automatiquement un pod web toutes les heures** ‚Üí Force votre syst√®me √† √™tre r√©silient !

## Am√©lioration Continue

### Analyse Post-Test

**Questions √† se poser apr√®s chaque test :**

1. **Le test s'est-il pass√© comme pr√©vu ?**
   - Oui ‚Üí Syst√®me conforme
   - Non ‚Üí Investiguer les √©carts

2. **Les temps mesur√©s sont-ils acceptables ?**
   - RTO < objectif ‚Üí ‚úÖ
   - RTO > objectif ‚Üí Identifier les goulots

3. **Y a-t-il eu des surprises ?**
   - Comportements inattendus ?
   - Erreurs non anticip√©es ?

4. **Les alertes ont-elles fonctionn√© ?**
   - Alertes d√©clench√©es ?
   - Notifications re√ßues ?
   - Faux positifs ?

5. **La documentation √©tait-elle √† jour ?**
   - Proc√©dures correctes ?
   - Runbooks utiles ?

6. **L'√©quipe √©tait-elle pr√™te ?**
   - Communication efficace ?
   - R√¥les clairs ?

### Boucle d'Am√©lioration

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       CYCLE D'AM√âLIORATION CONTINUE         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. PLANIFIER
   ‚îî‚îÄ‚ñ∫ D√©finir objectifs du test
       Pr√©parer proc√©dures

2. EX√âCUTER
   ‚îî‚îÄ‚ñ∫ Lancer le test
       Collecter m√©triques

3. ANALYSER
   ‚îî‚îÄ‚ñ∫ Comparer r√©sultats vs objectifs
       Identifier √©carts

4. AM√âLIORER
   ‚îî‚îÄ‚ñ∫ Corriger faiblesses
       Mettre √† jour proc√©dures
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ Retour √† 1 ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Actions d'Am√©lioration Typiques

**Suite √† des tests :**

**Probl√®me d√©tect√© :** RTO de 5 minutes (objectif 3 min)
**Action :** Augmenter le nombre de replicas de 2 √† 3
**R√©sultat :** RTO r√©duit √† 2 minutes ‚úÖ

**Probl√®me d√©tect√© :** Alerte non d√©clench√©e lors saturation disque
**Action :** Configurer alerte Prometheus √† 80% (au lieu de 90%)
**R√©sultat :** Alerte pr√©coce ‚úÖ

**Probl√®me d√©tect√© :** Proc√©dure de restauration incompl√®te
**Action :** Mettre √† jour documentation avec √©tapes manquantes
**R√©sultat :** Restauration plus fluide au prochain test ‚úÖ

### Partage de Connaissances

**Post-mortem blameless :**
Apr√®s chaque test (surtout les √©checs), organiser une r√©union pour :
- D√©briefer ce qui s'est pass√©
- Identifier les am√©liorations
- Partager les apprentissages
- **SANS chercher de coupable** (culture blameless)

**Documentation vivante :**
- Wiki interne avec retours d'exp√©rience
- Runbooks mis √† jour apr√®s chaque test
- Vid√©os de tests pour formation

## Checklist Finale de R√©silience

**Avant de consid√©rer votre cluster "production-ready" :**

```markdown
# Checklist R√©silience Cluster Kubernetes

## Infrastructure
- [ ] 3+ n≈ìuds control plane
- [ ] N≈ìuds r√©partis sur diff√©rentes zones (racks, alim, etc.)
- [ ] UPS install√©
- [ ] Network bonding configur√©

## Stockage
- [ ] Longhorn avec 3 replicas minimum
- [ ] Backups automatiques quotidiens
- [ ] Backups test√©s (restauration r√©ussie)
- [ ] Snapshots configur√©s

## Applications
- [ ] Toutes apps critiques avec replicas ‚â• 2
- [ ] PodDisruptionBudgets configur√©s
- [ ] Anti-affinity sur apps critiques
- [ ] Health checks (liveness + readiness) partout

## Monitoring
- [ ] Prometheus + Grafana actifs
- [ ] Alertes critiques configur√©es
- [ ] Slack/Email notifications fonctionnelles
- [ ] Dashboards de r√©silience cr√©√©s

## Tests
- [ ] Test worker failover : ‚úÖ SUCC√àS
- [ ] Test leader failover : ‚úÖ SUCC√àS
- [ ] Test saturation disque : ‚úÖ SUCC√àS
- [ ] Test restauration backup : ‚úÖ SUCC√àS
- [ ] Tests mensuels programm√©s
- [ ] Documentation √† jour

## Proc√©dures
- [ ] Runbooks √† jour
- [ ] Plan de reprise d'activit√© (DRP) √©crit
- [ ] Contacts d'escalade d√©finis
- [ ] √âquipe form√©e

## M√©triques
- [ ] RTO mesur√© et document√©
- [ ] RPO d√©fini et respect√©
- [ ] Uptime > 99.9% sur 3 mois
- [ ] Historique des tests maintenu

## R√©sultat
- [ ] ‚úÖ Cluster pr√™t pour la production
- [ ] ‚ö†Ô∏è Am√©lioration n√©cessaire (d√©tails: ___)
- [ ] ‚ùå Non pr√™t (actions: ___)
```

## Points Cl√©s √† Retenir

1. **Tester = Valider** : Vous ne savez pas si √ßa marche tant que vous n'avez pas test√©
2. **Fr√©quence** : Tests mensuels minimum (plus au d√©but)
3. **Documentation** : Tout doit √™tre document√© (avant, pendant, apr√®s)
4. **M√©triques** : RTO, RPO, Uptime doivent √™tre mesur√©s
5. **Automatisation** : Scripts pour r√©p√©ter les tests facilement
6. **Am√©lioration continue** : Chaque test = opportunit√© d'am√©liorer
7. **Culture blameless** : Chercher √† comprendre, pas √† bl√¢mer
8. **Tests vari√©s** : N≈ìuds, pods, disques, r√©seau, restauration
9. **Environnement de test** : Id√©alement isol√© de la production
10. **Chaos engineering** : Passer au niveau sup√©rieur avec des pannes al√©atoires

## Conclusion du Chapitre 21

**F√©licitations !** Vous avez compl√©t√© le chapitre 21 sur le multi-node et la haute disponibilit√©. Vous avez appris √† :

- Comprendre l'architecture multi-node (21.1)
- Ma√Ætriser Dqlite et la HA (21.2)
- Ajouter des n≈ìuds (21.3)
- Configurer un cluster HA complet (21.4)
- Ajouter des workers (21.5)
- Impl√©menter le load balancing (21.6)
- G√©rer le stockage distribu√© (21.7)
- Sauvegarder le control plane (21.8)
- Concevoir des strat√©gies de redondance (21.9)
- **Tester et valider la r√©silience (21.10)** ‚úÖ

Votre cluster est maintenant :
- **Hautement disponible** (tol√®re les pannes)
- **R√©silient** (r√©cup√©ration automatique)
- **√âvolutif** (ajout de n≈ìuds facile)
- **Sauvegard√©** (protection contre les catastrophes)
- **Test√©** (vous SAVEZ qu'il fonctionne)

**Vous √™tes maintenant capable de g√©rer un cluster Kubernetes de qualit√© production !** üéâüöÄ

---

**Bravo !** Vous ma√Ætrisez maintenant les tests de failover et la validation de r√©silience. Vous savez non seulement construire un cluster r√©silient, mais aussi prouver qu'il l'est vraiment !

‚è≠Ô∏è [Sauvegarde et Restauration](/22-sauvegarde-et-restauration/README.md)
