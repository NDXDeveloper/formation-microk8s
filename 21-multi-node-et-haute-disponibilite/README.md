ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 21. Multi-Node et Haute DisponibilitÃ©

## Introduction au Chapitre

Jusqu'Ã  prÃ©sent, vous avez explorÃ© Kubernetes sur un **nÅ“ud unique** - une seule machine qui fait tourner tous les composants de votre cluster. C'est parfait pour apprendre, expÃ©rimenter et dÃ©velopper. Mais que se passe-t-il si :

- Votre serveur tombe en panne ? ğŸ’¥
- Vous devez faire une mise Ã  jour systÃ¨me ? ğŸ”„
- Vous manquez de ressources (CPU, RAM) ? ğŸ“ˆ
- Un disque dur lÃ¢che ? ğŸ’¾
- Une application gourmande monopolise tout ? ğŸ·

Sur un cluster mono-node, **la rÃ©ponse est la mÃªme : tout s'arrÃªte**. Vos applications deviennent inaccessibles. Vos utilisateurs voient des pages d'erreur. Votre business est impactÃ©.

C'est lÃ  qu'intervient le **multi-node** et la **haute disponibilitÃ©** (HA - High Availability).

## Qu'est-ce que le Multi-Node ?

### DÃ©finition Simple

Un cluster **multi-node** est composÃ© de **plusieurs machines** (physiques ou virtuelles) qui travaillent ensemble comme un seul systÃ¨me unifiÃ©.

**Analogie du restaurant :**

**Restaurant mono-node :**
```
ğŸ  Restaurant avec 1 seul cuisinier
   â†“
   Si le cuisinier est malade â†’ Restaurant fermÃ© âŒ
```

**Restaurant multi-node :**
```
ğŸ  Restaurant avec 3 cuisiniers
   â†“
   Si un cuisinier est malade â†’ Les 2 autres continuent âœ…
   Restaurant ouvert !
```

**Dans Kubernetes, c'est pareil :**
- Plusieurs machines (nÅ“uds) = Plusieurs cuisiniers
- Si une machine tombe = Les autres continuent
- Vos applications restent accessibles âœ…

### Architecture Visuelle

**Cluster Single-Node (actuel) :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        UNE SEULE MACHINE   â”‚
â”‚                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Control Plane      â”‚  â”‚
â”‚  â”‚   (Chef orchestre)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Worker             â”‚  â”‚
â”‚  â”‚   (Vos applications) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Stockage           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    âš ï¸ Point unique de dÃ©faillance
```

**Cluster Multi-Node (objectif) :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Node 1     â”‚  â”‚   Node 2     â”‚  â”‚   Node 3     â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Control  â”‚â—„â”¼â”€â”€â”¼â–ºâ”‚ Control  â”‚â—„â”¼â”€â”€â”¼â–ºâ”‚ Control  â”‚ â”‚
â”‚ â”‚ Plane    â”‚ â”‚  â”‚ â”‚ Plane    â”‚ â”‚  â”‚ â”‚ Plane    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Worker   â”‚ â”‚  â”‚ â”‚ Worker   â”‚ â”‚  â”‚ â”‚ Worker   â”‚ â”‚
â”‚ â”‚ App A    â”‚ â”‚  â”‚ â”‚ App B    â”‚ â”‚  â”‚ â”‚ App A    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         âœ… RÃ©silience et redondance
```

**Avantages immÃ©diats :**
- ğŸ›¡ï¸ **RÃ©silience** : Une machine tombe ? Le cluster continue
- ğŸ“Š **Performance** : RÃ©partir la charge sur plusieurs machines
- ğŸ”§ **Maintenance** : Mise Ã  jour d'un nÅ“ud sans downtime
- ğŸ“ˆ **ScalabilitÃ©** : Ajouter des nÅ“uds = plus de capacitÃ©

## Qu'est-ce que la Haute DisponibilitÃ© (HA) ?

### DÃ©finition

La **Haute DisponibilitÃ©** (High Availability - HA) est la capacitÃ© d'un systÃ¨me Ã  rester **opÃ©rationnel et accessible** mÃªme en cas de panne d'un ou plusieurs composants.

**Objectif chiffrÃ© :**
- 99% = 7h20min d'indisponibilitÃ© par mois (pas terrible)
- 99.9% = 43 minutes par mois (bien)
- **99.95% = 21 minutes par mois (trÃ¨s bien)**
- 99.99% = 4 minutes par mois (excellent)
- 99.999% = 26 secondes par mois (exceptionnel)

### Les Trois Piliers de la HA

**1. Redondance**
Avoir plusieurs exemplaires de chaque composant critique.

```
Component unique âŒ     Composants redondants âœ…
     â”‚                        â”‚    â”‚    â”‚
     Pod A                  Pod A Pod A Pod A
     â”‚                        â”‚    â”‚    â”‚
     â””â”€ Tombe â†’ Service HS   Un tombe â†’ 2 autres continuent
```

**2. RÃ©plication**
Copier les donnÃ©es sur plusieurs machines.

```
Stockage unique âŒ              Stockage rÃ©pliquÃ© âœ…
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ DonnÃ©es  â”‚                  â”‚Data  â”‚ â”‚Data  â”‚ â”‚Data  â”‚
â”‚ Volume   â”‚                  â”‚Copy 1â”‚ â”‚Copy 2â”‚ â”‚Copy 3â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
Disque HS â†’ Tout perdu        1 disque HS â†’ 2 copies restent
```

**3. Failover Automatique**
Basculement automatique vers un composant de secours en cas de panne.

```
         Node 1                Node 2
         (Actif)               (Standby)
            â”‚                     â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚   Heartbeat         â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚                     â”‚
         Tombe ! âŒ               â”‚
                                  â”‚
                          Prend le relais âœ…
                          (nouveau actif)
```

### HA vs Backup : Quelle DiffÃ©rence ?

Beaucoup de dÃ©butants confondent **HA** et **Backup**. Voici la diffÃ©rence :

**Haute DisponibilitÃ© (HA) :**
- **Objectif** : Ã‰viter les interruptions de service
- **Protection contre** : Pannes matÃ©rielles, crashes, surcharges
- **Temps de rÃ©cupÃ©ration** : Secondes Ã  minutes
- **Exemple** : Node1 tombe â†’ Pods redistribuÃ©s automatiquement sur Node2

**Backup (Sauvegarde) :**
- **Objectif** : ProtÃ©ger contre la perte de donnÃ©es
- **Protection contre** : Suppressions accidentelles, corruptions, catastrophes
- **Temps de rÃ©cupÃ©ration** : Minutes Ã  heures
- **Exemple** : Suppression accidentelle de la base â†’ Restauration depuis backup

**Les deux sont complÃ©mentaires !**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         STRATÃ‰GIE COMPLÃˆTE                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                           â”‚
â”‚  HA (Haute DisponibilitÃ©)                 â”‚
â”‚  â””â”€â–º Cluster multi-node                   â”‚
â”‚      RÃ©plication des pods                 â”‚
â”‚      Stockage distribuÃ©                   â”‚
â”‚      â†“                                    â”‚
â”‚      ProtÃ¨ge contre : Pannes courantes    â”‚
â”‚      Downtime : ~0 secondes               â”‚
â”‚                                           â”‚
â”‚              +                            â”‚
â”‚                                           â”‚
â”‚  Backup (Sauvegardes)                     â”‚
â”‚  â””â”€â–º Backups quotidiens                   â”‚
â”‚      Snapshots                            â”‚
â”‚      RÃ©plication hors-site                â”‚
â”‚      â†“                                    â”‚
â”‚      ProtÃ¨ge contre : Catastrophes        â”‚
â”‚      Downtime : Minutes Ã  heures          â”‚
â”‚                                           â”‚
â”‚  = Cluster de production robuste âœ…       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Pourquoi Passer au Multi-Node ?

### ScÃ©narios OÃ¹ le Single-Node Pose ProblÃ¨me

**ScÃ©nario 1 : Panne MatÃ©rielle**
```
ğŸ“… Vendredi 23h - Votre serveur unique
ğŸ”¥ La carte mÃ¨re grille

ConsÃ©quence avec single-node :
- Tous les services down âŒ
- Impossible de dÃ©ployer des correctifs
- Vous devez commander une nouvelle carte, attendre la livraison...
- Downtime : Plusieurs jours ğŸ˜±

ConsÃ©quence avec multi-node :
- Node1 tombe, mais Node2 et Node3 continuent âœ…
- Les pods sont automatiquement redistribuÃ©s
- Downtime : 0 seconde
- Vous rÃ©parez tranquillement Node1 plus tard
```

**ScÃ©nario 2 : Maintenance SystÃ¨me**
```
ğŸ”§ Vous devez installer des mises Ã  jour de sÃ©curitÃ© urgentes
ğŸ“¦ NÃ©cessite un reboot du serveur

Avec single-node :
- Planifier un crÃ©neau de maintenance (ex: 2h-4h du matin)
- PrÃ©venir les utilisateurs
- Stopper toutes les applications
- Rebooter
- EspÃ©rer que tout redÃ©marre correctement
- Downtime : 15-30 minutes

Avec multi-node :
- Drain Node1 (Ã©vacuer les pods vers Node2/Node3)
- Mettre Ã  jour et rebooter Node1
- Faire pareil avec Node2, puis Node3
- Downtime : 0 seconde âœ…
```

**ScÃ©nario 3 : Charge Croissante**
```
ğŸ“ˆ Votre application devient populaire
ğŸš€ Le trafic augmente de 300%

Avec single-node :
- CPU/RAM saturÃ©s
- Applications ralentissent ou crashent
- Impossible d'ajouter plus de ressources (limite physique)
- Solution : Acheter un serveur plus gros (coÃ»teux, downtime)

Avec multi-node :
- Ajouter simplement un nouveau nÅ“ud (Node4)
- Kubernetes rÃ©partit automatiquement la charge
- Scaling horizontal (+ de machines) vs vertical (machine + grosse)
- CoÃ»t optimisÃ©, pas de downtime âœ…
```

**ScÃ©nario 4 : DÃ©ploiement RatÃ©**
```
ğŸš¢ Vous dÃ©ployez une nouvelle version de l'app
ğŸ› Bug critique dÃ©couvert en production

Avec single-node :
- Le mauvais pod crashe et redÃ©marre en boucle
- Pendant ce temps, service inaccessible
- Stress maximal pour faire un rollback rapide

Avec multi-node + rÃ©plication :
- Vous avez 3 pods de l'ancienne version qui tournent
- Vous dÃ©ployez 3 pods de la nouvelle version
- Si la nouvelle version crashe â†’ L'ancienne continue de servir
- Rolling update progressif et sÃ»r âœ…
```

### Pour Qui le Multi-Node ?

**Vous DEVRIEZ passer au multi-node si :**
- âœ… Vous prÃ©parez une application pour la production
- âœ… Vous avez des utilisateurs qui comptent sur votre service
- âœ… Votre business dÃ©pend de la disponibilitÃ© (e-commerce, SaaS, API)
- âœ… Vous voulez pouvoir faire des maintenances sans downtime
- âœ… Vous voulez apprendre les vraies pratiques DevOps/SRE
- âœ… Vous avez 2-3 machines (ou plus) disponibles

**Le single-node reste OK pour :**
- âœ… Apprentissage et expÃ©rimentation
- âœ… Environnement de dÃ©veloppement personnel
- âœ… Proof of concept (POC)
- âœ… Homelab sans enjeu de disponibilitÃ©
- âœ… Budget limitÃ© (une seule machine)

## Ce Que Vous Allez Apprendre

Ce chapitre 21 est structurÃ© en **10 sections progressives** qui vous guideront de votre cluster single-node actuel vers un cluster multi-node hautement disponible, rÃ©silient et production-ready.

### Vue d'Ensemble du Chapitre

**Phase 1 : Comprendre (Sections 21.1 - 21.2)**
```
21.1 Architecture Multi-Node
     â””â”€â–º Comprendre les rÃ´les (control plane, worker, etcd)
         SchÃ©mas d'architecture
         DiffÃ©rence avec single-node

21.2 Haute DisponibilitÃ© avec Dqlite
     â””â”€â–º DÃ©couvrir Dqlite (alternative Ã  etcd)
         Consensus et quorum
         Comment MicroK8s gÃ¨re la HA
```

**Phase 2 : Construire (Sections 21.3 - 21.5)**
```
21.3 Ajout de NÅ“uds Ã  un Cluster MicroK8s
     â””â”€â–º PrÃ©parer les machines
         Commandes join/add-node
         VÃ©rifications

21.4 Configuration d'un Cluster HA
     â””â”€â–º Former un cluster 3 nÅ“uds control plane
         Tests de quorum
         Validation HA

21.5 Ajout de Workers SupplÃ©mentaires
     â””â”€â–º SÃ©parer control plane et workers
         Scaling horizontal
         Gestion des labels
```

**Phase 3 : Optimiser (Sections 21.6 - 21.7)**
```
21.6 Load Balancing Entre NÅ“uds
     â””â”€â–º MetalLB pour IP externes
         HAProxy pour API server
         RÃ©partition de charge

21.7 Stockage DistribuÃ©
     â””â”€â–º Longhorn pour volumes persistants rÃ©pliquÃ©s
         Snapshots et backups
         RÃ©silience des donnÃ©es
```

**Phase 4 : SÃ©curiser (Sections 21.8 - 21.10)**
```
21.8 Backup du Control Plane
     â””â”€â–º Sauvegarder Dqlite et l'Ã©tat du cluster
         Velero pour backups automatiques
         ProcÃ©dures de restauration

21.9 StratÃ©gies de Redondance
     â””â”€â–º Redondance Ã  tous les niveaux
         Ã‰liminer les SPOF (Single Points of Failure)
         Matrice de rÃ©silience

21.10 Tests de Failover
      â””â”€â–º Valider que tout fonctionne rÃ©ellement
          Chaos engineering
          Mesurer RTO et RPO
```

### Progression PÃ©dagogique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            VOTRE PARCOURS D'APPRENTISSAGE      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Vous Ãªtes ici                                 â”‚
â”‚       â†“                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚ Single-Node â”‚  Cluster 1 machine            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚         â”‚                                      â”‚
â”‚  (21.1 - 21.2) Apprendre les concepts          â”‚
â”‚         â”‚                                      â”‚
â”‚         â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚ Multi-Node  â”‚  Cluster 3 machines           â”‚
â”‚  â”‚ (basique)   â”‚  sans HA                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚         â”‚                                      â”‚
â”‚  (21.3 - 21.4) Former le cluster HA            â”‚
â”‚         â”‚                                      â”‚
â”‚         â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚ Cluster HA  â”‚  3 control plane              â”‚
â”‚  â”‚ (production)â”‚  + workers                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚         â”‚                                      â”‚
â”‚  (21.5 - 21.7) Optimiser et sÃ©curiser          â”‚
â”‚         â”‚                                      â”‚
â”‚         â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚ Cluster     â”‚  Stockage distribuÃ©           â”‚
â”‚  â”‚ Production  â”‚  Load balancing               â”‚
â”‚  â”‚ Complet     â”‚  Backups auto                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚         â”‚                                      â”‚
â”‚  (21.8 - 21.10) Valider et tester              â”‚
â”‚         â”‚                                      â”‚
â”‚         â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚ Cluster     â”‚  RÃ©silient, testÃ©             â”‚
â”‚  â”‚ Production  â”‚  et documentÃ© âœ…              â”‚
â”‚  â”‚ Ready       â”‚                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## PrÃ©requis MatÃ©riels

Pour suivre ce chapitre, vous aurez besoin de **plusieurs machines**. Voici les configurations recommandÃ©es selon votre situation :

### Configuration Minimale (Apprentissage)

**Pour tester les concepts :**
- 3 machines virtuelles (VMs)
- 2 GB RAM par VM
- 2 vCPU par VM
- 20 GB disque par VM
- RÃ©seau local entre les VMs

**Exemples d'environnements :**
- VirtualBox / VMware sur votre PC
- Proxmox (hyperviseur open-source)
- Cloud : AWS EC2 t3.micro, Google Cloud e2-micro (attention aux coÃ»ts)

### Configuration Standard (Homelab)

**Pour un vrai cluster fonctionnel :**
- 3 mini-PCs ou Raspberry Pi 4
- 4 GB RAM par machine (8 GB recommandÃ©)
- 4 cores CPU par machine
- 50-100 GB SSD par machine
- Switch gigabit
- Alimentations et cÃ¢bles

**Budget estimÃ© :** 600-900â‚¬ pour 3 mini-PCs d'occasion

### Configuration Production (Petite Entreprise)

**Pour un usage professionnel :**
- 5+ serveurs (3 control plane + 2+ workers)
- 16-32 GB RAM par serveur
- 8+ cores CPU par serveur
- SSD NVMe 250 GB+ par serveur
- Switch manageable 10GbE
- UPS (onduleur)
- Redondance rÃ©seau/alimentation

**Budget estimÃ© :** 3000-8000â‚¬

### Ce Que Vous Pouvez RÃ©utiliser

**MatÃ©riel que vous avez peut-Ãªtre dÃ©jÃ  :**
- âœ… Vieux laptops (retirez la batterie si gonflÃ©e)
- âœ… Mini-PCs type Intel NUC
- âœ… Raspberry Pi (3B+, 4, 5)
- âœ… Anciens serveurs
- âœ… Desktop PC avec VMs

**Astuce budget :** Commencez avec 3 machines modestes pour apprendre, puis investissez dans du matÃ©riel plus sÃ©rieux quand vous maÃ®trisez.

## PrÃ©requis Logiciels et Connaissances

### Connaissances Requises

Avant de commencer ce chapitre, vous devriez Ãªtre Ã  l'aise avec :
- âœ… Utilisation de base de Kubernetes (Pods, Deployments, Services)
- âœ… Commandes kubectl
- âœ… Linux et ligne de commande (SSH, apt, systemctl)
- âœ… Concepts rÃ©seau de base (IP, ports, DNS)
- âœ… MicroK8s sur single-node (chapitres prÃ©cÃ©dents)

**Si vous Ãªtes bloquÃ© sur ces concepts, revoyez les chapitres prÃ©cÃ©dents du tutoriel !**

### Logiciels Ã  PrÃ©parer

**Sur chaque machine du cluster :**
- Ubuntu 22.04 LTS ou 24.04 LTS (recommandÃ©)
- AccÃ¨s SSH configurÃ©
- Utilisateur avec droits sudo
- IPs statiques configurÃ©es
- Pare-feu ouvert entre les nÅ“uds

**Sur votre poste de travail :**
- Client SSH (terminal Linux/Mac, PuTTY sur Windows)
- kubectl (ou utilisation via microk8s kubectl)
- Ã‰diteur de texte (nano, vim, VS Code)

## Comment Aborder Ce Chapitre

### Approche RecommandÃ©e

**1. Lecture complÃ¨te d'abord**
Lisez tout le chapitre une premiÃ¨re fois pour comprendre le "big picture" avant de commencer Ã  manipuler.

**2. Environnement de test**
CrÃ©ez d'abord votre cluster multi-node dans un environnement de test (VMs) avant de toucher Ã  vos machines physiques.

**3. Progression mÃ©thodique**
Suivez les sections dans l'ordre :
- Ne sautez pas d'Ã©tapes
- Validez chaque section avant de passer Ã  la suivante
- Prenez des notes sur ce qui fonctionne/ne fonctionne pas

**4. Documentation**
Documentez votre installation :
- IPs de chaque nÅ“ud
- Commandes utilisÃ©es
- ProblÃ¨mes rencontrÃ©s et solutions
- SchÃ©ma de votre architecture

**5. Tests rÃ©guliers**
AprÃ¨s chaque section, testez que tout fonctionne encore avant d'avancer.

### Temps EstimÃ©

**Par section :**
- 21.1 - 21.2 : 2-3 heures (lecture + comprÃ©hension)
- 21.3 - 21.4 : 4-6 heures (installation cluster HA)
- 21.5 - 21.7 : 6-8 heures (workers, load balancing, stockage)
- 21.8 - 21.10 : 4-6 heures (backups, redondance, tests)

**Total chapitre 21 : 16-23 heures**

Ã‰talez sur plusieurs jours/semaines selon votre disponibilitÃ©.

### En Cas de ProblÃ¨me

**Checklist de dÃ©pannage rapide :**
1. VÃ©rifier les logs : `journalctl -u snap.microk8s.daemon-kubelite`
2. VÃ©rifier la connectivitÃ© rÃ©seau entre nÅ“uds : `ping`, `telnet`
3. VÃ©rifier le statut : `microk8s status`
4. Consulter la documentation officielle : https://microk8s.io/docs
5. Chercher sur les forums : MicroK8s Discourse, Stack Overflow
6. Revenir Ã  un Ã©tat stable (snapshot VM ou backup)

**N'hÃ©sitez pas Ã  recommencer** : Avec des VMs, vous pouvez dÃ©truire et reconstruire rapidement pour repartir sur de bonnes bases.

## Objectifs du Chapitre

Ã€ la fin du chapitre 21, vous serez capable de :

âœ… **Comprendre** l'architecture multi-node et les concepts de HA
âœ… **Construire** un cluster MicroK8s 3+ nÅ“uds avec haute disponibilitÃ©
âœ… **Configurer** le control plane HA avec Dqlite
âœ… **Ajouter** et gÃ©rer des workers supplÃ©mentaires
âœ… **ImplÃ©menter** le load balancing avec MetalLB
âœ… **DÃ©ployer** du stockage distribuÃ© avec Longhorn
âœ… **Sauvegarder** et restaurer le control plane
âœ… **Concevoir** des stratÃ©gies de redondance complÃ¨tes
âœ… **Tester** la rÃ©silience avec des tests de failover
âœ… **GÃ©rer** un cluster Kubernetes de production

**CompÃ©tences transfÃ©rables :**
Les concepts appris ici (HA, redondance, failover) s'appliquent Ã  tous les clusters Kubernetes (kubeadm, k3s, EKS, GKE, AKS) et mÃªme au-delÃ  de Kubernetes (bases de donnÃ©es, systÃ¨mes distribuÃ©s).

## Ã‰tat d'Esprit

Construire un cluster multi-node hautement disponible peut sembler intimidant au dÃ©but. C'est normal ! Vous allez manipuler plusieurs machines, des concepts de systÃ¨mes distribuÃ©s, de la rÃ©plication, du consensus...

**Quelques conseils :**

**1. Acceptez que Ã§a ne marchera pas du premier coup**
Vous allez rencontrer des problÃ¨mes. C'est **normal** et **formateur**. Chaque erreur est une opportunitÃ© d'apprentissage.

**2. Allez-y progressivement**
Ne cherchez pas Ã  tout faire en mÃªme temps. Cluster basique d'abord, puis HA, puis optimisations.

**3. Testez chaque Ã©tape**
AprÃ¨s chaque modification, vÃ©rifiez que Ã§a fonctionne. N'accumulez pas les changements non testÃ©s.

**4. Documentez votre parcours**
Prenez des notes. Vous les consulterez souvent et elles aideront les futurs vous.

**5. CÃ©lÃ©brez les victoires**
Quand votre cluster HA est opÃ©rationnel, c'est une vraie rÃ©ussite ! Vous aurez acquis des compÃ©tences que beaucoup de professionnels n'ont pas.

## PrÃªt Ã  Commencer ?

Vous avez maintenant une vision claire de ce qui vous attend. Le voyage vers un cluster Kubernetes de production commence maintenant !

**Dans la prochaine section (21.1 - Architecture Multi-Node), vous allez plonger dans les dÃ©tails techniques de l'architecture distribuÃ©e et comprendre comment tous les composants travaillent ensemble.**

---

Prenez une grande inspiration, prÃ©parez vos machines, et c'est parti pour l'aventure du multi-node ! ğŸš€

*Note : Gardez cette introduction Ã  portÃ©e de main. Elle vous servira de carte pour naviguer dans ce chapitre dense mais passionnant.*

â­ï¸ [Architecture multi-node](/21-multi-node-et-haute-disponibilite/01-architecture-multi-node.md)
