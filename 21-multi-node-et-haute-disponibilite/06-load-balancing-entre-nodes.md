ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 21.6 Load Balancing entre Nodes

## Introduction

Maintenant que vous avez un cluster multi-node avec plusieurs workers, une question essentielle se pose : **comment rÃ©partir le trafic entre tous ces nÅ“uds** ? Comment faire en sorte que les utilisateurs accÃ¨dent Ã  vos applications de maniÃ¨re optimale, peu importe sur quel nÅ“ud les pods tournent ?

C'est lÃ  qu'intervient le **load balancing** (rÃ©partition de charge). Dans ce chapitre, nous allons dÃ©couvrir :
- Les diffÃ©rents types de Services Kubernetes
- Comment fonctionne le load balancing natif
- MetalLB : la solution pour les clusters bare-metal
- Comment distribuer le trafic Ã©quitablement entre vos nÅ“uds
- Les bonnes pratiques de configuration

Le load balancing est crucial pour :
- **Performance** : RÃ©partir la charge entre plusieurs serveurs
- **DisponibilitÃ©** : Si un nÅ“ud tombe, le trafic va vers les autres
- **ScalabilitÃ©** : Ajouter de la capacitÃ© sans changer les points d'entrÃ©e
- **SimplicitÃ©** : Une seule adresse IP pour accÃ©der Ã  l'application

## Qu'est-ce que le Load Balancing ?

### DÃ©finition Simple

Le **load balancing** (Ã©quilibrage de charge) consiste Ã  **distribuer les requÃªtes** entre plusieurs serveurs pour Ã©viter qu'un seul soit surchargÃ©.

**Analogie du restaurant :**
Imaginez un restaurant avec plusieurs serveurs :
- **Sans load balancer** : Tous les clients vont vers le mÃªme serveur â†’ il est dÃ©bordÃ©, les autres s'ennuient
- **Avec load balancer** : Un maÃ®tre d'hÃ´tel (load balancer) rÃ©partit les clients entre tous les serveurs â†’ charge Ã©quilibrÃ©e

**Dans Kubernetes :**
```
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Internet  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                     â”‚    Load     â”‚
                     â”‚   Balancer  â”‚  â† RÃ©partit le trafic
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚ Node 1  â”‚         â”‚ Node 2  â”‚         â”‚ Node 3  â”‚
   â”‚ Pod A   â”‚         â”‚ Pod B   â”‚         â”‚ Pod C   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Le load balancer reÃ§oit une requÃªte et la transmet Ã  l'un des pods, peu importe sur quel nÅ“ud il se trouve.

### Les ProblÃ¨mes Ã  RÃ©soudre

**Sans load balancing dans un cluster multi-node :**

1. **Point d'entrÃ©e multiple**
   - Chaque nÅ“ud a sa propre IP
   - L'utilisateur doit connaÃ®tre toutes les IPs
   - Si un nÅ“ud tombe, les requÃªtes vers cette IP Ã©chouent

2. **Distribution inÃ©gale**
   - Un nÅ“ud peut recevoir 80% du trafic
   - Les autres nÅ“uds sont sous-utilisÃ©s
   - Gaspillage de ressources

3. **ComplexitÃ© pour l'utilisateur**
   - Quelle IP utiliser ?
   - Comment gÃ©rer les pannes ?
   - Comment ajouter/retirer des nÅ“uds ?

**Avec load balancing :**
- Une seule IP virtuelle (VIP) pour accÃ©der Ã  l'application
- Trafic automatiquement distribuÃ©
- RÃ©silience automatique (si un nÅ“ud tombe)
- Ajout/retrait de nÅ“uds transparent

## Types de Services Kubernetes

Kubernetes propose plusieurs types de **Services** pour exposer vos applications. Comprendre ces types est essentiel pour bien configurer le load balancing.

### 1. ClusterIP (Par DÃ©faut)

**CaractÃ©ristiques :**
- IP virtuelle **interne** au cluster uniquement
- Accessible seulement depuis l'intÃ©rieur du cluster
- Parfait pour la communication entre microservices

**Exemple :**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  type: ClusterIP
  selector:
    app: backend
  ports:
  - port: 8080
    targetPort: 8080
```

**SchÃ©ma :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            CLUSTER                         â”‚
â”‚                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Frontend  â”‚â”€â”€â”€â”€â”€â–ºâ”‚  Backend   â”‚        â”‚
â”‚  â”‚  Pod       â”‚      â”‚  Service   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                             â”‚              â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚                    â”‚        â”‚        â”‚     â”‚
â”‚               â”Œâ”€â”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”  â”‚
â”‚               â”‚ Pod 1 â”‚ â”‚ Pod 2 â”‚ â”‚Pod 3â”‚  â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Usage :**
- Bases de donnÃ©es internes
- APIs internes
- Microservices qui ne doivent pas Ãªtre exposÃ©s Ã  l'extÃ©rieur

**Limitation :** Pas accessible depuis l'extÃ©rieur du cluster.

### 2. NodePort

**CaractÃ©ristiques :**
- Ouvre un **port** sur chaque nÅ“ud du cluster
- Le port est le mÃªme sur tous les nÅ“uds
- Accessible via `<NodeIP>:<NodePort>`

**Exemple :**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-service
spec:
  type: NodePort
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30080  # Port entre 30000-32767
```

**SchÃ©ma :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CLUSTER                           â”‚
â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Node 1    â”‚   â”‚   Node 2    â”‚   â”‚   Node 3    â”‚  â”‚
â”‚  â”‚ :30080      â”‚   â”‚ :30080      â”‚   â”‚ :30080      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                 â”‚                 â”‚         â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                           â”‚                           â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚                  â”‚   Web Service   â”‚                  â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                           â”‚                           â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚                  â”‚        â”‚        â”‚                  â”‚
â”‚             â”Œâ”€â”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”               â”‚
â”‚             â”‚ Pod 1 â”‚ â”‚ Pod 2 â”‚ â”‚Pod 3â”‚               â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Internet â†’ http://192.168.1.10:30080 â†’ Web Service â†’ Pods
Internet â†’ http://192.168.1.11:30080 â†’ Web Service â†’ Pods
Internet â†’ http://192.168.1.12:30080 â†’ Web Service â†’ Pods
```

**Usage :**
- AccÃ¨s externe simple (dev, test)
- Pas besoin de load balancer externe
- Debugging

**Limitations :**
- Port dans la plage 30000-32767 (pas standard)
- Pas de vraie rÃ©partition de charge (utilisateur doit choisir une IP)
- Pas professionnel pour la production

### 3. LoadBalancer (La Solution IdÃ©ale)

**CaractÃ©ristiques :**
- CrÃ©e automatiquement un **load balancer externe**
- Une **IP unique** pour accÃ©der au service
- RÃ©partition automatique du trafic entre les nÅ“uds
- Haute disponibilitÃ© intÃ©grÃ©e

**Exemple :**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-service
spec:
  type: LoadBalancer
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 8080
```

**SchÃ©ma :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Internet                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Load Balancer â”‚
                 â”‚ 192.168.1.100 â”‚  â† IP Virtuelle (VIP)
                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚ Node 1  â”‚     â”‚ Node 2  â”‚     â”‚ Node 3  â”‚
   â”‚ Pod A   â”‚     â”‚ Pod B   â”‚     â”‚ Pod C   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Avantages :**
- Une seule IP Ã  retenir
- Distribution automatique du trafic
- Haute disponibilitÃ© (si un nÅ“ud tombe, le trafic va ailleurs)
- Port standard (80, 443, etc.)

**Le ProblÃ¨me :**
Dans les clouds publics (AWS, GCP, Azure), le type `LoadBalancer` crÃ©e automatiquement un load balancer cloud. Mais sur **bare-metal** (serveurs physiques, homelab), il ne se passe rien !

```bash
microk8s kubectl get svc web-service
```
```
NAME          TYPE           EXTERNAL-IP   PORT(S)
web-service   LoadBalancer   <pending>     80:30080/TCP
```

L'`EXTERNAL-IP` reste en `<pending>` indÃ©finiment. ğŸ˜

**Solution :** Utiliser **MetalLB** !

## MetalLB : Load Balancer pour Bare-Metal

### Qu'est-ce que MetalLB ?

**MetalLB** est un load balancer logiciel spÃ©cialement conÃ§u pour les clusters Kubernetes **bare-metal** (non-cloud). Il Ã©mule le comportement des load balancers cloud (AWS ELB, GCP Load Balancer, etc.) sur votre propre infrastructure.

**Ce que fait MetalLB :**
1. Attribue une **IP rÃ©elle** aux Services de type `LoadBalancer`
2. **Annonce** cette IP sur le rÃ©seau (via ARP ou BGP)
3. **Distribue** le trafic entrant entre les nÅ“uds

**Analogie :**
C'est comme avoir votre propre service postal (load balancer cloud) mais Ã  domicile (bare-metal). MetalLB joue le rÃ´le du facteur qui distribue le courrier (trafic) aux bonnes boÃ®tes aux lettres (pods).

### Modes de Fonctionnement

MetalLB supporte deux modes :

#### 1. Mode Layer 2 (L2) - RecommandÃ© pour DÃ©buter

**Principe :**
- MetalLB rÃ©pond aux requÃªtes ARP pour l'IP virtuelle
- Un nÅ“ud "leader" est Ã©lu pour chaque IP
- Le trafic arrive sur ce nÅ“ud, puis est distribuÃ© vers les pods

**Avantages :**
- Simple Ã  configurer
- Fonctionne sur n'importe quel rÃ©seau
- Pas besoin de configuration spÃ©ciale sur le switch/routeur

**InconvÃ©nients :**
- Le trafic passe toujours par le mÃªme nÅ“ud (pas de vraie distribution L2)
- Si le nÅ“ud leader tombe, Ã©lection d'un nouveau leader (quelques secondes de coupure)

**SchÃ©ma Layer 2 :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RÃ©seau Local                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ VIP:          â”‚
                 â”‚ 192.168.1.100 â”‚
                 â”‚               â”‚
                 â”‚ MetalLB dit:  â”‚
                 â”‚ "Je suis sur  â”‚
                 â”‚  Node 2!"     â”‚  â† RÃ©ponse ARP
                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚                â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” (Leader) â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Node 1  â”‚     â”‚ Node 2  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Node 3  â”‚
   â”‚         â”‚     â”‚ â–¼ Traficâ”‚          â”‚          â”‚
   â”‚ Pod A   â”‚     â”‚ Pod B   â”‚          â”‚ Pod C    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. Mode BGP - AvancÃ©

**Principe :**
- MetalLB annonce les IPs via le protocole BGP
- Le routeur rÃ©seau distribue le trafic entre les nÅ“uds
- Vraie distribution L3 (couche rÃ©seau)

**Avantages :**
- Vraie rÃ©partition de charge au niveau rÃ©seau
- Haute disponibilitÃ© native
- Pas de single point of failure

**InconvÃ©nients :**
- NÃ©cessite un routeur compatible BGP
- Configuration plus complexe
- Overkill pour un homelab

**Pour la plupart des cas (homelab, PME), le mode Layer 2 est suffisant.**

### Installation de MetalLB avec MicroK8s

**Bonne nouvelle :** MicroK8s intÃ¨gre MetalLB comme addon ! L'installation est **ultra simple**.

**Sur n'importe quel nÅ“ud du cluster :**
```bash
microk8s enable metallb
```

**Question posÃ©e :**
```
Enter each IP address range delimited by comma (e.g. '10.64.140.43-10.64.140.49,192.168.0.105-192.168.0.111'):
```

**Exemple de rÃ©ponse :**
```
192.168.1.100-192.168.1.110
```

**Explication :**
- MetalLB va utiliser les IPs de **192.168.1.100** Ã  **192.168.1.110**
- Il peut attribuer jusqu'Ã  **11 IPs** diffÃ©rentes (une par Service LoadBalancer)
- Ces IPs doivent Ãªtre :
  - **Dans votre rÃ©seau local** (mÃªme sous-rÃ©seau que les nÅ“uds)
  - **Non utilisÃ©es** par d'autres machines
  - **En dehors du range DHCP** de votre routeur

**VÃ©rifier l'installation :**
```bash
microk8s kubectl get pods -n metallb-system
```

```
NAME                          READY   STATUS    RESTARTS
controller-xxx                1/1     Running   0
speaker-yyy                   1/1     Running   0
speaker-zzz                   1/1     Running   0
```

- **controller** : GÃ¨re l'attribution des IPs aux Services
- **speaker** : Un par nÅ“ud, annonce les IPs sur le rÃ©seau (ARP)

**MetalLB est maintenant opÃ©rationnel !** ğŸ‰

## Configuration de MetalLB

### Choisir le Bon Range d'IPs

**Ã‰tapes pour dÃ©terminer le range :**

**1. Identifier votre rÃ©seau local :**
```bash
ip addr show
```

Exemple de sortie :
```
eth0: inet 192.168.1.10/24
```

Votre rÃ©seau est `192.168.1.0/24` (de 192.168.1.1 Ã  192.168.1.254).

**2. VÃ©rifier le range DHCP de votre routeur :**
Connectez-vous Ã  l'interface web de votre routeur (souvent `192.168.1.1`).

Exemple de range DHCP :
```
DHCP Range: 192.168.1.50 - 192.168.1.200
```

**3. Choisir un range en dehors du DHCP :**
```
Range MetalLB: 192.168.1.100 - 192.168.1.110
```

**Attention :** Ce range est **dans** le range DHCP de l'exemple ci-dessus ! Il faut soit :
- **Option A** : RÃ©duire le range DHCP dans le routeur (ex: 192.168.1.50-192.168.1.99)
- **Option B** : Utiliser un range plus bas (ex: 192.168.1.10-192.168.1.20)
- **Option C** : Utiliser un range plus haut (ex: 192.168.1.201-192.168.1.210)

**Recommandation pour un homelab :**
```
Range MetalLB: 192.168.1.200 - 192.168.1.210
```

### Exemple de Configuration RÃ©seau ComplÃ¨te

```
Routeur:             192.168.1.1
Range DHCP:          192.168.1.50 - 192.168.1.149
NÅ“uds Kubernetes:
  - node1:           192.168.1.10 (IP statique)
  - node2:           192.168.1.11 (IP statique)
  - node3:           192.168.1.12 (IP statique)
  - worker1:         192.168.1.20 (IP statique)
  - worker2:         192.168.1.21 (IP statique)
Range MetalLB:       192.168.1.200 - 192.168.1.210
```

**Avec cette configuration :**
- Les nÅ“uds Kubernetes ont des IPs fixes en dehors du DHCP
- MetalLB peut attribuer 11 IPs (200 Ã  210)
- Pas de conflit avec le DHCP

### Configuration AvancÃ©e (ConfigMap)

Si vous voulez modifier la configuration aprÃ¨s l'installation :

```bash
microk8s kubectl get configmap -n metallb-system config -o yaml
```

**Exemple de ConfigMap MetalLB :**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 192.168.1.200-192.168.1.210
```

**Modifier le range :**
```bash
microk8s kubectl edit configmap -n metallb-system config
```

Changez les adresses, sauvegardez. MetalLB applique automatiquement les changements.

### Plusieurs Pools d'Adresses

Vous pouvez dÃ©finir plusieurs pools pour diffÃ©rents usages :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: production
      protocol: layer2
      addresses:
      - 192.168.1.200-192.168.1.205
    - name: development
      protocol: layer2
      addresses:
      - 192.168.1.206-192.168.1.210
```

**Utiliser un pool spÃ©cifique dans un Service :**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-prod
  annotations:
    metallb.universe.tf/address-pool: production
spec:
  type: LoadBalancer
  selector:
    app: web
  ports:
  - port: 80
```

## Utiliser MetalLB : Premier Service LoadBalancer

### DÃ©ployer une Application de Test

**1. CrÃ©er un Deployment :**
```yaml
# deployment-nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-lb-test
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
```

```bash
microk8s kubectl apply -f deployment-nginx.yaml
```

**2. CrÃ©er un Service LoadBalancer :**
```yaml
# service-nginx.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-lb
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
  - port: 80
    targetPort: 80
```

```bash
microk8s kubectl apply -f service-nginx.yaml
```

**3. VÃ©rifier l'IP attribuÃ©e par MetalLB :**
```bash
microk8s kubectl get svc nginx-lb
```

**Avant MetalLB :**
```
NAME       TYPE           EXTERNAL-IP   PORT(S)
nginx-lb   LoadBalancer   <pending>     80:30123/TCP
```

**Avec MetalLB :**
```
NAME       TYPE           EXTERNAL-IP     PORT(S)
nginx-lb   LoadBalancer   192.168.1.200   80:30123/TCP
```

**L'IP 192.168.1.200 est attribuÃ©e !** ğŸ‰

**4. Tester l'accÃ¨s :**
```bash
curl http://192.168.1.200
```

Vous devriez voir la page d'accueil de Nginx.

**Depuis n'importe quelle machine du rÃ©seau local :**
Ouvrez un navigateur et allez sur `http://192.168.1.200` â†’ Nginx fonctionne !

## Comment Fonctionne le Load Balancing ?

### Distribution du Trafic

Avec 3 rÃ©plicas de Nginx sur 3 nÅ“uds diffÃ©rents :

```bash
microk8s kubectl get pods -o wide
```

```
NAME                  NODE     STATUS
nginx-lb-test-abc     node1    Running
nginx-lb-test-def     node2    Running
nginx-lb-test-ghi     worker1  Running
```

**Quand vous faites une requÃªte Ã  `192.168.1.200` :**

1. **RequÃªte ARP** : Votre machine demande "Qui a 192.168.1.200 ?"
2. **MetalLB speaker** sur le nÅ“ud leader rÃ©pond : "C'est moi (node2) !"
3. **Trafic arrive sur node2**
4. **kube-proxy** sur node2 distribue vers l'un des 3 pods (round-robin par dÃ©faut)
5. Le pod traite la requÃªte et rÃ©pond

**Important :** MÃªme si le pod est sur node1 ou worker1, le trafic peut arriver sur node2, puis Ãªtre redirigÃ©. C'est transparent.

### Algorithmes de RÃ©partition

**Par dÃ©faut :** Kubernetes utilise **round-robin** (chacun son tour).

**Exemple de 6 requÃªtes :**
```
RequÃªte 1 â†’ Pod sur node1
RequÃªte 2 â†’ Pod sur node2
RequÃªte 3 â†’ Pod sur worker1
RequÃªte 4 â†’ Pod sur node1
RequÃªte 5 â†’ Pod sur node2
RequÃªte 6 â†’ Pod sur worker1
```

**Session Affinity (Sticky Sessions) :**
Si vous voulez qu'un client soit toujours redirigÃ© vers le mÃªme pod :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-lb-sticky
spec:
  type: LoadBalancer
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 3600  # 1 heure
  selector:
    app: nginx
  ports:
  - port: 80
```

Maintenant, chaque client (IP source) sera toujours envoyÃ© vers le mÃªme pod pendant 1 heure.

### Haute DisponibilitÃ©

**ScÃ©nario : Un pod crash**

```bash
# Simuler un crash
microk8s kubectl delete pod nginx-lb-test-abc
```

**Ce qui se passe :**
1. Kubernetes dÃ©tecte que le pod est mort
2. Un nouveau pod est automatiquement crÃ©Ã© (Deployment avec replicas=3)
3. Le Service redirige le trafic vers les 2 pods restants pendant la recrÃ©ation
4. Une fois le nouveau pod prÃªt, il est ajoutÃ© Ã  la rotation

**Downtime pour l'utilisateur : 0 seconde** (si au moins 2 rÃ©plicas fonctionnent)

**ScÃ©nario : Un nÅ“ud tombe**

```bash
# Sur node2
sudo shutdown -h now
```

**Ce qui se passe :**
1. MetalLB dÃ©tecte que node2 (leader) ne rÃ©pond plus
2. Un nouveau leader est Ã©lu (node1 ou worker1) en quelques secondes
3. Le nouveau leader rÃ©pond aux requÃªtes ARP
4. Kubernetes reschedule les pods de node2 sur d'autres nÅ“uds
5. Le Service continue de fonctionner

**Downtime : 5-10 secondes** (temps de l'Ã©lection du nouveau leader)

## Services LoadBalancer Multiples

MetalLB peut gÃ©rer **plusieurs Services LoadBalancer** avec des IPs diffÃ©rentes.

**Exemple avec 3 applications :**

```bash
# Application 1
microk8s kubectl create deployment app1 --image=nginx
microk8s kubectl expose deployment app1 --type=LoadBalancer --port=80

# Application 2
microk8s kubectl create deployment app2 --image=httpd
microk8s kubectl expose deployment app2 --type=LoadBalancer --port=80

# Application 3
microk8s kubectl create deployment app3 --image=caddy
microk8s kubectl expose deployment app3 --type=LoadBalancer --port=80

# VÃ©rifier les IPs
microk8s kubectl get svc
```

**RÃ©sultat :**
```
NAME   TYPE           EXTERNAL-IP     PORT(S)
app1   LoadBalancer   192.168.1.200   80:30001/TCP
app2   LoadBalancer   192.168.1.201   80:30002/TCP
app3   LoadBalancer   192.168.1.202   80:30003/TCP
```

Chaque application a sa **propre IP** !

**AccÃ¨s :**
- `http://192.168.1.200` â†’ Nginx
- `http://192.168.1.201` â†’ Apache
- `http://192.168.1.202` â†’ Caddy

### Limites du Range

Si vous avez dÃ©fini `192.168.1.200-192.168.1.210` (11 IPs), vous pouvez avoir jusqu'Ã  **11 Services LoadBalancer**.

Si vous crÃ©ez un 12Ã¨me Service :
```bash
microk8s kubectl get svc app12
```
```
NAME    TYPE           EXTERNAL-IP   PORT(S)
app12   LoadBalancer   <pending>     80:30012/TCP
```

L'IP reste en `<pending>` â†’ **plus d'IPs disponibles dans le pool**.

**Solution :** Agrandir le range dans la ConfigMap MetalLB.

## IntÃ©gration avec Ingress

MetalLB et Ingress sont **complÃ©mentaires** :

**MetalLB** : Fournit une IP externe au Service
**Ingress** : Route le trafic HTTP/HTTPS vers diffÃ©rents Services en fonction du hostname ou du path

**Architecture typique :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Internet                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   MetalLB     â”‚
                 â”‚ 192.168.1.200 â”‚
                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                 â”‚   Ingress   â”‚
                 â”‚  Controller â”‚
                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
   â”‚ Service  â”‚   â”‚ Service  â”‚   â”‚ Service  â”‚
   â”‚   App1   â”‚   â”‚   App2   â”‚   â”‚   App3   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Exemple de configuration :**

```bash
# 1. Activer Ingress (NGINX)
microk8s enable ingress

# 2. Exposer l'Ingress Controller avec LoadBalancer
# (MicroK8s le fait automatiquement)
```

**VÃ©rifier :**
```bash
microk8s kubectl get svc -n ingress
```

```
NAME                   TYPE           EXTERNAL-IP
nginx-ingress-...      LoadBalancer   192.168.1.200
```

Maintenant, **une seule IP (192.168.1.200)** peut servir plusieurs applications :

**Ingress avec hostnames :**
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: multi-app
spec:
  rules:
  - host: app1.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app1-service
            port:
              number: 80
  - host: app2.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app2-service
            port:
              number: 80
```

**RÃ©sultat :**
- `http://app1.example.com` â†’ app1-service
- `http://app2.example.com` â†’ app2-service
- Les deux utilisent la mÃªme IP (192.168.1.200)

**Ã‰conomie d'IPs !** Vous n'utilisez qu'une seule IP du pool MetalLB pour plusieurs applications.

## Monitoring et ObservabilitÃ©

### VÃ©rifier l'Ã‰tat de MetalLB

**Pods MetalLB :**
```bash
microk8s kubectl get pods -n metallb-system
```

Tous les pods doivent Ãªtre en `Running`.

**Logs du Controller :**
```bash
microk8s kubectl logs -n metallb-system -l component=controller
```

**Logs d'un Speaker :**
```bash
microk8s kubectl logs -n metallb-system -l component=speaker
```

**Ã‰vÃ©nements MetalLB :**
```bash
microk8s kubectl get events -n metallb-system --sort-by='.lastTimestamp'
```

### Voir les Attributions d'IPs

**Liste de tous les Services LoadBalancer :**
```bash
microk8s kubectl get svc --all-namespaces -o wide | grep LoadBalancer
```

**DÃ©tails d'un Service spÃ©cifique :**
```bash
microk8s kubectl describe svc nginx-lb
```

Cherchez la section `LoadBalancer Ingress` :
```
LoadBalancer Ingress:     192.168.1.200
```

### Tester la Distribution

**Script pour gÃ©nÃ©rer du trafic et voir la distribution :**
```bash
#!/bin/bash
# test-load-balance.sh

SERVICE_IP="192.168.1.200"

for i in {1..100}; do
  curl -s http://$SERVICE_IP | grep "Server address" &
done

wait
```

Vous verrez que les requÃªtes sont distribuÃ©es entre diffÃ©rents pods.

### Prometheus et Grafana

Si Prometheus est activÃ©, MetalLB expose des mÃ©triques :

**RequÃªtes PromQL utiles :**
```promql
# Nombre de Services LoadBalancer
count(kube_service_info{type="LoadBalancer"})

# IPs attribuÃ©es par MetalLB
metallb_allocator_addresses_in_use_total
```

## Bonnes Pratiques

### 1. Planifier le Range d'IPs

**Ã‰vitez de changer le range frÃ©quemment.**

Planifiez en fonction de vos besoins :
- **Homelab petit** : 5-10 IPs suffisent
- **Homelab moyen** : 10-20 IPs
- **PME** : 20-50 IPs

**Laissez de la marge pour la croissance.**

### 2. Documenter les Attributions

**Maintenir un fichier avec les IPs utilisÃ©es :**
```markdown
# IPs MetalLB

Range total: 192.168.1.200-192.168.1.210

Attributions:
- 192.168.1.200: nginx-lb (namespace: default)
- 192.168.1.201: ingress-controller (namespace: ingress)
- 192.168.1.202: monitoring-grafana (namespace: observability)
- 192.168.1.203: app-production (namespace: prod)
- 192.168.1.204-210: Disponibles
```

### 3. Utiliser Ingress pour Ã‰conomiser des IPs

**Au lieu de :**
```
app1 â†’ LoadBalancer â†’ 192.168.1.200
app2 â†’ LoadBalancer â†’ 192.168.1.201
app3 â†’ LoadBalancer â†’ 192.168.1.202
```

**Faire :**
```
Ingress â†’ LoadBalancer â†’ 192.168.1.200
  â”œâ”€ app1.example.com â†’ app1
  â”œâ”€ app2.example.com â†’ app2
  â””â”€ app3.example.com â†’ app3
```

**Ã‰conomie : 2 IPs !**

### 4. DÃ©finir des Pools SÃ©parÃ©s

SÃ©parez les environnements avec des pools :
```yaml
address-pools:
- name: production
  protocol: layer2
  addresses:
  - 192.168.1.200-192.168.1.205
- name: staging
  protocol: layer2
  addresses:
  - 192.168.1.206-192.168.1.208
- name: development
  protocol: layer2
  addresses:
  - 192.168.1.209-192.168.1.210
```

### 5. Monitoring et Alertes

**Alertes recommandÃ©es :**
- Pool MetalLB avec < 20% d'IPs disponibles
- Speaker pod en erreur
- Service LoadBalancer en `<pending>` > 5 minutes

### 6. DNS Local

**CrÃ©er des enregistrements DNS pour les IPs :**

Dans votre DNS local (Pi-hole, dnsmasq, router) :
```
192.168.1.200    nginx.local
192.168.1.201    grafana.local
192.168.1.202    app.local
```

**Avantage :** AccÃ¨s par nom plutÃ´t que par IP.

## DÃ©pannage

### ProblÃ¨me : EXTERNAL-IP reste en `<pending>`

**Causes possibles :**

**1. MetalLB pas installÃ©**
```bash
microk8s kubectl get pods -n metallb-system
```

Si vide, installer MetalLB :
```bash
microk8s enable metallb
```

**2. Pool d'IPs vide**
```bash
microk8s kubectl get configmap -n metallb-system config -o yaml
```

VÃ©rifier que le range d'adresses est dÃ©fini.

**3. Toutes les IPs du pool sont utilisÃ©es**
```bash
microk8s kubectl get svc --all-namespaces | grep LoadBalancer
```

Comptez le nombre de Services. Si Ã©gal au nombre d'IPs du pool, agrandir le pool.

**4. Speaker pod en erreur**
```bash
microk8s kubectl get pods -n metallb-system
```

Si un speaker est en `Error` ou `CrashLoopBackOff` :
```bash
microk8s kubectl logs -n metallb-system <speaker-pod-name>
```

### ProblÃ¨me : IP attribuÃ©e mais pas accessible

**Diagnostic :**

**1. Tester depuis un nÅ“ud du cluster**
```bash
# Depuis node1
curl http://192.168.1.200
```

Si Ã§a fonctionne depuis le cluster mais pas depuis l'extÃ©rieur :

**2. VÃ©rifier le firewall**
```bash
# Sur chaque nÅ“ud
sudo ufw status
```

Autoriser le port :
```bash
sudo ufw allow 80/tcp
```

**3. VÃ©rifier le routeur**
Assurez-vous que le routeur n'a pas de filtrage MAC ou rÃ¨gles qui bloquent les IPs MetalLB.

**4. Tester ARP**
```bash
# Depuis une machine du rÃ©seau
arp -a | grep 192.168.1.200
```

Vous devriez voir l'IP avec l'adresse MAC d'un des nÅ“uds.

### ProblÃ¨me : Trafic pas distribuÃ© Ã©quitablement

**C'est normal avec Layer 2 !**

En mode Layer 2, MetalLB choisit un nÅ“ud leader. Tout le trafic passe par ce nÅ“ud avant d'Ãªtre distribuÃ© vers les pods.

**Solution pour une vraie distribution :**
- Utiliser le mode BGP (nÃ©cessite routeur BGP)
- Ou accepter cette limitation (acceptable pour la plupart des cas)

### ProblÃ¨me : Changement de leader frÃ©quent

**SymptÃ´me :** Les logs MetalLB montrent beaucoup de "leader election".

**Cause possible :** ProblÃ¨me rÃ©seau (latence, paquets perdus).

**Diagnostic :**
```bash
# Tester la latence entre nÅ“uds
ping -c 100 192.168.1.10
```

Si > 10% de paquets perdus ou latence > 50ms, investiguer le rÃ©seau.

## Alternatives Ã  MetalLB

### 1. NodePort + Reverse Proxy Externe

**Principe :**
- Exposer les apps avec NodePort
- Un reverse proxy externe (HAProxy, Nginx) distribue vers les nÅ“uds

**Avantages :**
- Pas de MetalLB nÃ©cessaire
- ContrÃ´le total

**InconvÃ©nients :**
- Plus complexe
- Reverse proxy = point de dÃ©faillance unique (SPOF)

### 2. Ingress seul (sans LoadBalancer)

**Principe :**
- Ingress Controller en NodePort
- AccÃ¨s via `<NodeIP>:30080`

**Avantages :**
- Simple
- Pas besoin de MetalLB

**InconvÃ©nients :**
- Port non-standard
- Pas de distribution L2

### 3. kube-vip

**Alternative Ã  MetalLB :**
- Similaire Ã  MetalLB
- Supporte aussi ARP et BGP
- Moins populaire mais viable

### 4. Pure LB

**Alternative Ã©mergente :**
- Plus moderne
- Moins mature que MetalLB

**MetalLB reste la solution la plus Ã©prouvÃ©e et recommandÃ©e pour bare-metal.**

## Points ClÃ©s Ã  Retenir

1. **Load balancing** = Distribution du trafic entre plusieurs serveurs
2. **Service Types** : ClusterIP (interne), NodePort (simple), LoadBalancer (idÃ©al)
3. **MetalLB** = Load balancer logiciel pour bare-metal (non-cloud)
4. **Mode Layer 2** : Simple, recommandÃ© pour dÃ©buter
5. **Range d'IPs** : Choisir un range libre, hors DHCP
6. **Installation** : `microk8s enable metallb` (ultra simple)
7. **Une IP par Service** LoadBalancer, ou Ingress pour partager
8. **HA native** : Si un nÅ“ud tombe, trafic automatiquement redirigÃ©
9. **Monitoring** : VÃ©rifier les speakers et les attributions d'IPs
10. **Bonnes pratiques** : Planifier le range, documenter, utiliser Ingress

## Prochaines Ã‰tapes

Vous maÃ®trisez maintenant le load balancing entre nÅ“uds ! Les prochains chapitres couvriront :

- **21.7** : Stockage distribuÃ© pour la rÃ©silience des donnÃ©es
- **21.8** : Backup du control plane et restauration
- **21.9** : StratÃ©gies de redondance avancÃ©es
- **21.10** : Tests de failover complets

Avec MetalLB, votre cluster multi-node est maintenant accessible de maniÃ¨re professionnelle, avec une IP unique et une haute disponibilitÃ© ! ğŸš€

---

**FÃ©licitations !** Vous savez maintenant configurer et utiliser le load balancing dans un cluster MicroK8s multi-node. C'est une compÃ©tence essentielle pour des dÃ©ploiements professionnels et rÃ©silients !

â­ï¸ [Stockage distribuÃ©](/21-multi-node-et-haute-disponibilite/07-stockage-distribue.md)
