üîù Retour au [Sommaire](/SOMMAIRE.md)

# 17.9 Strat√©gies de d√©ploiement

## Introduction

Vous avez construit un pipeline CI/CD complet, vos applications sont test√©es automatiquement, et vous √™tes pr√™t √† d√©ployer en production. Mais comment minimiser les risques lors d'une mise √† jour ? Comment d√©ployer une nouvelle version sans interruption de service ? Comment tester une nouvelle fonctionnalit√© sur un petit pourcentage d'utilisateurs avant de la g√©n√©raliser ?

C'est l√† qu'interviennent les **strat√©gies de d√©ploiement**. Elles d√©finissent **comment** une nouvelle version d'une application remplace l'ancienne, avec diff√©rents niveaux de risque, de complexit√© et d'interruption de service.

Dans ce chapitre, nous allons explorer les principales strat√©gies de d√©ploiement, de la plus simple (Recreate) √† la plus sophistiqu√©e (Canary avec analyse automatique), en comprenant quand et comment utiliser chacune d'elles sur votre cluster MicroK8s.

---

## Pourquoi les strat√©gies de d√©ploiement sont importantes

### Le probl√®me du d√©ploiement traditionnel

**Avant Kubernetes**, les d√©ploiements ressemblaient √† ceci :

1. Arr√™ter l'application (downtime)
2. Mettre √† jour le code
3. Red√©marrer l'application
4. Croiser les doigts ü§û

**Probl√®mes** :
- ‚ùå Interruption de service (downtime)
- ‚ùå Impossible de revenir en arri√®re facilement
- ‚ùå Tout ou rien : tous les utilisateurs impact√©s imm√©diatement
- ‚ùå Stress maximal lors des d√©ploiements

### Les objectifs des strat√©gies modernes

‚úÖ **Zero-downtime** : Pas d'interruption de service

‚úÖ **Rollback rapide** : Retour en arri√®re en quelques secondes

‚úÖ **D√©ploiement progressif** : Exposition graduelle de la nouvelle version

‚úÖ **Validation automatique** : D√©tection automatique des probl√®mes

‚úÖ **R√©duction des risques** : Limiter l'impact d'un bug

‚úÖ **Confiance** : D√©ployer sereinement plusieurs fois par jour

---

## Vue d'ensemble des strat√©gies

### Tableau comparatif

| Strat√©gie | Downtime | Rollback | Complexit√© | Co√ªt ressources | Use case |
|-----------|----------|----------|------------|-----------------|----------|
| **Recreate** | ‚ö†Ô∏è Oui | Moyen | Tr√®s simple | Faible | Dev, apps non-critiques |
| **Rolling Update** | ‚úÖ Non | Rapide | Simple | Faible | Production standard |
| **Blue-Green** | ‚úÖ Non | Instantan√© | Moyen | √âlev√© (2x) | Production critique |
| **Canary** | ‚úÖ Non | Rapide | √âlev√© | Moyen | Fonctionnalit√©s risqu√©es |
| **A/B Testing** | ‚úÖ Non | Rapide | √âlev√© | Moyen | Test de features |

### Le spectre risque/complexit√©

```
Risque √©lev√©                                         Risque faible
‚îÇ                                                                ‚îÇ
Recreate ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Rolling Update ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Canary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Blue-Green
‚îÇ                                                                ‚îÇ
Simple                                                    Complexe
```

---

## Strat√©gie 1 : Recreate (Recr√©ation)

### Principe

La strat√©gie **Recreate** est la plus simple : tous les pods existants sont **arr√™t√©s** avant de cr√©er les nouveaux pods avec la nouvelle version.

**Workflow** :

```
√âtat initial:     v1  v1  v1  (3 pods version 1)
                   ‚Üì   ‚Üì   ‚Üì
Arr√™t:            üõë  üõë  üõë  (tous arr√™t√©s)
                   ‚Üì   ‚Üì   ‚Üì
D√©marrage:        v2  v2  v2  (3 pods version 2)
```

### Configuration Kubernetes

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mon-app
spec:
  replicas: 3
  strategy:
    type: Recreate  # Strat√©gie Recreate
  selector:
    matchLabels:
      app: mon-app
  template:
    metadata:
      labels:
        app: mon-app
    spec:
      containers:
      - name: app
        image: localhost:32000/mon-app:v2.0.0
        ports:
        - containerPort: 8080
```

### D√©ploiement

```bash
# Mettre √† jour l'image
kubectl set image deployment/mon-app app=localhost:32000/mon-app:v2.0.0

# Observer le d√©ploiement
kubectl get pods -w

# R√©sultat :
# mon-app-v1-xxx   1/1   Running       0   5m
# mon-app-v1-yyy   1/1   Running       0   5m
# mon-app-v1-zzz   1/1   Running       0   5m
# mon-app-v1-xxx   1/1   Terminating   0   5m
# mon-app-v1-yyy   1/1   Terminating   0   5m
# mon-app-v1-zzz   1/1   Terminating   0   5m
# ...quelques secondes plus tard...
# mon-app-v2-aaa   0/1   Pending       0   0s
# mon-app-v2-bbb   0/1   Pending       0   0s
# mon-app-v2-ccc   0/1   Pending       0   0s
# mon-app-v2-aaa   1/1   Running       0   10s
# mon-app-v2-bbb   1/1   Running       0   10s
# mon-app-v2-ccc   1/1   Running       0   10s
```

### Avantages et inconv√©nients

**‚úÖ Avantages** :
- Tr√®s simple √† impl√©menter
- Pas de versions mixtes (v1 et v2 simultan√©ment)
- Consommation de ressources minimale
- √âvite les probl√®mes de compatibilit√© entre versions

**‚ùå Inconv√©nients** :
- **Downtime** : service indisponible pendant plusieurs secondes/minutes
- Pas adapt√© √† la production
- Tous les utilisateurs impact√©s simultan√©ment
- Rollback lent (n√©cessite de recr√©er les pods)

**Cas d'usage** :
- Environnements de d√©veloppement
- Applications non-critiques
- Maintenance planifi√©e avec fen√™tre de downtime
- Migrations de base de donn√©es incompatibles

---

## Strat√©gie 2 : Rolling Update (Mise √† jour progressive)

### Principe

La strat√©gie **Rolling Update** (par d√©faut dans Kubernetes) remplace progressivement les pods de l'ancienne version par la nouvelle, en maintenant toujours un certain nombre de pods disponibles.

**Workflow** :

```
√âtat initial:     v1  v1  v1  v1  v1  (5 pods)
                   ‚Üì
√âtape 1:          v1  v1  v1  v1  v2  (1 nouveau pod v2 cr√©√©)
                   ‚Üì
√âtape 2:          v1  v1  v1  v2  v2  (1 ancien pod v1 supprim√©)
                   ‚Üì
√âtape 3:          v1  v1  v2  v2  v2  (continue...)
                   ‚Üì
√âtape 4:          v1  v2  v2  v2  v2
                   ‚Üì
√âtat final:       v2  v2  v2  v2  v2  (tous en v2)
```

### Configuration Kubernetes

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mon-app
spec:
  replicas: 5
  strategy:
    type: RollingUpdate  # Par d√©faut
    rollingUpdate:
      maxSurge: 1        # Nombre max de pods suppl√©mentaires
      maxUnavailable: 1  # Nombre max de pods indisponibles
  selector:
    matchLabels:
      app: mon-app
  template:
    metadata:
      labels:
        app: mon-app
        version: v2.0.0
    spec:
      containers:
      - name: app
        image: localhost:32000/mon-app:v2.0.0
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
```

### Param√®tres importants

#### maxSurge

Nombre **maximum de pods** qui peuvent √™tre cr√©√©s au-dessus du nombre d√©sir√© pendant la mise √† jour.

- **Valeur** : nombre absolu (ex: `2`) ou pourcentage (ex: `25%`)
- **Exemple** : Si `replicas: 5` et `maxSurge: 2`, il peut y avoir jusqu'√† **7 pods** temporairement

```yaml
maxSurge: 1       # D√©ploiement conservateur (1 pod √† la fois)
maxSurge: 2       # √âquilibr√©
maxSurge: 50%     # Rapide (la moiti√© des pods en plus)
maxSurge: 100%    # Tr√®s rapide (double la capacit√© temporairement)
```

#### maxUnavailable

Nombre **maximum de pods** qui peuvent √™tre indisponibles pendant la mise √† jour.

- **Valeur** : nombre absolu (ex: `1`) ou pourcentage (ex: `25%`)
- **Exemple** : Si `replicas: 5` et `maxUnavailable: 1`, il y aura toujours au minimum **4 pods disponibles**

```yaml
maxUnavailable: 0    # Aucun pod indisponible (tr√®s s√ªr, mais lent)
maxUnavailable: 1    # Standard
maxUnavailable: 25%  # Un quart peut √™tre indisponible
```

**‚ö†Ô∏è Important** : `maxSurge` et `maxUnavailable` ne peuvent pas √™tre tous les deux √† 0.

### Sc√©narios de configuration

#### Configuration conservatrice (production critique)

```yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 0
```

**Comportement** :
- Toujours 5 pods disponibles
- 1 nouveau pod cr√©√©, attend qu'il soit ready
- 1 ancien pod supprim√©
- R√©p√®te jusqu'√† compl√©tion
- **Lent mais tr√®s s√ªr**

#### Configuration √©quilibr√©e (standard)

```yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 1
```

**Comportement** :
- 3-6 pods disponibles en permanence
- √âquilibre entre vitesse et s√©curit√©
- **Recommand√© pour la plupart des cas**

#### Configuration rapide (non-critique)

```yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 100%
    maxUnavailable: 50%
```

**Comportement** :
- Peut aller jusqu'√† 10 pods temporairement
- Au moins 3 pods disponibles
- **Tr√®s rapide mais consomme plus de ressources**

### D√©ploiement et monitoring

```bash
# Mettre √† jour l'image
kubectl set image deployment/mon-app app=localhost:32000/mon-app:v2.0.0

# Surveiller le rollout
kubectl rollout status deployment/mon-app

# Observer en temps r√©el
kubectl get pods -w -l app=mon-app

# Voir l'historique
kubectl rollout history deployment/mon-app
```

### Rollback

```bash
# Annuler le d√©ploiement en cours
kubectl rollout undo deployment/mon-app

# Revenir √† une r√©vision sp√©cifique
kubectl rollout history deployment/mon-app
kubectl rollout undo deployment/mon-app --to-revision=3

# Pause/Resume
kubectl rollout pause deployment/mon-app
# ...v√©rifications...
kubectl rollout resume deployment/mon-app
```

### Avantages et inconv√©nients

**‚úÖ Avantages** :
- Zero-downtime
- Natif dans Kubernetes (pas d'outil externe)
- Rollback rapide et facile
- Consommation de ressources contr√¥l√©e
- Adapt√© √† la plupart des cas d'usage

**‚ùå Inconv√©nients** :
- Versions mixtes temporaires (v1 et v2 coexistent)
- Difficile de contr√¥ler quel utilisateur voit quelle version
- Si v2 a un bug, une partie des utilisateurs est impact√©e avant d√©tection
- Pas de validation automatique avant compl√©tion

**Cas d'usage** :
- D√©ploiements en production standard
- Applications stateless
- Mises √† jour compatibles backward

---

## Strat√©gie 3 : Blue-Green Deployment

### Principe

Le d√©ploiement **Blue-Green** maintient deux environnements de production identiques :
- **Blue** : version actuelle en production
- **Green** : nouvelle version en pr√©paration

Une fois la version Green valid√©e, on bascule le trafic instantan√©ment de Blue vers Green.

**Workflow** :

```
√âtat initial:
  Blue (v1)  ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Trafic (100%)
  Green      ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Idle

D√©ploiement Green:
  Blue (v1)  ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Trafic (100%)
  Green (v2) ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Tests

Validation OK, bascule:
  Blue (v1)  ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Idle
  Green (v2) ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Trafic (100%)

Cleanup (optionnel):
  Green (v2) ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Trafic (100%)
  (Blue supprim√©)
```

### Impl√©mentation avec Kubernetes

#### M√©thode 1 : Via les labels de Service

**D√©ploiements Blue et Green** :

```yaml
# deployment-blue.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mon-app-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mon-app
      version: blue
  template:
    metadata:
      labels:
        app: mon-app
        version: blue
    spec:
      containers:
      - name: app
        image: localhost:32000/mon-app:v1.0.0
        ports:
        - containerPort: 8080
---
# deployment-green.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mon-app-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mon-app
      version: green
  template:
    metadata:
      labels:
        app: mon-app
        version: green
    spec:
      containers:
      - name: app
        image: localhost:32000/mon-app:v2.0.0
        ports:
        - containerPort: 8080
```

**Service pointant initialement vers Blue** :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mon-app
spec:
  selector:
    app: mon-app
    version: blue  # Pointe vers Blue
  ports:
  - port: 80
    targetPort: 8080
```

**Bascule vers Green** :

```bash
# D√©ployer Green
kubectl apply -f deployment-green.yaml

# Attendre que Green soit pr√™t
kubectl wait --for=condition=ready pod -l version=green --timeout=300s

# Tests sur Green (via port-forward ou service temporaire)
kubectl port-forward deployment/mon-app-green 8081:8080
curl http://localhost:8081/health

# Bascule instantan√©e du trafic
kubectl patch service mon-app -p '{"spec":{"selector":{"version":"green"}}}'

# V√©rifier
kubectl describe service mon-app

# Apr√®s validation, supprimer Blue
kubectl delete deployment mon-app-blue
```

#### M√©thode 2 : Via deux Services et Ingress

**Services d√©di√©s** :

```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: mon-app-blue
spec:
  selector:
    app: mon-app
    version: blue
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: mon-app-green
spec:
  selector:
    app: mon-app
    version: green
  ports:
  - port: 80
    targetPort: 8080
```

**Ingress avec bascule** :

```yaml
# ingress-blue.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: mon-app
spec:
  ingressClassName: nginx
  rules:
  - host: mon-app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: mon-app-blue  # Trafic vers Blue
            port:
              number: 80
```

**Bascule** :

```bash
# Modifier l'Ingress pour pointer vers Green
kubectl patch ingress mon-app --type='json' \
  -p='[{"op": "replace", "path": "/spec/rules/0/http/paths/0/backend/service/name", "value":"mon-app-green"}]'
```

### Script d'automatisation

```bash
#!/bin/bash
# blue-green-deploy.sh

set -e

APP_NAME="mon-app"
NEW_VERSION=$1
CURRENT_COLOR=$2  # blue ou green
NEW_COLOR=$3      # green ou blue

echo "üöÄ D√©ploiement Blue-Green"
echo "Version actuelle: ${CURRENT_COLOR}"
echo "Nouvelle version: ${NEW_VERSION} sur ${NEW_COLOR}"

# 1. D√©ployer la nouvelle version
echo "üì¶ D√©ploiement de ${NEW_COLOR}..."
kubectl set image deployment/${APP_NAME}-${NEW_COLOR} \
  app=localhost:32000/${APP_NAME}:${NEW_VERSION}

# 2. Attendre que ce soit pr√™t
echo "‚è≥ Attente que ${NEW_COLOR} soit pr√™t..."
kubectl rollout status deployment/${APP_NAME}-${NEW_COLOR}
kubectl wait --for=condition=ready pod -l version=${NEW_COLOR} --timeout=300s

# 3. Tests de smoke
echo "üî• Tests de smoke sur ${NEW_COLOR}..."
POD=$(kubectl get pod -l version=${NEW_COLOR} -o jsonpath='{.items[0].metadata.name}')
kubectl exec ${POD} -- wget -qO- http://localhost:8080/health || {
  echo "‚ùå Tests √©chou√©s!"
  exit 1
}

# 4. Bascule du trafic
echo "üîÄ Bascule du trafic vers ${NEW_COLOR}..."
kubectl patch service ${APP_NAME} -p "{\"spec\":{\"selector\":{\"version\":\"${NEW_COLOR}\"}}}"

# 5. V√©rification post-bascule
echo "‚úÖ V√©rification..."
sleep 10
curl -f http://mon-app.example.com/health || {
  echo "‚ùå √âchec post-bascule! Rollback..."
  kubectl patch service ${APP_NAME} -p "{\"spec\":{\"selector\":{\"version\":\"${CURRENT_COLOR}\"}}}"
  exit 1
}

echo "üéâ D√©ploiement Blue-Green r√©ussi!"
echo "üí° Pour nettoyer l'ancienne version: kubectl scale deployment/${APP_NAME}-${CURRENT_COLOR} --replicas=0"
```

**Utilisation** :

```bash
./blue-green-deploy.sh v2.0.0 blue green
```

### Avantages et inconv√©nients

**‚úÖ Avantages** :
- Rollback **instantan√©** (basculer vers l'ancienne version)
- Tests complets sur Green avant bascule
- Pas de versions mixtes en production
- Downtime minimal (quelques secondes max pour la bascule)

**‚ùå Inconv√©nients** :
- **Co√ªt** : double des ressources (2x pods)
- Complexit√© de mise en ≈ìuvre
- N√©cessite synchronisation des donn√©es si stateful
- Gestion de deux environnements simultan√©s

**Cas d'usage** :
- Applications critiques n√©cessitant rollback instantan√©
- D√©ploiements √† fort enjeu
- Validation compl√®te n√©cessaire avant production
- Budget permettant le doublement des ressources

---

## Strat√©gie 4 : Canary Deployment

### Principe

Le d√©ploiement **Canary** expose progressivement la nouvelle version √† un pourcentage croissant d'utilisateurs, permettant de valider en conditions r√©elles avant g√©n√©ralisation.

**Origine** : Le nom vient des canaris dans les mines de charbon, utilis√©s pour d√©tecter les gaz toxiques avant qu'ils n'affectent les mineurs.

**Workflow** :

```
Phase 1:  v1 (95%)  v2 (5%)   ‚Üê 5% du trafic sur v2
          ‚Üì
Phase 2:  v1 (80%)  v2 (20%)  ‚Üê Si OK, 20%
          ‚Üì
Phase 3:  v1 (50%)  v2 (50%)  ‚Üê Si OK, 50%
          ‚Üì
Phase 4:  v1 (0%)   v2 (100%) ‚Üê Si OK, 100%
```

### Impl√©mentation manuelle avec Kubernetes

**D√©ploiements stable et canary** :

```yaml
---
# deployment-stable.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mon-app-stable
spec:
  replicas: 9  # 90% du trafic
  selector:
    matchLabels:
      app: mon-app
      track: stable
  template:
    metadata:
      labels:
        app: mon-app
        track: stable
        version: v1.0.0
    spec:
      containers:
      - name: app
        image: localhost:32000/mon-app:v1.0.0
        ports:
        - containerPort: 8080
---
# deployment-canary.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mon-app-canary
spec:
  replicas: 1  # 10% du trafic
  selector:
    matchLabels:
      app: mon-app
      track: canary
  template:
    metadata:
      labels:
        app: mon-app
        track: canary
        version: v2.0.0
    spec:
      containers:
      - name: app
        image: localhost:32000/mon-app:v2.0.0
        ports:
        - containerPort: 8080
---
# service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mon-app
spec:
  selector:
    app: mon-app  # S√©lectionne stable ET canary
  ports:
  - port: 80
    targetPort: 8080
```

**Progression manuelle** :

```bash
# Phase 1 : 10% canary
kubectl apply -f deployment-stable.yaml  # 9 replicas
kubectl apply -f deployment-canary.yaml  # 1 replica

# Attendre et surveiller les m√©triques
sleep 300

# Phase 2 : 20% canary
kubectl scale deployment/mon-app-stable --replicas=8
kubectl scale deployment/mon-app-canary --replicas=2

# Phase 3 : 50% canary
kubectl scale deployment/mon-app-stable --replicas=5
kubectl scale deployment/mon-app-canary --replicas=5

# Phase 4 : 100% canary
kubectl scale deployment/mon-app-stable --replicas=0
kubectl scale deployment/mon-app-canary --replicas=10
```

### Impl√©mentation avanc√©e avec Flagger

**Flagger** automatise les d√©ploiements Canary avec analyse de m√©triques.

**Installation** :

```bash
# Ajouter le repo Helm
helm repo add flagger https://flagger.app

# Installer Flagger
helm upgrade -i flagger flagger/flagger \
  --namespace flagger-system \
  --create-namespace \
  --set meshProvider=nginx \
  --set metricsServer=http://prometheus:9090
```

**D√©finition d'un Canary** :

```yaml
apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: mon-app
  namespace: production
spec:
  # Deployment √† g√©rer
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mon-app

  # Service
  service:
    port: 80
    targetPort: 8080

  # Analyse des m√©triques
  analysis:
    # Intervalle entre chaque √©tape
    interval: 1m
    # Seuil avant rollback automatique
    threshold: 5
    # Nombre d'it√©rations
    iterations: 10

    # M√©triques Prometheus √† surveiller
    metrics:
    - name: request-success-rate
      thresholdRange:
        min: 99
      interval: 1m
    - name: request-duration
      thresholdRange:
        max: 500
      interval: 1m

    # Webhooks pour tests (optionnel)
    webhooks:
    - name: smoke-test
      url: http://flagger-loadtester/
      timeout: 5s
      metadata:
        type: cmd
        cmd: "curl -sd 'test' http://mon-app-canary/health"
    - name: load-test
      url: http://flagger-loadtester/
      timeout: 5s
      metadata:
        cmd: "hey -z 1m -q 10 -c 2 http://mon-app-canary/"

  # Progression du Canary
  canaryAnalysis:
    stepWeight: 10  # Augmente de 10% √† chaque √©tape
    maxWeight: 50   # Maximum 50%
```

**D√©ployer une nouvelle version** :

```bash
# Simplement mettre √† jour le Deployment
kubectl set image deployment/mon-app app=localhost:32000/mon-app:v2.0.0

# Flagger d√©tecte le changement et commence automatiquement :
# 1. Cr√©e un d√©ploiement canary
# 2. Envoie 10% du trafic
# 3. Analyse les m√©triques pendant 1 minute
# 4. Si OK, augmente √† 20%
# 5. R√©p√®te jusqu'√† 50%
# 6. Promeut la version si tout va bien
# 7. OU fait un rollback automatique si probl√®me
```

**Surveiller** :

```bash
# Statut du Canary
kubectl get canary -w

# √âv√©nements d√©taill√©s
kubectl describe canary mon-app

# Exemple de sortie :
# Events:
#   Type     Reason  Age   From     Message
#   ----     ------  ----  ----     -------
#   Normal   Synced  3m    flagger  New revision detected! Scaling up mon-app.production
#   Normal   Synced  2m    flagger  Starting canary analysis for mon-app.production
#   Normal   Synced  1m    flagger  Advance mon-app.production canary weight 10
#   Warning  Synced  30s   flagger  Halt advancement no values found for metric request-success-rate
```

### Avantages et inconv√©nients

**‚úÖ Avantages** :
- **Risque minimal** : seul un petit % d'utilisateurs est impact√©
- Validation en conditions r√©elles
- Rollback automatique possible (avec Flagger)
- Permet de tester progressivement

**‚ùå Inconv√©nients** :
- Complexit√© de mise en ≈ìuvre
- N√©cessite du monitoring et des m√©triques
- D√©ploiement plus long (progressif)
- Versions mixtes en production

**Cas d'usage** :
- Fonctionnalit√©s √† risque
- Refonte majeure d'une application
- Changement d'algorithme sensible
- Nouvelle version avec incertitude

---

## Strat√©gie 5 : A/B Testing

### Principe

L'**A/B Testing** est similaire au Canary, mais avec un objectif diff√©rent : tester **deux variantes** d'une fonctionnalit√© pour mesurer laquelle performe le mieux.

**Diff√©rence avec Canary** :
- **Canary** : Valider qu'une nouvelle version fonctionne (pas de bug)
- **A/B Test** : Comparer deux versions pour choisir la meilleure (business metrics)

**Exemple** : Tester deux designs de bouton d'achat pour voir lequel convertit le mieux.

### Impl√©mentation avec Ingress NGINX

**D√©ploiements A et B** :

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mon-app-variant-a
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mon-app
      variant: a
  template:
    metadata:
      labels:
        app: mon-app
        variant: a
    spec:
      containers:
      - name: app
        image: localhost:32000/mon-app:variant-a
        env:
        - name: VARIANT
          value: "A"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mon-app-variant-b
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mon-app
      variant: b
  template:
    metadata:
      labels:
        app: mon-app
        variant: b
    spec:
      containers:
      - name: app
        image: localhost:32000/mon-app:variant-b
        env:
        - name: VARIANT
          value: "B"
```

**Services** :

```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: mon-app-a
spec:
  selector:
    app: mon-app
    variant: a
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: mon-app-b
spec:
  selector:
    app: mon-app
    variant: b
  ports:
  - port: 80
    targetPort: 8080
```

**Ingress avec split de trafic** :

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: mon-app-ab
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "50"
spec:
  ingressClassName: nginx
  rules:
  - host: mon-app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: mon-app-a
            port:
              number: 80
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: mon-app-ab-canary
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "50"
spec:
  ingressClassName: nginx
  rules:
  - host: mon-app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: mon-app-b
            port:
              number: 80
```

### Routage bas√© sur les headers

```yaml
# Routage selon un header (ex: user type)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: mon-app-ab-header
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-by-header: "X-User-Type"
    nginx.ingress.kubernetes.io/canary-by-header-value: "premium"
spec:
  ingressClassName: nginx
  rules:
  - host: mon-app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: mon-app-b  # Users premium voient variant B
            port:
              number: 80
```

**Test** :

```bash
# Utilisateur normal ‚Üí variant A
curl http://mon-app.example.com/

# Utilisateur premium ‚Üí variant B
curl -H "X-User-Type: premium" http://mon-app.example.com/
```

---

## Strat√©gie 6 : Shadow Deployment (Traffic Mirroring)

### Principe

Le **Shadow Deployment** (ou traffic mirroring) envoie le trafic de production √† **deux versions simultan√©ment** :
- Version **stable** : re√ßoit et r√©pond aux requ√™tes (production)
- Version **shadow** : re√ßoit une copie des requ√™tes mais ses r√©ponses sont ignor√©es

**Int√©r√™t** : Tester la nouvelle version avec du trafic r√©el sans risque pour les utilisateurs.

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Requ√™te   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ                     ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  Version    ‚îÇ      ‚îÇ  Version    ‚îÇ
         ‚îÇ  Stable     ‚îÇ      ‚îÇ  Shadow     ‚îÇ
         ‚îÇ  (v1)       ‚îÇ      ‚îÇ  (v2)       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ                     ‚îÇ
         R√©ponse retourn√©e      R√©ponse ignor√©e
                ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   Client    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Impl√©mentation avec Istio

Istio facilite le traffic mirroring.

**Installation d'Istio sur MicroK8s** :

```bash
# T√©l√©charger Istio
curl -L https://istio.io/downloadIstio | sh -
cd istio-*
export PATH=$PWD/bin:$PATH

# Installer
istioctl install --set profile=demo -y

# Activer l'injection automatique
kubectl label namespace default istio-injection=enabled
```

**D√©ploiements** :

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mon-app-v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mon-app
      version: v1
  template:
    metadata:
      labels:
        app: mon-app
        version: v1
    spec:
      containers:
      - name: app
        image: localhost:32000/mon-app:v1.0.0
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mon-app-v2
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mon-app
      version: v2
  template:
    metadata:
      labels:
        app: mon-app
        version: v2
    spec:
      containers:
      - name: app
        image: localhost:32000/mon-app:v2.0.0
```

**VirtualService avec mirroring** :

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: mon-app
spec:
  hosts:
  - mon-app
  http:
  - route:
    - destination:
        host: mon-app
        subset: v1
      weight: 100
    mirror:
      host: mon-app
      subset: v2
    mirrorPercentage:
      value: 100.0  # Miroir 100% du trafic
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: mon-app
spec:
  host: mon-app
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

**R√©sultat** :
- Tous les utilisateurs re√ßoivent des r√©ponses de v1
- v2 re√ßoit une copie de toutes les requ√™tes
- Vous pouvez analyser les logs/m√©triques de v2 pour d√©tecter des probl√®mes

### Avantages et inconv√©nients

**‚úÖ Avantages** :
- **Zero-risque** : la nouvelle version n'impacte pas les utilisateurs
- Test avec trafic r√©el de production
- Permet de comparer les performances
- D√©tection de bugs avant mise en production

**‚ùå Inconv√©nients** :
- N√©cessite un service mesh (Istio, Linkerd)
- Double les ressources de compute
- Pas de test des effets de bord (√©criture DB par exemple)
- Complexit√© de mise en ≈ìuvre

**Cas d'usage** :
- Refonte majeure d'un algorithme
- Migration de stack technique
- Validation de performances

---

## Comparaison et choix de strat√©gie

### Arbre de d√©cision

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Downtime acceptable ?           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  OUI    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ RECREATE
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  NON    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Budget pour doubler ressources? ‚îÇ
‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚îÇ       ‚îÇ
  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  ‚îÇ  OUI    ‚îÇ
  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚îÇ       ‚îÇ
  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  ‚îÇ Besoin de rollback instantan√©? ‚îÇ
  ‚îÇ  ‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚îÇ    ‚îÇ    ‚îÇ
  ‚îÇ    ‚îÇ  ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ    ‚îÇ  ‚îÇ  OUI    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ BLUE-GREEN ‚îÇ
  ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚îÇ  ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  ‚îÇ  NON    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ CANARY
  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚îÇ
‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  NON    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ROLLING UPDATE
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Tableau r√©capitulatif d√©taill√©

| Crit√®re | Recreate | Rolling | Blue-Green | Canary | A/B Test |
|---------|----------|---------|------------|--------|----------|
| **Downtime** | ‚ö†Ô∏è Oui | ‚úÖ Non | ‚úÖ Non | ‚úÖ Non | ‚úÖ Non |
| **Rollback** | Lent | Rapide | Instantan√© | Automatique | Rapide |
| **Ressources** | 1x | 1.2x | 2x | 1.3x | 1.5x |
| **Complexit√©** | ‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Dur√©e** | Rapide | Moyenne | Rapide | Lente | Variable |
| **Testing** | Aucun | Basique | Complet | Progressif | M√©trique |
| **Outils n√©cessaires** | Aucun | Natif K8s | Natif K8s | Flagger | Istio/Nginx |

---

## Bonnes pratiques

### 1. Toujours d√©finir des readinessProbe

```yaml
readinessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
  failureThreshold: 3
```

**Pourquoi** : Kubernetes n'envoie du trafic qu'aux pods "ready", √©vitant les erreurs pendant le d√©ploiement.

### 2. Utiliser PodDisruptionBudget

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: mon-app-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: mon-app
```

**Pourquoi** : Garantit qu'au moins 2 pods sont toujours disponibles, m√™me pendant les mises √† jour.

### 3. Tester en staging d'abord

```bash
# Toujours d√©ployer en staging avant production
helm upgrade mon-app ./chart -f values-staging.yaml -n staging
# Tests...
helm upgrade mon-app ./chart -f values-production.yaml -n production
```

### 4. Automatiser les smoke tests

```bash
#!/bin/bash
# smoke-test.sh

# Attendre que les pods soient pr√™ts
kubectl rollout status deployment/mon-app

# Tests de base
curl -f http://mon-app.example.com/health || exit 1
curl -f http://mon-app.example.com/api/version || exit 1

# Test de charge l√©ger
ab -n 100 -c 10 http://mon-app.example.com/ || exit 1

echo "‚úÖ Smoke tests r√©ussis"
```

### 5. Monitorer activement pendant le d√©ploiement

```yaml
# Cr√©er des alertes Prometheus pour d√©ploiements
- alert: DeploymentHighErrorRate
  expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
  for: 2m
  annotations:
    summary: "Taux d'erreur √©lev√© pendant le d√©ploiement"
```

### 6. Documenter la strat√©gie

```yaml
# Dans le Chart.yaml ou README
annotations:
  deployment-strategy: blue-green
  rollback-procedure: "kubectl patch service mon-app -p '{\"spec\":{\"selector\":{\"version\":\"blue\"}}}'"
```

### 7. Versionner s√©mantiquement

```bash
# Tags Git clairs
git tag -a v2.1.0 -m "Feature: nouveau dashboard"
git push origin v2.1.0

# Images Docker avec version
docker build -t mon-app:v2.1.0 .
docker tag mon-app:v2.1.0 mon-app:2.1
docker tag mon-app:v2.1.0 mon-app:2
```

### 8. Planifier les rollbacks

```bash
# Avoir un runbook de rollback
# 1. Identifier la version stable
kubectl rollout history deployment/mon-app

# 2. Rollback
kubectl rollout undo deployment/mon-app --to-revision=3

# 3. V√©rifier
kubectl rollout status deployment/mon-app

# 4. Alerter l'√©quipe
```

---

## R√©capitulatif

### Points cl√©s

**Les strat√©gies de d√©ploiement, c'est** :
- Minimiser les risques lors des mises √† jour
- Offrir diff√©rents niveaux de s√©curit√© et rapidit√©
- Permettre le rollback rapide
- Adapter la strat√©gie au contexte

**Strat√©gies principales** :

1. **Recreate** : Simple, downtime, dev/test
2. **Rolling Update** : Standard, zero-downtime, production
3. **Blue-Green** : Rollback instantan√©, co√ªteux, critique
4. **Canary** : Progressif, validation r√©elle, risqu√©
5. **A/B Testing** : Comparaison m√©trique, business
6. **Shadow** : Zero-risque, test r√©el, complexe

**Choix de strat√©gie** :
- Production standard ‚Üí **Rolling Update**
- Application critique ‚Üí **Blue-Green**
- Fonctionnalit√© risqu√©e ‚Üí **Canary**
- Test de feature ‚Üí **A/B Testing**
- Dev/Staging ‚Üí **Recreate** ou **Rolling**

**Outils** :
- Kubernetes natif : Recreate, Rolling, Blue-Green
- Flagger : Canary automatis√©
- Istio : A/B Testing, Shadow
- Argo Rollouts : Alternative √† Flagger

### B√©n√©fices

Avec des strat√©gies de d√©ploiement adapt√©es :
- ‚úÖ D√©ploiements sans stress
- ‚úÖ Zero ou minimal downtime
- ‚úÖ Rollback rapide en cas de probl√®me
- ‚úÖ Validation progressive
- ‚úÖ R√©duction des risques
- ‚úÖ Confiance pour d√©ployer fr√©quemment

---

## Conclusion

Les strat√©gies de d√©ploiement sont la **derni√®re pi√®ce du puzzle DevOps**. Elles transforment les d√©ploiements d'√©v√©nements stressants en op√©rations routini√®res et s√©curis√©es.

**Votre progression** :

Vous √™tes parti de d√©ploiements manuels avec `kubectl apply`, et vous ma√Ætrisez maintenant :

1. **Git** : Versioning du code et des configs
2. **Registry** : Stockage des images
3. **CI/CD** : Automatisation des builds
4. **ArgoCD** : Synchronisation GitOps
5. **Helm** : Packaging des applications
6. **Tests** : Validation automatique
7. **Strat√©gies** : D√©ploiement s√©curis√©

**Le workflow complet** :

```
Code ‚Üí Git ‚Üí CI/CD ‚Üí Registry ‚Üí Helm Chart ‚Üí ArgoCD
                                                  ‚Üì
                                        Strat√©gie de d√©ploiement
                                                  ‚Üì
                                            Kubernetes
                                                  ‚Üì
                                            Monitoring
```

Avec ce workflow, vous pouvez :
- D√©ployer plusieurs fois par jour
- Rollback en quelques secondes
- Tester de nouvelles features en toute s√©curit√©
- Maintenir une qualit√© √©lev√©e
- Dormir tranquille

**F√©licitations !** üéâ

Vous avez compl√©t√© le chapitre DevOps et CI/CD. Vous disposez maintenant d'une infrastructure professionnelle compl√®te sur MicroK8s, avec toutes les pratiques et outils utilis√©s par les √©quipes les plus avanc√©es.

La suite de votre parcours vous attend dans les chapitres suivants sur la gestion des ressources, le scaling, la s√©curit√© et bien plus encore !

**Continuez l'aventure Kubernetes ! üöÄ**

‚è≠Ô∏è [Blue-Green deployments](/17-devops-et-cicd/10-blue-green-deployments.md)
