ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 17.11 Canary deployments

## Introduction

Dans la section prÃ©cÃ©dente, nous avons explorÃ© les dÃ©ploiements Blue-Green qui permettent un basculement instantanÃ© entre deux versions. Maintenant, nous allons dÃ©couvrir les **dÃ©ploiements Canary**, une stratÃ©gie qui privilÃ©gie la **progressivitÃ©** et la **validation en conditions rÃ©elles** plutÃ´t que le basculement brutal.

Le nom "Canary" vient d'une pratique historique dans les mines de charbon : les mineurs emmenaient des canaris avec eux car ces oiseaux sont trÃ¨s sensibles aux gaz toxiques. Si le canari montrait des signes de dÃ©tresse, c'Ã©tait le signal d'Ã©vacuer avant que les mineurs ne soient affectÃ©s.

En informatique, c'est le mÃªme principe : on expose la nouvelle version Ã  un **petit groupe d'utilisateurs** (le canari) pour dÃ©tecter les problÃ¨mes avant qu'ils n'impactent tout le monde. Si le "canari" survit et se comporte bien, on augmente progressivement l'exposition jusqu'Ã  remplacer complÃ¨tement l'ancienne version.

---

## Principe et philosophie

### Le concept Canary

Le dÃ©ploiement Canary consiste Ã  dÃ©ployer une nouvelle version **en parallÃ¨le** de l'ancienne, puis Ã  router **progressivement** le trafic vers la nouvelle version en surveillant attentivement les mÃ©triques.

**Workflow typique** :

```
Phase 0 : 100% v1
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  v1 (100%)                             â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 1 : 95% v1, 5% v2 (Canary)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  v1 (95%)                    v2 (5%)   â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆ     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ Surveiller 5-10 minutes

Phase 2 : 80% v1, 20% v2
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  v1 (80%)               v2 (20%)       â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ Surveiller 10-15 minutes

Phase 3 : 50% v1, 50% v2
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  v1 (50%)          v2 (50%)            â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ Surveiller 15-30 minutes

Phase 4 : 0% v1, 100% v2 (Promotion)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            v2 (100%)   â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DiffÃ©rences clÃ©s avec Blue-Green

| Aspect | Blue-Green | Canary |
|--------|-----------|--------|
| **Transition** | InstantanÃ©e (switch) | Progressive (graduÃ©e) |
| **Risque** | Tous impactÃ©s en cas de bug | Seul un % est impactÃ© |
| **DurÃ©e** | Rapide (minutes) | Lente (heures) |
| **Validation** | Tests prÃ©-bascule | Validation continue |
| **Ressources** | 2x (temporaire) | 1.2-1.5x (progressif) |
| **Rollback** | InstantanÃ© | Automatique basÃ© sur mÃ©triques |
| **ComplexitÃ©** | Moyenne | Ã‰levÃ©e |

**Blue-Green** : "J'ai testÃ©, je suis confiant, on bascule tout d'un coup"

**Canary** : "Je ne suis pas 100% sÃ»r, testons d'abord avec quelques utilisateurs rÃ©els"

### Cas d'usage idÃ©aux

**âœ… Quand utiliser Canary** :

- Nouvelle fonctionnalitÃ© jamais testÃ©e en production rÃ©elle
- Refonte majeure d'un algorithme
- Changement de comportement incertain
- Application Ã  trÃ¨s fort trafic
- Besoin de valider les performances en condition rÃ©elle
- Incertitude sur la stabilitÃ©

**âŒ Quand ne pas utiliser Canary** :

- Simple correction de bug
- Application Ã  faible trafic
- Changements mineurs
- Urgence (hotfix critique)
- Migrations de schÃ©ma incompatibles

---

## ImplÃ©mentation manuelle avec Kubernetes

### Architecture de base

Pour un Canary manuel, on utilise deux Deployments avec un Service qui les sÃ©lectionne tous les deux :

```
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Service  â”‚
                     â”‚ (app=mon)  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Deployment       â”‚       â”‚  Deployment     â”‚
    â”‚  Stable           â”‚       â”‚  Canary         â”‚
    â”‚  (app=mon,        â”‚       â”‚  (app=mon,      â”‚
    â”‚   track=stable)   â”‚       â”‚   track=canary) â”‚
    â”‚                   â”‚       â”‚                 â”‚
    â”‚  Replicas: 9      â”‚       â”‚  Replicas: 1    â”‚
    â”‚  (90% traffic)    â”‚       â”‚  (10% traffic)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Manifestes Kubernetes

#### DÃ©ploiement stable

```yaml
---
# deployment-stable.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mon-app-stable
  namespace: production
  labels:
    app: mon-app
    track: stable
spec:
  replicas: 9  # 90% du trafic
  selector:
    matchLabels:
      app: mon-app
      track: stable
  template:
    metadata:
      labels:
        app: mon-app
        track: stable
        version: v1.0.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: app
        image: localhost:32000/mon-app:v1.0.0
        ports:
        - name: http
          containerPort: 8080
        env:
        - name: VERSION
          value: "v1.0.0"
        - name: TRACK
          value: "stable"
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 3
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
```

#### DÃ©ploiement Canary

```yaml
---
# deployment-canary.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mon-app-canary
  namespace: production
  labels:
    app: mon-app
    track: canary
spec:
  replicas: 1  # 10% du trafic initial
  selector:
    matchLabels:
      app: mon-app
      track: canary
  template:
    metadata:
      labels:
        app: mon-app
        track: canary
        version: v2.0.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: app
        image: localhost:32000/mon-app:v2.0.0
        ports:
        - name: http
          containerPort: 8080
        env:
        - name: VERSION
          value: "v2.0.0"
        - name: TRACK
          value: "canary"
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 3
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
```

#### Service

```yaml
---
# service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mon-app
  namespace: production
  labels:
    app: mon-app
spec:
  type: ClusterIP
  selector:
    app: mon-app
    # Note : Pas de selector sur 'track'
    # Le service sÃ©lectionne stable ET canary
  ports:
  - name: http
    port: 80
    targetPort: 8080
    protocol: TCP
```

### Script de progression manuelle

```bash
#!/bin/bash
# canary-manual.sh - Gestion manuelle d'un dÃ©ploiement Canary

set -euo pipefail

APP_NAME="mon-app"
NAMESPACE="production"
NEW_VERSION="${1:-}"

# Couleurs
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

log_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

if [ -z "$NEW_VERSION" ]; then
    log_error "Usage: $0 <version>"
    exit 1
fi

# Phase 1 : DÃ©ployer le Canary (10%)
log_info "Phase 1/5: DÃ©ploiement du Canary (10%)"
kubectl set image deployment/${APP_NAME}-canary \
    app=localhost:32000/${APP_NAME}:${NEW_VERSION} \
    -n ${NAMESPACE}

kubectl scale deployment/${APP_NAME}-stable --replicas=9 -n ${NAMESPACE}
kubectl scale deployment/${APP_NAME}-canary --replicas=1 -n ${NAMESPACE}

kubectl rollout status deployment/${APP_NAME}-canary -n ${NAMESPACE}
log_info "Canary dÃ©ployÃ©. Surveillance 5 minutes..."
sleep 300

# VÃ©rification des mÃ©triques (simulÃ©e ici)
read -p "Les mÃ©triques sont-elles bonnes? (yes/no): " -r
if [[ ! $REPLY =~ ^[Yy][Ee][Ss]$ ]]; then
    log_error "Rollback du Canary"
    kubectl scale deployment/${APP_NAME}-canary --replicas=0 -n ${NAMESPACE}
    exit 1
fi

# Phase 2 : 20%
log_info "Phase 2/5: Augmentation Ã  20%"
kubectl scale deployment/${APP_NAME}-stable --replicas=8 -n ${NAMESPACE}
kubectl scale deployment/${APP_NAME}-canary --replicas=2 -n ${NAMESPACE}
log_info "20% sur Canary. Surveillance 10 minutes..."
sleep 600

read -p "Les mÃ©triques sont-elles bonnes? (yes/no): " -r
if [[ ! $REPLY =~ ^[Yy][Ee][Ss]$ ]]; then
    log_error "Rollback du Canary"
    kubectl scale deployment/${APP_NAME}-stable --replicas=9 -n ${NAMESPACE}
    kubectl scale deployment/${APP_NAME}-canary --replicas=1 -n ${NAMESPACE}
    exit 1
fi

# Phase 3 : 50%
log_info "Phase 3/5: Augmentation Ã  50%"
kubectl scale deployment/${APP_NAME}-stable --replicas=5 -n ${NAMESPACE}
kubectl scale deployment/${APP_NAME}-canary --replicas=5 -n ${NAMESPACE}
log_info "50% sur Canary. Surveillance 15 minutes..."
sleep 900

read -p "Les mÃ©triques sont-elles bonnes? (yes/no): " -r
if [[ ! $REPLY =~ ^[Yy][Ee][Ss]$ ]]; then
    log_error "Rollback du Canary"
    kubectl scale deployment/${APP_NAME}-stable --replicas=9 -n ${NAMESPACE}
    kubectl scale deployment/${APP_NAME}-canary --replicas=1 -n ${NAMESPACE}
    exit 1
fi

# Phase 4 : 100% (Promotion)
log_info "Phase 4/5: Promotion Ã  100%"
kubectl scale deployment/${APP_NAME}-stable --replicas=0 -n ${NAMESPACE}
kubectl scale deployment/${APP_NAME}-canary --replicas=10 -n ${NAMESPACE}
log_info "100% sur Canary. Surveillance 30 minutes..."
sleep 1800

# Phase 5 : Finalisation
log_info "Phase 5/5: Finalisation"
read -p "Promouvoir dÃ©finitivement le Canary? (yes/no): " -r
if [[ $REPLY =~ ^[Yy][Ee][Ss]$ ]]; then
    # Mettre Ã  jour stable avec la nouvelle version
    kubectl set image deployment/${APP_NAME}-stable \
        app=localhost:32000/${APP_NAME}:${NEW_VERSION} \
        -n ${NAMESPACE}
    kubectl scale deployment/${APP_NAME}-stable --replicas=10 -n ${NAMESPACE}
    kubectl scale deployment/${APP_NAME}-canary --replicas=0 -n ${NAMESPACE}

    log_info "âœ… DÃ©ploiement Canary rÃ©ussi!"
else
    log_warn "Canary maintenu Ã  100% sans promotion"
fi
```

**Utilisation** :

```bash
chmod +x canary-manual.sh
./canary-manual.sh v2.0.0
```

---

## ImplÃ©mentation automatisÃ©e avec Flagger

### PrÃ©sentation de Flagger

**Flagger** est un outil de Weaveworks qui automatise complÃ¨tement les dÃ©ploiements Canary, incluant :

- Progression automatique du trafic
- Analyse des mÃ©triques Prometheus
- Rollback automatique en cas de problÃ¨me
- Tests de charge automatiques
- Notifications

**Architecture** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Flagger Controller                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Canary    â”‚â†’ â”‚ Analysis â”‚â†’ â”‚  Promotion/        â”‚  â”‚
â”‚  â”‚  Detection â”‚  â”‚  Engine  â”‚  â”‚  Rollback          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                  â”‚                    â”‚
       â–¼                  â–¼                    â–¼
  Deployment         Prometheus           Service/Ingress
```

### Installation de Flagger

```bash
# Ajouter le repo Helm
helm repo add flagger https://flagger.app

# Installer Flagger avec support Prometheus
kubectl create namespace flagger-system

helm upgrade -i flagger flagger/flagger \
    --namespace flagger-system \
    --set meshProvider=kubernetes \
    --set metricsServer=http://prometheus.monitoring:9090

# Installer Grafana (optionnel mais recommandÃ©)
helm upgrade -i flagger-grafana flagger/grafana \
    --namespace flagger-system \
    --set url=http://prometheus.monitoring:9090

# Installer le load tester (pour tests automatiques)
kubectl apply -k https://github.com/fluxcd/flagger//kustomize/tester?ref=main
```

### Configuration d'un Canary avec Flagger

#### Ressource Canary

```yaml
---
# canary.yaml
apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: mon-app
  namespace: production
spec:
  # Deployment cible Ã  gÃ©rer
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mon-app

  # Service Kubernetes
  service:
    port: 80
    targetPort: 8080
    name: mon-app
    portDiscovery: true

  # Configuration de l'analyse
  analysis:
    # Intervalle entre chaque Ã©tape
    interval: 1m

    # Seuil : nombre d'Ã©checs avant rollback
    threshold: 5

    # Nombre maximum d'itÃ©rations
    maxWeight: 50

    # Poids Ã  ajouter Ã  chaque itÃ©ration
    stepWeight: 10

    # MÃ©triques Prometheus Ã  surveiller
    metrics:

    # MÃ©trique 1 : Taux de succÃ¨s des requÃªtes
    - name: request-success-rate
      # Template prÃ©dÃ©fini par Flagger
      templateRef:
        name: request-success-rate
        namespace: flagger-system
      # Seuil minimum acceptable
      thresholdRange:
        min: 99
      interval: 1m

    # MÃ©trique 2 : DurÃ©e des requÃªtes
    - name: request-duration
      templateRef:
        name: request-duration
        namespace: flagger-system
      # Seuil maximum acceptable (en ms)
      thresholdRange:
        max: 500
      interval: 1m

    # MÃ©trique personnalisÃ©e 3 : Taux d'erreur
    - name: error-rate
      query: |
        100 - (
          sum(
            rate(
              http_requests_total{
                namespace="production",
                deployment="mon-app-canary",
                status!~"5.."
              }[1m]
            )
          )
          /
          sum(
            rate(
              http_requests_total{
                namespace="production",
                deployment="mon-app-canary"
              }[1m]
            )
          )
          * 100
        )
      thresholdRange:
        max: 1
      interval: 1m

    # Webhooks pour tests automatiques
    webhooks:

    # Pre-rollout : Tests avant de commencer
    - name: pre-rollout-tests
      type: pre-rollout
      url: http://flagger-loadtester.flagger-system/
      timeout: 15s
      metadata:
        type: bash
        cmd: |
          curl -sd 'test' http://mon-app-canary.production:80/ | grep -q OK

    # Rollout : Tests de charge pendant le dÃ©ploiement
    - name: load-test
      type: rollout
      url: http://flagger-loadtester.flagger-system/
      timeout: 5s
      metadata:
        type: cmd
        cmd: "hey -z 1m -q 10 -c 2 http://mon-app-canary.production:80/"

    # Post-rollout : Tests aprÃ¨s succÃ¨s
    - name: post-rollout-tests
      type: post-rollout
      url: http://flagger-loadtester.flagger-system/
      timeout: 15s
      metadata:
        type: bash
        cmd: |
          curl -sd 'test' http://mon-app.production:80/version | grep -q v2.0.0

    # Alertes en cas de problÃ¨me
    - name: send-alert
      type: event
      url: http://event-receiver.monitoring/
      metadata:
        alerts:
          - name: canary-failed
            severity: error
            message: "Canary deployment failed"
```

#### Templates de mÃ©triques personnalisÃ©s

```yaml
---
# metric-templates.yaml
apiVersion: flagger.app/v1beta1
kind: MetricTemplate
metadata:
  name: request-success-rate
  namespace: flagger-system
spec:
  provider:
    type: prometheus
    address: http://prometheus.monitoring:9090
  query: |
    sum(
      rate(
        http_requests_total{
          namespace="{{ namespace }}",
          deployment="{{ target }}",
          status!~"5.."
        }[{{ interval }}]
      )
    )
    /
    sum(
      rate(
        http_requests_total{
          namespace="{{ namespace }}",
          deployment="{{ target }}"
        }[{{ interval }}]
      )
    )
    * 100
---
apiVersion: flagger.app/v1beta1
kind: MetricTemplate
metadata:
  name: request-duration
  namespace: flagger-system
spec:
  provider:
    type: prometheus
    address: http://prometheus.monitoring:9090
  query: |
    histogram_quantile(
      0.99,
      sum(
        rate(
          http_request_duration_seconds_bucket{
            namespace="{{ namespace }}",
            deployment="{{ target }}"
          }[{{ interval }}]
        )
      ) by (le)
    )
    * 1000
---
apiVersion: flagger.app/v1beta1
kind: MetricTemplate
metadata:
  name: cpu-usage
  namespace: flagger-system
spec:
  provider:
    type: prometheus
    address: http://prometheus.monitoring:9090
  query: |
    sum(
      rate(
        container_cpu_usage_seconds_total{
          namespace="{{ namespace }}",
          pod=~"{{ target }}-.*"
        }[{{ interval }}]
      )
    )
```

### DÃ©clenchement d'un dÃ©ploiement Canary

Avec Flagger, dÃ©clencher un Canary est **extrÃªmement simple** :

```bash
# Il suffit de mettre Ã  jour l'image du Deployment
kubectl set image deployment/mon-app \
    app=localhost:32000/mon-app:v2.0.0 \
    -n production

# Flagger dÃ©tecte automatiquement le changement et :
# 1. CrÃ©e un dÃ©ploiement canary
# 2. Commence la progression automatique
# 3. Analyse les mÃ©triques Ã  chaque Ã©tape
# 4. Promeut ou rollback automatiquement
```

**Observer le processus** :

```bash
# Voir le statut du Canary
kubectl get canary -n production -w

# Exemple de sortie :
# NAME      STATUS        WEIGHT   LASTTRANSITIONTIME
# mon-app   Progressing   0        2024-01-15T10:00:00Z
# mon-app   Progressing   10       2024-01-15T10:01:00Z
# mon-app   Progressing   20       2024-01-15T10:02:00Z
# mon-app   Progressing   30       2024-01-15T10:03:00Z
# mon-app   Progressing   40       2024-01-15T10:04:00Z
# mon-app   Progressing   50       2024-01-15T10:05:00Z
# mon-app   Promoting     50       2024-01-15T10:06:00Z
# mon-app   Succeeded     0        2024-01-15T10:07:00Z

# Voir les events dÃ©taillÃ©s
kubectl describe canary mon-app -n production

# Logs du controller Flagger
kubectl logs -n flagger-system deployment/flagger -f
```

### Timeline d'un dÃ©ploiement Canary avec Flagger

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T+0min : DÃ©tection de la nouvelle version                   â”‚
â”‚   - Flagger dÃ©tecte le changement d'image                   â”‚
â”‚   - CrÃ©e un nouveau ReplicaSet canary                       â”‚
â”‚   - Status: Initializing                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T+1min : Pre-rollout tests                                  â”‚
â”‚   - ExÃ©cute les webhooks pre-rollout                        â”‚
â”‚   - Si Ã©chec â†’ ABORT                                        â”‚
â”‚   - Status: Initialized                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T+2min : Canary 10%                                         â”‚
â”‚   - Route 10% du trafic vers canary                         â”‚
â”‚   - Lance les load tests                                    â”‚
â”‚   - Analyse les mÃ©triques pendant 1min                      â”‚
â”‚   - Si Ã©chec > threshold â†’ ROLLBACK                         â”‚
â”‚   - Status: Progressing (weight: 10)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T+3min : Canary 20%                                         â”‚
â”‚   - Route 20% du trafic vers canary                         â”‚
â”‚   - Analyse continue                                        â”‚
â”‚   - Status: Progressing (weight: 20)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ... Continue jusqu'Ã  maxWeight (50%) ...                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T+6min : Canary 50%                                         â”‚
â”‚   - Route 50% du trafic vers canary                         â”‚
â”‚   - DerniÃ¨re validation des mÃ©triques                       â”‚
â”‚   - Status: Progressing (weight: 50)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T+7min : Promotion                                          â”‚
â”‚   - Remplace la version stable par canary                   â”‚
â”‚   - Met Ã  jour le primary deployment                        â”‚
â”‚   - Status: Promoting                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T+8min : Post-rollout tests                                 â”‚
â”‚   - ExÃ©cute les tests post-rollout                          â”‚
â”‚   - Finalisation                                            â”‚
â”‚   - Status: Succeeded                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Rollback automatique

Si les mÃ©triques dÃ©passent les seuils, Flagger effectue un **rollback automatique** :

```
T+0min : Canary 10%
         MÃ©triques OK
         â†“
T+1min : Canary 20%
         MÃ©triques OK
         â†“
T+2min : Canary 30%
         âŒ Error rate: 5% (seuil: 1%)
         âŒ Ã‰chec 1/5
         â†“
T+3min : Canary 30% (retry)
         âŒ Error rate: 6%
         âŒ Ã‰chec 2/5
         â†“
T+4min : Canary 30% (retry)
         âŒ Error rate: 7%
         âŒ Ã‰chec 3/5
         â†“
         ... continues jusqu'Ã  5 Ã©checs ...
         â†“
T+7min : ğŸš¨ ROLLBACK AUTOMATIQUE
         - Trafic retournÃ© Ã  100% vers stable
         - Canary supprimÃ©
         - Status: Failed
         - Alertes envoyÃ©es
```

---

## MÃ©triques et observabilitÃ©

### MÃ©triques essentielles

Pour un Canary rÃ©ussi, vous devez surveiller ces mÃ©triques clÃ©s :

#### 1. Taux de succÃ¨s des requÃªtes

```promql
# Pourcentage de requÃªtes rÃ©ussies (non 5xx)
sum(rate(http_requests_total{status!~"5..", deployment="mon-app-canary"}[1m]))
/
sum(rate(http_requests_total{deployment="mon-app-canary"}[1m]))
* 100
```

**Seuil recommandÃ©** : > 99%

#### 2. Latence P95/P99

```promql
# 95Ã¨me percentile de latence
histogram_quantile(0.95,
  sum(rate(http_request_duration_seconds_bucket{deployment="mon-app-canary"}[1m]))
  by (le)
)
```

**Seuil recommandÃ©** : < 500ms

#### 3. Taux d'erreur

```promql
# Pourcentage d'erreurs
sum(rate(http_requests_total{status=~"5..", deployment="mon-app-canary"}[1m]))
/
sum(rate(http_requests_total{deployment="mon-app-canary"}[1m]))
* 100
```

**Seuil recommandÃ©** : < 1%

#### 4. Utilisation des ressources

```promql
# CPU usage
sum(rate(container_cpu_usage_seconds_total{pod=~"mon-app-canary-.*"}[1m]))

# Memory usage
sum(container_memory_working_set_bytes{pod=~"mon-app-canary-.*"})
```

**Seuil recommandÃ©** : Pas plus de 20% d'augmentation par rapport Ã  stable

### Dashboard Grafana pour Canary

```json
{
  "dashboard": {
    "title": "Canary Deployment Monitoring",
    "rows": [
      {
        "panels": [
          {
            "title": "Canary Status",
            "targets": [
              {
                "expr": "flagger_canary_status{name='mon-app'}",
                "legendFormat": "Status: {{ status }}"
              }
            ]
          },
          {
            "title": "Traffic Weight",
            "targets": [
              {
                "expr": "flagger_canary_weight{name='mon-app'}",
                "legendFormat": "Canary Weight"
              }
            ]
          }
        ]
      },
      {
        "panels": [
          {
            "title": "Request Success Rate",
            "targets": [
              {
                "expr": "sum(rate(http_requests_total{status!~'5..',deployment='mon-app-canary'}[1m])) / sum(rate(http_requests_total{deployment='mon-app-canary'}[1m])) * 100",
                "legendFormat": "Canary"
              },
              {
                "expr": "sum(rate(http_requests_total{status!~'5..',deployment='mon-app-stable'}[1m])) / sum(rate(http_requests_total{deployment='mon-app-stable'}[1m])) * 100",
                "legendFormat": "Stable"
              }
            ],
            "alert": {
              "conditions": [
                {
                  "evaluator": {
                    "params": [99],
                    "type": "lt"
                  }
                }
              ]
            }
          },
          {
            "title": "Request Duration P95",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{deployment='mon-app-canary'}[1m])) by (le))",
                "legendFormat": "Canary P95"
              },
              {
                "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{deployment='mon-app-stable'}[1m])) by (le))",
                "legendFormat": "Stable P95"
              }
            ]
          }
        ]
      },
      {
        "panels": [
          {
            "title": "Error Rate",
            "targets": [
              {
                "expr": "sum(rate(http_requests_total{status=~'5..',deployment='mon-app-canary'}[1m])) / sum(rate(http_requests_total{deployment='mon-app-canary'}[1m])) * 100",
                "legendFormat": "Canary Errors"
              },
              {
                "expr": "sum(rate(http_requests_total{status=~'5..',deployment='mon-app-stable'}[1m])) / sum(rate(http_requests_total{deployment='mon-app-stable'}[1m])) * 100",
                "legendFormat": "Stable Errors"
              }
            ]
          },
          {
            "title": "Request Rate",
            "targets": [
              {
                "expr": "sum(rate(http_requests_total{deployment='mon-app-canary'}[1m]))",
                "legendFormat": "Canary RPS"
              },
              {
                "expr": "sum(rate(http_requests_total{deployment='mon-app-stable'}[1m]))",
                "legendFormat": "Stable RPS"
              }
            ]
          }
        ]
      }
    ]
  }
}
```

### Alertes Prometheus

```yaml
# prometheus-alerts.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: canary-alerts
  namespace: monitoring
spec:
  groups:
  - name: canary-deployment
    interval: 30s
    rules:

    # Alerte si le Canary Ã©choue
    - alert: CanaryDeploymentFailed
      expr: flagger_canary_status{status="failed"} == 1
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Canary deployment failed for {{ $labels.name }}"
        description: "Canary deployment has failed and rolled back"

    # Alerte si le taux d'erreur est Ã©levÃ© sur le Canary
    - alert: CanaryHighErrorRate
      expr: |
        (
          sum(rate(http_requests_total{status=~"5..", track="canary"}[1m]))
          /
          sum(rate(http_requests_total{track="canary"}[1m]))
        ) > 0.01
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High error rate on canary deployment"
        description: "Canary has {{ $value | humanizePercentage }} error rate"

    # Alerte si la latence est trop Ã©levÃ©e
    - alert: CanaryHighLatency
      expr: |
        histogram_quantile(0.95,
          sum(rate(http_request_duration_seconds_bucket{track="canary"}[1m]))
          by (le)
        ) > 0.5
      for: 3m
      labels:
        severity: warning
      annotations:
        summary: "High latency on canary deployment"
        description: "Canary P95 latency is {{ $value }}s"

    # Alerte si le Canary progresse (informatif)
    - alert: CanaryProgressing
      expr: flagger_canary_status{status="progressing"} == 1
      labels:
        severity: info
      annotations:
        summary: "Canary deployment in progress for {{ $labels.name }}"
        description: "Current weight: {{ $labels.weight }}%"
```

---

## Patterns avancÃ©s

### Pattern 1 : Canary avec header-based routing

Router vers le Canary basÃ© sur un header HTTP (pour tests ciblÃ©s).

```yaml
# Avec Istio
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: mon-app
spec:
  hosts:
  - mon-app.example.com
  http:
  # Route 1 : Header spÃ©cifique â†’ Canary
  - match:
    - headers:
        x-canary:
          exact: "true"
    route:
    - destination:
        host: mon-app
        subset: canary

  # Route 2 : Utilisateurs premium â†’ Canary
  - match:
    - headers:
        x-user-type:
          exact: "premium"
    route:
    - destination:
        host: mon-app
        subset: canary
      weight: 100

  # Route 3 : Traffic normal â†’ Split
  - route:
    - destination:
        host: mon-app
        subset: stable
      weight: 90
    - destination:
        host: mon-app
        subset: canary
      weight: 10
```

**Usage** :

```bash
# Tester le Canary directement
curl -H "x-canary: true" https://app.example.com/

# Utilisateur normal (10% chance de canary)
curl https://app.example.com/
```

### Pattern 2 : Canary par rÃ©gion gÃ©ographique

DÃ©ployer le Canary dans une rÃ©gion avant les autres.

```yaml
# Canary pour la rÃ©gion EU seulement
apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: mon-app-eu
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mon-app-eu

  analysis:
    interval: 5m
    threshold: 3
    maxWeight: 50
    stepWeight: 25

    # MÃ©triques filtrÃ©es par rÃ©gion
    metrics:
    - name: request-success-rate
      query: |
        sum(rate(http_requests_total{region="eu", status!~"5.."}[1m]))
        /
        sum(rate(http_requests_total{region="eu"}[1m]))
        * 100
      thresholdRange:
        min: 99
```

**Workflow** :

1. DÃ©ployer Canary en EU
2. Si succÃ¨s â†’ DÃ©ployer en US
3. Si succÃ¨s â†’ DÃ©ployer en ASIA

### Pattern 3 : Canary avec mirroring (Shadow traffic)

Combiner Canary avec traffic mirroring pour valider sans risque.

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: mon-app
spec:
  hosts:
  - mon-app
  http:
  - route:
    - destination:
        host: mon-app
        subset: stable
      weight: 100
    mirror:
      host: mon-app
      subset: canary
    mirrorPercentage:
      value: 100.0
```

**BÃ©nÃ©fice** : Le Canary reÃ§oit 100% du trafic en copie, mais les rÃ©ponses sont ignorÃ©es. Validation des performances sans risque.

### Pattern 4 : Multi-stage Canary

Plusieurs phases avec des seuils diffÃ©rents.

```yaml
apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: mon-app
spec:
  analysis:
    interval: 1m
    threshold: 5

    # Phase 1 : 0-10% (seuils stricts)
    metrics:
    - name: early-stage-success-rate
      query: |
        sum(rate(http_requests_total{status!~"5.."}[1m]))
        / sum(rate(http_requests_total[1m])) * 100
      thresholdRange:
        min: 99.5  # TrÃ¨s strict au dÃ©but
      interval: 1m

    # Phase 2 : 10-50% (seuils normaux)
    - name: late-stage-success-rate
      query: |
        sum(rate(http_requests_total{status!~"5.."}[5m]))
        / sum(rate(http_requests_total[5m])) * 100
      thresholdRange:
        min: 99  # Un peu plus tolÃ©rant
      interval: 1m

    stepWeightPromotion: 10  # Commencer avec 10%
    maxWeight: 50
    stepWeight: 10
```

---

## IntÃ©gration avec GitOps (ArgoCD + Flagger)

### Architecture GitOps Canary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Git      â”‚  Version source de vÃ©ritÃ©
â”‚  Repository  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ git push
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ArgoCD     â”‚  Synchronise les manifestes
â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ kubectl apply
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kubernetes      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Deployment  â”‚  â”‚  DÃ©tecte changement
â”‚  â”‚   (v2.0)    â”‚  â”‚         â†“
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   â”‚  â”‚   Flagger    â”‚
â”‚                   â”‚  â”‚  Controller  â”‚
â”‚                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                   â”‚         â†“
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  GÃ¨re le Canary
â”‚  â”‚   Canary    â”‚  â”‚  automatiquement
â”‚  â”‚ Deployment  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configuration ArgoCD

```yaml
# argocd-application.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: mon-app
  namespace: argocd
spec:
  project: default

  source:
    repoURL: https://github.com/mon-org/mon-app.git
    targetRevision: main
    path: k8s/

  destination:
    server: https://kubernetes.default.svc
    namespace: production

  syncPolicy:
    automated:
      prune: false  # Important : Ne pas supprimer le canary
      selfHeal: true
    syncOptions:
    - CreateNamespace=true

    # Ignorer les diffÃ©rences gÃ©rÃ©es par Flagger
    ignoreDifferences:
    - group: apps
      kind: Deployment
      jsonPointers:
      - /spec/replicas
      - /spec/template/spec/containers/0/image
```

### Workflow complet

```bash
# 1. Developer push une nouvelle version
git add deployment.yaml
git commit -m "Update to v2.0.0"
git push origin main

# 2. ArgoCD dÃ©tecte le changement et synchronise
# (peut Ãªtre automatique ou manuel selon config)

# 3. ArgoCD met Ã  jour le Deployment avec la nouvelle image
kubectl set image deployment/mon-app app=mon-app:v2.0.0

# 4. Flagger dÃ©tecte le changement
#    â†’ CrÃ©e automatiquement le Canary
#    â†’ Lance la progression automatique
#    â†’ Analyse les mÃ©triques
#    â†’ Promeut ou rollback

# 5. Monitoring dans ArgoCD
argocd app get mon-app

# 6. Monitoring dans Flagger
kubectl get canary mon-app -w
```

---

## Bonnes pratiques

### 1. DÃ©finir des SLIs/SLOs clairs

**SLI** (Service Level Indicator) : MÃ©trique mesurable
**SLO** (Service Level Objective) : Objectif pour cette mÃ©trique

```yaml
# Exemple de SLOs pour un Canary
SLOs:
  - metric: availability
    threshold: 99.9%

  - metric: latency_p95
    threshold: 500ms

  - metric: error_rate
    threshold: 0.1%

  - metric: saturation_cpu
    threshold: 80%
```

### 2. Commencer conservateur

```yaml
# PremiÃ¨re fois : TrÃ¨s conservateur
analysis:
  interval: 5m      # Long intervalle
  threshold: 3      # Peu tolÃ©rant
  maxWeight: 20     # Seulement 20%
  stepWeight: 5     # Petites Ã©tapes

# AprÃ¨s confiance : Plus agressif
analysis:
  interval: 1m
  threshold: 5
  maxWeight: 50
  stepWeight: 10
```

### 3. Tester les webhooks

```bash
# Tester manuellement les webhooks avant utilisation
kubectl run test-webhook --rm -it --image=curlimages/curl -- \
  curl -d '{"name":"test"}' \
  http://flagger-loadtester.flagger-system/
```

### 4. Documenter les dÃ©cisions

```yaml
metadata:
  annotations:
    canary-strategy: "progressive-10-50"
    canary-reason: "Major algorithm refactoring"
    canary-started-by: "john.doe@example.com"
    canary-started-at: "2024-01-15T10:00:00Z"
```

### 5. Alerter l'Ã©quipe

```yaml
webhooks:
- name: notify-slack
  type: event
  url: https://hooks.slack.com/services/YOUR/WEBHOOK/URL
  metadata:
    type: slack
    channel: "#deployments"
    username: "Flagger"
```

### 6. Avoir un plan de rollback manuel

```bash
# Script de rollback d'urgence
#!/bin/bash
# rollback-canary.sh

kubectl patch canary mon-app -n production \
  --type=json \
  -p='[{"op": "replace", "path": "/spec/analysis/threshold", "value": 0}]'

# Force le rollback en mettant threshold Ã  0
```

### 7. Monitorer les coÃ»ts

Le Canary double temporairement les ressources :

```yaml
# Calculer le coÃ»t
# Stable: 10 pods Ã— 0.5 CPU = 5 CPU
# Canary:  5 pods Ã— 0.5 CPU = 2.5 CPU (Ã  50%)
# Total: 7.5 CPU pendant 30 minutes
# SurcoÃ»t: 50% pendant la durÃ©e du Canary
```

---

## Troubleshooting

### ProblÃ¨me 1 : Canary bloquÃ© en "Initializing"

**SymptÃ´mes** :
```bash
kubectl get canary
# NAME      STATUS         WEIGHT
# mon-app   Initializing   0
```

**Causes possibles** :
- Service ou Deployment introuvable
- MÃ©triques Prometheus non disponibles
- Webhooks qui Ã©chouent

**Diagnostic** :
```bash
kubectl describe canary mon-app
kubectl logs -n flagger-system deployment/flagger
```

**Solution** :
```bash
# VÃ©rifier le service
kubectl get svc mon-app

# VÃ©rifier Prometheus
kubectl run test --rm -it --image=curlimages/curl -- \
  curl http://prometheus.monitoring:9090/-/healthy

# Tester les webhooks
kubectl run test --rm -it --image=curlimages/curl -- \
  curl http://flagger-loadtester.flagger-system/
```

### ProblÃ¨me 2 : Rollback intempestif

**SymptÃ´mes** :
Le Canary rollback alors que tout semble OK.

**Causes** :
- Seuils trop stricts
- MÃ©triques incorrectes
- Pas assez de trafic pour des mÃ©triques fiables

**Solution** :
```yaml
# Ajuster les seuils
analysis:
  threshold: 10  # Plus tolÃ©rant

  metrics:
  - name: request-success-rate
    thresholdRange:
      min: 95  # Moins strict (au lieu de 99)
```

### ProblÃ¨me 3 : MÃ©triques non disponibles

**SymptÃ´mes** :
```
Halt advancement: no values found for metric request-success-rate
```

**Solution** :
```bash
# VÃ©rifier que l'app expose des mÃ©triques
kubectl port-forward deployment/mon-app-canary 8080:8080
curl http://localhost:8080/metrics

# VÃ©rifier que Prometheus scrape
kubectl port-forward -n monitoring svc/prometheus 9090:9090
# Ouvrir http://localhost:9090 et chercher les mÃ©triques
```

### ProblÃ¨me 4 : Progression trop lente

**SymptÃ´mes** :
Le Canary prend des heures.

**Solution** :
```yaml
# RÃ©duire l'intervalle et augmenter stepWeight
analysis:
  interval: 30s      # Au lieu de 1m
  stepWeight: 20     # Au lieu de 10
  maxWeight: 50
```

---

## Comparaison finale : Manuel vs Flagger

| Aspect | Manuel | Flagger |
|--------|--------|---------|
| **Setup initial** | Simple | Complexe |
| **OpÃ©ration** | Manuelle | Automatique |
| **Analyse mÃ©triques** | Manuelle | Automatique |
| **Rollback** | Manuel | Automatique |
| **Tests** | Manuels | Automatiques |
| **Notifications** | Custom | IntÃ©grÃ©es |
| **Courbe d'apprentissage** | Faible | Ã‰levÃ©e |
| **Maintenance** | Ã‰levÃ©e | Faible |
| **FiabilitÃ©** | DÃ©pend de l'opÃ©rateur | TrÃ¨s Ã©levÃ©e |

**Recommandation** :
- **Manuel** : Apprentissage, POC, petites Ã©quipes
- **Flagger** : Production, Ã©quipes matures, multiples apps

---

## RÃ©capitulatif

### Points clÃ©s

**Les dÃ©ploiements Canary, c'est** :
- Exposition **progressive** de la nouvelle version
- Validation en **conditions rÃ©elles**
- **Rollback automatique** si problÃ¨me dÃ©tectÃ©
- RÃ©duction du **risque** (seul un % est impactÃ©)

**ImplÃ©mentation** :
1. **Manuelle** : Deux Deployments + scaling progressif
2. **AutomatisÃ©e** : Flagger + Prometheus + mÃ©triques

**MÃ©triques essentielles** :
- Taux de succÃ¨s (> 99%)
- Latence P95/P99 (< 500ms)
- Taux d'erreur (< 1%)
- Utilisation ressources

**Progression typique** :
10% â†’ 20% â†’ 30% â†’ 40% â†’ 50% â†’ 100%

**DurÃ©e** : 30 minutes Ã  2 heures selon configuration

### Avantages et limites

**âœ… Avantages** :
- Risque minimal (impact limitÃ©)
- Validation en production rÃ©elle
- Rollback automatique possible
- DÃ©tection prÃ©coce des problÃ¨mes
- Confiance accrue

**âŒ Limites** :
- DurÃ©e longue
- ComplexitÃ© (surtout avec Flagger)
- NÃ©cessite monitoring robuste
- CoÃ»t temporaire (ressources supplÃ©mentaires)
- Versions mixtes en production

### Quand utiliser Canary

**âœ… RecommandÃ© pour** :
- Nouvelle fonctionnalitÃ© majeure
- Refonte d'algorithme
- Changement Ã  risque Ã©levÃ©
- Production avec fort trafic
- Besoin de validation rÃ©elle

**âŒ Non recommandÃ© pour** :
- Hotfix urgent
- Correction de bug simple
- Migration incompatible
- Application Ã  faible trafic
- Dev/Test

---

## Conclusion

Les dÃ©ploiements Canary reprÃ©sentent l'une des stratÃ©gies les plus **sÃ»res et sophistiquÃ©es** pour mettre Ã  jour vos applications en production. En exposant progressivement la nouvelle version et en surveillant les mÃ©triques en temps rÃ©el, vous minimisez drastiquement le risque d'un dÃ©ploiement catastrophique.

**Avec la mÃ©thode manuelle**, vous comprenez les fondamentaux et pouvez implÃ©menter un Canary basique avec les outils natifs de Kubernetes.

**Avec Flagger**, vous automatisez complÃ¨tement le processus, incluant l'analyse des mÃ©triques, les tests de charge, et le rollback automatique. C'est l'outil de choix pour les Ã©quipes qui dÃ©ploient frÃ©quemment en production.

**Votre progression DevOps** :

Vous maÃ®trisez maintenant toutes les stratÃ©gies de dÃ©ploiement :

1. **Recreate** : Simple mais downtime
2. **Rolling Update** : Standard Kubernetes
3. **Blue-Green** : Rollback instantanÃ©
4. **Canary** : Validation progressive âœ…

**Prochaine Ã©tape** : Vous avez terminÃ© le chapitre 17 sur DevOps et CI/CD ! Vous disposez maintenant d'un workflow complet et professionnel pour dÃ©ployer vos applications sur MicroK8s avec confiance.

Les chapitres suivants vous attendent pour dÃ©couvrir d'autres aspects avancÃ©s de Kubernetes : gestion des ressources, sÃ©curitÃ©, networking avancÃ©, et bien plus !

**FÃ©licitations pour ce parcours ! ğŸ‰ğŸš€**

â­ï¸ [Gestion des Ressources](/18-gestion-des-ressources/README.md)
