üîù Retour au [Sommaire](/SOMMAIRE.md)

# 20.3 Node Affinity et Anti-Affinity

## Introduction

Dans la section pr√©c√©dente, vous avez d√©couvert les **Node Selectors**, un moyen simple de diriger vos Pods vers des n≈ìuds sp√©cifiques. Les **Node Affinity** (affinit√© de n≈ìud) sont la version √©volu√©e et beaucoup plus puissante des Node Selectors.

Si les Node Selectors sont comme dire "Je veux absolument cette table", les Node Affinity permettent de dire "Je pr√©f√©rerais cette table, mais si elle n'est pas disponible, cette autre table fera l'affaire" ou encore "Je veux n'importe quelle table sauf celle-ci".

Dans cette section, nous allons explorer ce m√©canisme sophistiqu√© qui vous donne un contr√¥le fin sur l'ordonnancement de vos Pods.

---

## Pourquoi Node Affinity au Lieu de Node Selectors ?

Les Node Selectors ont des limitations importantes :

### Limitations des Node Selectors

1. **Contrainte rigide uniquement** : Soit le Pod est plac√©, soit il reste Pending
2. **Logique simple** : Uniquement des conditions "ET" (AND)
3. **Pas de pr√©f√©rences** : Impossible d'exprimer "je pr√©f√®re X mais j'accepte Y"
4. **Pas de n√©gation** : Impossible de dire "PAS ce n≈ìud"
5. **Pas d'op√©rateurs** : Impossible de dire "version > 1.20"

### Avantages des Node Affinity

Les Node Affinity r√©solvent toutes ces limitations :

‚úÖ **Contraintes souples et rigides** : Pr√©f√©rences (soft) vs obligations (hard)
‚úÖ **Logique complexe** : Op√©rateurs OR, NOT, comparaisons
‚úÖ **Pr√©f√©rences gradu√©es** : Poids pour prioriser certains n≈ìuds
‚úÖ **Expressions riches** : In, NotIn, Exists, DoesNotExist, Gt, Lt
‚úÖ **Flexibilit√©** : Comportement d√©grad√© si les n≈ìuds pr√©f√©r√©s sont indisponibles

---

## Les Deux Types d'Affinity

Kubernetes propose deux types principaux de Node Affinity :

### 1. Required During Scheduling (Requis)

**Nom complet** : `requiredDuringSchedulingIgnoredDuringExecution`

**Comportement** :
- Le Pod **doit** √™tre plac√© sur un n≈ìud qui satisfait la r√®gle
- Si aucun n≈ìud ne correspond, le Pod reste **Pending**
- C'est l'√©quivalent d'un Node Selector, mais plus puissant

**Analogie** : "Je dois absolument avoir un si√®ge pr√®s de la fen√™tre, sinon je ne monte pas dans l'avion"

### 2. Preferred During Scheduling (Pr√©f√©r√©)

**Nom complet** : `preferredDuringSchedulingIgnoredDuringExecution`

**Comportement** :
- Le scheduler **pr√©f√®re** placer le Pod sur un n≈ìud qui satisfait la r√®gle
- Si aucun n≈ìud ne correspond, le Pod sera quand m√™me plac√© ailleurs
- Vous pouvez d√©finir un **poids** (weight) pour prioriser les pr√©f√©rences

**Analogie** : "J'aimerais bien un si√®ge pr√®s de la fen√™tre, mais n'importe quel si√®ge fera l'affaire"

### Note sur "IgnoredDuringExecution"

La partie `IgnoredDuringExecution` signifie que si les labels d'un n≈ìud changent apr√®s que le Pod y soit d√©j√† en cours d'ex√©cution, le Pod ne sera **pas** expuls√©. Les r√®gles d'affinity ne sont √©valu√©es qu'au moment de l'ordonnancement.

---

## Syntaxe de Base

La syntaxe des Node Affinity est plus verbeuse que celle des Node Selectors, mais elle offre beaucoup plus de possibilit√©s.

### Structure G√©n√©rale

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mon-pod
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: <label-key>
            operator: <operator>
            values:
            - <value>
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: <1-100>
        preference:
          matchExpressions:
          - key: <label-key>
            operator: <operator>
            values:
            - <value>
  containers:
  - name: mon-conteneur
    image: nginx
```

---

## Les Op√©rateurs Disponibles

Les Node Affinity supportent plusieurs op√©rateurs pour cr√©er des r√®gles sophistiqu√©es :

### 1. **In** - Dans la liste

Le label doit avoir l'une des valeurs sp√©cifi√©es.

```yaml
- key: environment
  operator: In
  values:
  - production
  - staging
```

**Signification** : Le n≈ìud doit avoir le label `environment` avec la valeur `production` **ou** `staging`.

### 2. **NotIn** - Pas dans la liste

Le label ne doit **pas** avoir l'une des valeurs sp√©cifi√©es.

```yaml
- key: environment
  operator: NotIn
  values:
  - development
  - test
```

**Signification** : Le n≈ìud ne doit **pas** avoir le label `environment` avec la valeur `development` ou `test`.

### 3. **Exists** - Le label existe

Le n≈ìud doit avoir ce label, peu importe sa valeur.

```yaml
- key: ssd
  operator: Exists
```

**Signification** : Le n≈ìud doit avoir un label `ssd` (avec n'importe quelle valeur, m√™me vide).

**Note** : Pas besoin de sp√©cifier `values` avec cet op√©rateur.

### 4. **DoesNotExist** - Le label n'existe pas

Le n≈ìud ne doit **pas** avoir ce label.

```yaml
- key: gpu
  operator: DoesNotExist
```

**Signification** : Le n≈ìud ne doit **pas** avoir de label `gpu`.

### 5. **Gt** - Plus grand que (Greater Than)

La valeur du label doit √™tre num√©riquement sup√©rieure.

```yaml
- key: cpu-cores
  operator: Gt
  values:
  - "8"
```

**Signification** : Le n≈ìud doit avoir un label `cpu-cores` avec une valeur > 8.

### 6. **Lt** - Moins que (Less Than)

La valeur du label doit √™tre num√©riquement inf√©rieure.

```yaml
- key: load-average
  operator: Lt
  values:
  - "2.0"
```

**Signification** : Le n≈ìud doit avoir un label `load-average` avec une valeur < 2.0.

---

## Required Affinity - Exemples Pratiques

### Exemple 1 : √âquivalent √† un Node Selector Simple

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-production
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: environment
            operator: In
            values:
            - production
  containers:
  - name: nginx
    image: nginx
```

**Ce que fait ce manifeste** :
- Le Pod **doit** √™tre plac√© sur un n≈ìud ayant le label `environment=production`
- C'est strictement √©quivalent √† : `nodeSelector: { environment: production }`

### Exemple 2 : Logique OR (OU)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-prod-ou-staging
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: environment
            operator: In
            values:
            - production
            - staging
  containers:
  - name: nginx
    image: nginx
```

**Ce que fait ce manifeste** :
- Le Pod peut √™tre plac√© sur un n≈ìud avec `environment=production` **OU** `environment=staging`
- Impossible √† faire avec un simple Node Selector !

### Exemple 3 : Exclusion de N≈ìuds

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-pas-en-dev
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: environment
            operator: NotIn
            values:
            - development
  containers:
  - name: nginx
    image: nginx
```

**Ce que fait ce manifeste** :
- Le Pod peut √™tre plac√© sur n'importe quel n≈ìud **sauf** ceux ayant `environment=development`
- Permet d'√©viter explicitement certains n≈ìuds

### Exemple 4 : V√©rifier l'Existence d'un Label

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-gpu
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: hardware.gpu
            operator: Exists
  containers:
  - name: tensorflow
    image: tensorflow/tensorflow:latest-gpu
```

**Ce que fait ce manifeste** :
- Le Pod doit √™tre plac√© sur un n≈ìud qui a un label `hardware.gpu`, quelle que soit sa valeur
- Utile quand la pr√©sence du label suffit, peu importe sa valeur exacte

### Exemple 5 : Conditions Multiples (AND)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-conditions-multiples
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: environment
            operator: In
            values:
            - production
          - key: disktype
            operator: In
            values:
            - ssd
          - key: zone
            operator: In
            values:
            - europe
  containers:
  - name: nginx
    image: nginx
```

**Ce que fait ce manifeste** :
- Le n≈ìud doit avoir `environment=production` **ET**
- Le n≈ìud doit avoir `disktype=ssd` **ET**
- Le n≈ìud doit avoir `zone=europe`
- Toutes les conditions doivent √™tre satisfaites simultan√©ment

### Exemple 6 : Plusieurs Groupes de Conditions (OR de AND)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-flexible
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: environment
            operator: In
            values:
            - production
          - key: zone
            operator: In
            values:
            - europe
        - matchExpressions:
          - key: environment
            operator: In
            values:
            - staging
          - key: zone
            operator: In
            values:
            - us
  containers:
  - name: nginx
    image: nginx
```

**Ce que fait ce manifeste** :
- **Option 1** : (`environment=production` ET `zone=europe`) **OU**
- **Option 2** : (`environment=staging` ET `zone=us`)
- Le Pod peut √™tre plac√© si l'une des deux options est satisfaite

---

## Preferred Affinity - Exemples Pratiques

Les Preferred Affinity permettent d'exprimer des **pr√©f√©rences** plut√¥t que des obligations.

### Exemple 1 : Pr√©f√©rence Simple

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-preference-ssd
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
  containers:
  - name: nginx
    image: nginx
```

**Ce que fait ce manifeste** :
- Le scheduler **pr√©f√®re** placer le Pod sur un n≈ìud avec `disktype=ssd`
- Si aucun n≈ìud SSD n'est disponible, le Pod sera plac√© sur n'importe quel autre n≈ìud
- Le `weight` (poids) de 100 indique la force de cette pr√©f√©rence

### Exemple 2 : Comprendre le Weight (Poids)

Le poids (weight) va de 1 √† 100 et permet de prioriser les pr√©f√©rences.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-preferences-multiples
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 80
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
      - weight: 20
        preference:
          matchExpressions:
          - key: zone
            operator: In
            values:
            - europe
  containers:
  - name: nginx
    image: nginx
```

**Comment le scheduler calcule** :
- Chaque n≈ìud re√ßoit un score bas√© sur les pr√©f√©rences satisfaites
- N≈ìud avec `disktype=ssd` : +80 points
- N≈ìud avec `zone=europe` : +20 points
- N≈ìud avec les deux : +100 points
- Le scheduler choisit le n≈ìud avec le score le plus √©lev√©

**Signification pratique** :
- La pr√©sence d'un SSD est **4 fois plus importante** que la zone g√©ographique (80 vs 20)
- Si aucun n≈ìud ne satisfait ces pr√©f√©rences, le Pod sera quand m√™me plac√©

### Exemple 3 : Combinaison Required + Preferred

Vous pouvez combiner des contraintes obligatoires et des pr√©f√©rences.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-smart
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: environment
            operator: In
            values:
            - production
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
      - weight: 50
        preference:
          matchExpressions:
          - key: cpu-cores
            operator: Gt
            values:
            - "8"
  containers:
  - name: nginx
    image: nginx
```

**Ce que fait ce manifeste** :
- **Obligatoire** : Le Pod **doit** √™tre sur un n≈ìud `environment=production`
- **Pr√©f√©r√© (poids 100)** : Id√©alement avec `disktype=ssd`
- **Pr√©f√©r√© (poids 50)** : Id√©alement avec plus de 8 c≈ìurs CPU
- Si aucun n≈ìud de production n'a de SSD ni beaucoup de CPU, le Pod ira quand m√™me sur un n≈ìud de production

---

## Cas d'Usage Pratiques avec MicroK8s

### Cas 1 : Optimisation des Performances

Vous avez un cluster avec des n≈ìuds de diff√©rentes puissances.

**Configuration des n≈ìuds** :
```bash
kubectl label nodes node1 performance=high cpu-cores=16
kubectl label nodes node2 performance=medium cpu-cores=8
kubectl label nodes node3 performance=low cpu-cores=4
```

**D√©ploiement d'une application exigeante** :
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-exigeante
spec:
  replicas: 3
  selector:
    matchLabels:
      app: exigeante
  template:
    metadata:
      labels:
        app: exigeante
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: performance
                operator: In
                values:
                - high
          - weight: 50
            preference:
              matchExpressions:
              - key: cpu-cores
                operator: Gt
                values:
                - "8"
      containers:
      - name: app
        image: mon-app-intensive
        resources:
          requests:
            cpu: "2000m"
            memory: "4Gi"
```

**R√©sultat** :
- Le scheduler privil√©gie fortement les n≈ìuds haute performance
- Si tous les n≈ìuds haute performance sont pleins, il utilise les n≈ìuds medium
- Les Pods seront quand m√™me d√©ploy√©s m√™me si aucun n≈ìud pr√©f√©r√© n'est disponible

### Cas 2 : Distribution G√©ographique avec Pr√©f√©rence

```bash
# Labellisation
kubectl label nodes node1 zone=europe region=west
kubectl label nodes node2 zone=europe region=east
kubectl label nodes node3 zone=us region=east
```

**D√©ploiement** :
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-globale
spec:
  replicas: 5
  selector:
    matchLabels:
      app: globale
  template:
    metadata:
      labels:
        app: globale
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: zone
                operator: In
                values:
                - europe
                - us
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            preference:
              matchExpressions:
              - key: zone
                operator: In
                values:
                - europe
          - weight: 40
            preference:
              matchExpressions:
              - key: region
                operator: In
                values:
                - west
      containers:
      - name: app
        image: mon-app:latest
```

**R√©sultat** :
- Tous les Pods doivent √™tre en Europe ou aux US (obligatoire)
- Pr√©f√©rence forte pour l'Europe (weight 80)
- Pr√©f√©rence secondaire pour la r√©gion West (weight 40)
- Ordre de pr√©f√©rence : europe-west > europe-east > us-east

### Cas 3 : √âviter les N≈ìuds Surcharg√©s

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-legere
spec:
  replicas: 10
  selector:
    matchLabels:
      app: legere
  template:
    metadata:
      labels:
        app: legere
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: workload
                operator: NotIn
                values:
                - heavy
                - database
      containers:
      - name: app
        image: app-legere:latest
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
```

**R√©sultat** :
- L'application l√©g√®re **ne sera jamais** plac√©e sur des n≈ìuds marqu√©s `workload=heavy` ou `workload=database`
- Permet de r√©server certains n≈ìuds pour des charges sp√©cifiques

### Cas 4 : Base de Donn√©es avec Exigences Strictes

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
                - nvme
              - key: memory
                operator: In
                values:
                - high
                - very-high
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: disktype
                operator: In
                values:
                - nvme
          - weight: 80
            preference:
              matchExpressions:
              - key: workload
                operator: In
                values:
                - database
      containers:
      - name: postgres
        image: postgres:14
        resources:
          requests:
            cpu: "2000m"
            memory: "8Gi"
```

**R√©sultat** :
- **Obligatoire** : Disque SSD ou NVMe + Haute m√©moire
- **Pr√©f√©r√©** : NVMe plut√¥t que SSD (weight 100)
- **Pr√©f√©r√©** : N≈ìud d√©di√© aux bases de donn√©es (weight 80)

---

## Node Anti-Affinity ?

Vous pourriez vous demander s'il existe une "Node Anti-Affinity" explicite. La r√©ponse est **non**, mais vous pouvez obtenir le m√™me r√©sultat avec les op√©rateurs existants.

### Simuler une Anti-Affinity

Pour dire "N'importe quel n≈ìud SAUF ceux-ci" :

```yaml
nodeAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
    nodeSelectorTerms:
    - matchExpressions:
      - key: environment
        operator: NotIn
        values:
        - test
        - development
```

Pour dire "N≈ìuds qui n'ont PAS ce label" :

```yaml
nodeAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
    nodeSelectorTerms:
    - matchExpressions:
      - key: restricted
        operator: DoesNotExist
```

---

## D√©bogage et Diagnostics

### Voir Pourquoi un Pod est Pending

```bash
kubectl describe pod <pod-name>
```

Recherchez dans les Events :
```
Events:
  Type     Reason            Message
  ----     ------            -------
  Warning  FailedScheduling  0/3 nodes are available:
                             3 node(s) didn't match Pod's node affinity/selector.
```

### V√©rifier les Labels des N≈ìuds

```bash
kubectl get nodes --show-labels
```

### Tester une Affinity Complexe

Cr√©ez d'abord un Pod de test :

```bash
kubectl run test-affinity --image=nginx --dry-run=client -o yaml > test-affinity.yaml
```

Ajoutez vos r√®gles d'affinity dans le fichier, puis :

```bash
kubectl apply -f test-affinity.yaml
kubectl get pod test-affinity -o wide
```

### Voir sur Quel N≈ìud un Pod a √©t√© Plac√©

```bash
kubectl get pods -o wide
```

---

## Comparaison : Node Selector vs Node Affinity

| Aspect | Node Selector | Node Affinity |
|--------|---------------|---------------|
| **Simplicit√©** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Tr√®s simple | ‚≠ê‚≠ê Plus complexe |
| **Flexibilit√©** | ‚≠ê‚≠ê Limit√©e | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Tr√®s flexible |
| **Logique OR** | ‚ùå Non | ‚úÖ Oui |
| **N√©gation** | ‚ùå Non | ‚úÖ Oui (NotIn, DoesNotExist) |
| **Pr√©f√©rences** | ‚ùå Non | ‚úÖ Oui (preferred) |
| **Poids** | ‚ùå Non | ‚úÖ Oui (weight) |
| **Op√©rateurs** | ‚ùå √âgalit√© uniquement | ‚úÖ In, NotIn, Exists, Gt, Lt, etc. |
| **Contrainte douce** | ‚ùå Non | ‚úÖ Oui (preferred) |

**Recommandation** :
- Utilisez **Node Selector** pour des besoins simples (√©quivalent √† un seul label)
- Utilisez **Node Affinity** pour des besoins complexes ou des pr√©f√©rences

---

## Bonnes Pratiques

### 1. Commencez Simple

Ne cr√©ez pas des r√®gles d'affinity complexes d√®s le d√©part. Commencez simple et ajoutez de la complexit√© uniquement si n√©cessaire.

### 2. Pr√©f√©rez les Preferred Quand Possible

Les `preferredDuringScheduling` sont plus flexibles et r√©silientes. Le Pod sera toujours plac√© quelque part.

```yaml
# ‚úÖ Bon : Pr√©f√©rence avec fallback
preferredDuringSchedulingIgnoredDuringExecution:
- weight: 100
  preference:
    matchExpressions:
    - key: disktype
      operator: In
      values:
      - ssd
```

```yaml
# ‚ö†Ô∏è √Ä √©viter sauf si vraiment n√©cessaire : Contrainte rigide
requiredDuringSchedulingIgnoredDuringExecution:
  nodeSelectorTerms:
  - matchExpressions:
    - key: disktype
      operator: In
      values:
      - ssd
```

### 3. Utilisez des Poids Significatifs

Espacez les poids pour exprimer clairement les priorit√©s :

```yaml
# ‚úÖ Bon : Diff√©rences claires
- weight: 100  # Tr√®s important
  preference: ...
- weight: 50   # Moyennement important
  preference: ...
- weight: 10   # Peu important
  preference: ...
```

```yaml
# ‚ùå Mauvais : Pas de distinction claire
- weight: 90
  preference: ...
- weight: 88
  preference: ...
- weight: 85
  preference: ...
```

### 4. Documentez vos R√®gles

Ajoutez des commentaires pour expliquer les r√®gles complexes :

```yaml
affinity:
  nodeAffinity:
    # Obligation : Seulement en production avec SSD
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: environment
          operator: In
          values:
          - production
        - key: disktype
          operator: In
          values:
          - ssd
    # Pr√©f√©rence : Id√©alement en Europe avec > 16 cores
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      preference:
        matchExpressions:
        - key: zone
          operator: In
          values:
          - europe
    - weight: 50
      preference:
        matchExpressions:
        - key: cpu-cores
          operator: Gt
          values:
          - "16"
```

### 5. Testez Vos R√®gles

Avant de d√©ployer en production :

1. Cr√©ez un Pod de test avec vos r√®gles d'affinity
2. V√©rifiez qu'il est plac√© sur le bon type de n≈ìud
3. Testez des sc√©narios de d√©faillance (n≈ìud down, etc.)

### 6. Surveillez les Pods Pending

```bash
# Script de surveillance simple
kubectl get pods --all-namespaces | grep Pending
```

Si vous avez beaucoup de Pods Pending, revoyez vos r√®gles d'affinity - elles sont peut-√™tre trop restrictives.

### 7. Combinez avec Resource Requests

Les affinity ne remplacent pas les resource requests :

```yaml
spec:
  affinity:
    nodeAffinity:
      # ... vos r√®gles ...
  containers:
  - name: app
    image: mon-app
    resources:
      requests:
        cpu: "500m"
        memory: "1Gi"
      limits:
        cpu: "1000m"
        memory: "2Gi"
```

---

## Exemple Complet : Application Multi-Tiers Avanc√©e

Voici un exemple complet combinant plusieurs concepts pour une architecture √† trois niveaux.

### Labellisation des N≈ìuds

```bash
# Frontend nodes
kubectl label nodes node1 tier=frontend disktype=ssd zone=europe
kubectl label nodes node2 tier=frontend disktype=ssd zone=us

# Backend nodes
kubectl label nodes node3 tier=backend disktype=ssd zone=europe cpu-cores=16
kubectl label nodes node4 tier=backend disktype=ssd zone=us cpu-cores=8

# Database node
kubectl label nodes node5 tier=database disktype=nvme zone=europe memory=very-high cpu-cores=32
```

### Frontend Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 4
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: tier
                operator: In
                values:
                - frontend
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            preference:
              matchExpressions:
              - key: zone
                operator: In
                values:
                - europe
          - weight: 20
            preference:
              matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
      containers:
      - name: nginx
        image: nginx:latest
        resources:
          requests:
            cpu: "200m"
            memory: "256Mi"
```

### Backend Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: tier
                operator: In
                values:
                - backend
              - key: disktype
                operator: In
                values:
                - ssd
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: cpu-cores
                operator: Gt
                values:
                - "12"
          - weight: 60
            preference:
              matchExpressions:
              - key: zone
                operator: In
                values:
                - europe
      containers:
      - name: api
        image: mon-api:v2
        resources:
          requests:
            cpu: "1000m"
            memory: "2Gi"
```

### Database StatefulSet

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database
spec:
  serviceName: db
  replicas: 1
  selector:
    matchLabels:
      app: database
  template:
    metadata:
      labels:
        app: database
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: tier
                operator: In
                values:
                - database
              - key: disktype
                operator: In
                values:
                - nvme
                - ssd
              - key: memory
                operator: In
                values:
                - very-high
                - high
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: disktype
                operator: In
                values:
                - nvme
          - weight: 80
            preference:
              matchExpressions:
              - key: cpu-cores
                operator: Gt
                values:
                - "24"
      containers:
      - name: postgres
        image: postgres:14
        resources:
          requests:
            cpu: "4000m"
            memory: "16Gi"
```

---

## Points Cl√©s √† Retenir

1. **Node Affinity = Node Selector++** : Plus puissant mais plus complexe

2. **Deux types** :
   - `required` : Obligation (comme Node Selector mais plus flexible)
   - `preferred` : Pr√©f√©rence (avec fallback)

3. **Op√©rateurs riches** : In, NotIn, Exists, DoesNotExist, Gt, Lt

4. **Poids (weight)** : Permet de prioriser les pr√©f√©rences (1-100)

5. **Logique complexe** : Supporte OR, AND, NOT

6. **Pr√©f√©rez preferred** : Plus flexible et r√©silient

7. **Testez toujours** : V√©rifiez que vos r√®gles fonctionnent comme pr√©vu

---

## Conclusion

Les Node Affinity repr√©sentent un outil puissant pour contr√¥ler finement le placement de vos Pods dans un cluster Kubernetes. Bien qu'elles soient plus complexes que les Node Selectors, elles offrent une flexibilit√© in√©gal√©e pour g√©rer des sc√©narios d'ordonnancement sophistiqu√©s.

En ma√Ætrisant les Node Affinity, vous pouvez :
- Optimiser l'utilisation des ressources mat√©rielles
- Am√©liorer les performances en pla√ßant les Pods strat√©giquement
- Cr√©er des architectures r√©silientes avec des pr√©f√©rences d√©grad√©es
- Isoler les workloads selon vos besoins

Dans la prochaine section, nous d√©couvrirons les **Pod Affinity et Anti-Affinity**, qui permettent de contr√¥ler le placement des Pods **les uns par rapport aux autres**, ouvrant encore plus de possibilit√©s pour l'orchestration de vos applications.

---

## Pour Aller Plus Loin

- **Documentation officielle** : [Node Affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)
- **Scheduling Framework** : Comprendre comment le scheduler calcule les scores
- **Pod Affinity** : Contr√¥ler le placement relatif des Pods (prochaine section)

---

**Prochain chapitre** : 20.4 Pod Affinity et Anti-Affinity - Orchestrer le placement relatif de vos Pods

‚è≠Ô∏è [Pod Affinity et Anti-Affinity](/20-ordonnancement-avance/04-pod-affinity-et-anti-affinity.md)
