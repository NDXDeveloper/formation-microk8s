üîù Retour au [Sommaire](/SOMMAIRE.md)

# 20.1 Scheduler Kubernetes

## Introduction

Le **Scheduler Kubernetes** (ordonnanceur) est l'un des composants les plus critiques du control plane de Kubernetes. Son r√¥le principal est de d√©cider sur quel n≈ìud (node) un Pod nouvellement cr√©√© doit √™tre ex√©cut√©. Cette d√©cision est prise en fonction de nombreux crit√®res et contraintes, et elle a un impact direct sur les performances, la fiabilit√© et l'efficacit√© de votre cluster.

Dans cette section, nous allons explorer en profondeur le fonctionnement du scheduler, comprendre ses m√©canismes et apprendre comment il prend ses d√©cisions.

---

## Qu'est-ce que le Scheduler ?

Le **kube-scheduler** est le composant du control plane responsable de l'ordonnancement des Pods. Lorsqu'un nouveau Pod est cr√©√© (par exemple via un Deployment, un Job ou directement), il commence dans un √©tat "Pending" (en attente) sans √™tre assign√© √† un n≈ìud sp√©cifique.

Le scheduler surveille en permanence les Pods non assign√©s et, pour chacun d'eux :
1. **Identifie les n≈ìuds candidats** qui pourraient h√©berger le Pod
2. **√âvalue et filtre** ces n≈ìuds selon diff√©rents crit√®res
3. **Score et classe** les n≈ìuds √©ligibles
4. **S√©lectionne le n≈ìud optimal** et assigne le Pod √† ce n≈ìud

Une fois qu'un Pod est assign√© √† un n≈ìud, le **kubelet** de ce n≈ìud prend le relais pour lancer effectivement le Pod.

---

## Pourquoi le Scheduler est-il Important ?

Le scheduler joue un r√¥le crucial pour plusieurs raisons :

### 1. **Optimisation des Ressources**
Il r√©partit intelligemment les Pods sur les n≈ìuds disponibles pour √©viter la surcharge de certains n≈ìuds et le sous-emploi d'autres.

### 2. **Respect des Contraintes**
Il s'assure que les Pods sont plac√©s uniquement sur des n≈ìuds qui satisfont toutes leurs exigences (ressources CPU/m√©moire, labels sp√©cifiques, etc.).

### 3. **Haute Disponibilit√©**
En distribuant les Pods d'une m√™me application sur diff√©rents n≈ìuds, il am√©liore la r√©silience du syst√®me.

### 4. **Performance**
Un bon placement des Pods peut am√©liorer les performances en tenant compte de la localit√© des donn√©es, de la latence r√©seau, etc.

---

## Le Processus d'Ordonnancement : √âtape par √âtape

Comprendre comment le scheduler fonctionne vous aidera √† mieux configurer vos Pods et √† r√©soudre les probl√®mes d'ordonnancement.

### √âtape 1 : Surveillance des Pods Non Assign√©s

Le scheduler surveille continuellement l'API Server pour d√©tecter les nouveaux Pods qui n'ont pas encore √©t√© assign√©s √† un n≈ìud (Pods avec `spec.nodeName` vide).

### √âtape 2 : Phase de Filtrage (Predicate Phase)

Pour chaque Pod en attente, le scheduler commence par **filtrer** les n≈ìuds disponibles pour ne garder que ceux qui sont **√©ligibles**. Un n≈ìud est √©ligible s'il passe tous les "predicates" (tests de faisabilit√©).

#### Exemples de Predicates Courants

1. **PodFitsResources**
   - V√©rifie que le n≈ìud dispose de suffisamment de CPU et de m√©moire pour satisfaire les `requests` du Pod

2. **PodFitsHostPorts**
   - V√©rifie qu'aucun autre Pod n'utilise d√©j√† les ports host requis

3. **NodeSelector**
   - V√©rifie que le n≈ìud poss√®de tous les labels sp√©cifi√©s dans le `nodeSelector` du Pod

4. **MatchNodeAffinity**
   - √âvalue les r√®gles d'affinit√© avec les n≈ìuds si elles sont d√©finies

5. **TaintToleration**
   - V√©rifie que le Pod tol√®re tous les taints pr√©sents sur le n≈ìud

6. **PodToleratesNodeTaints**
   - S'assure que le Pod peut √™tre planifi√© sur un n≈ìud avec des taints

7. **CheckVolumeBinding**
   - V√©rifie que les volumes demand√©s peuvent √™tre provisionn√©s et attach√©s sur ce n≈ìud

8. **NoDiskConflict**
   - S'assure qu'il n'y a pas de conflit avec les volumes existants

### √âtape 3 : Phase de Scoring (Priority Phase)

Une fois que le scheduler a filtr√© les n≈ìuds √©ligibles, il passe √† la phase de **scoring** (notation). Chaque n≈ìud restant re√ßoit un score bas√© sur plusieurs fonctions de priorit√©.

#### Exemples de Fonctions de Scoring

1. **LeastRequestedPriority**
   - Favorise les n≈ìuds ayant le plus de ressources disponibles (CPU/m√©moire)
   - Encourage une r√©partition √©quilibr√©e de la charge

2. **BalancedResourceAllocation**
   - Pr√©f√®re les n≈ìuds o√π les ressources CPU et m√©moire sont utilis√©es de mani√®re √©quilibr√©e
   - √âvite qu'un n≈ìud soit surcharg√© en CPU mais sous-utilis√© en m√©moire (ou vice versa)

3. **NodeAffinityPriority**
   - Augmente le score des n≈ìuds qui correspondent mieux aux pr√©f√©rences d'affinit√©

4. **InterPodAffinityPriority**
   - Favorise ou d√©favorise les n≈ìuds en fonction de la pr√©sence d'autres Pods

5. **ImageLocalityPriority**
   - Favorise les n≈ìuds qui ont d√©j√† t√©l√©charg√© l'image Docker du Pod
   - R√©duit le temps de d√©marrage

6. **TaintTolerationPriority**
   - Donne un score en fonction de la tol√©rance aux taints

### √âtape 4 : S√©lection du N≈ìud

Le scheduler s√©lectionne le n≈ìud ayant obtenu le **score le plus √©lev√©**. En cas d'√©galit√©, il choisit de mani√®re pseudo-al√©atoire parmi les n≈ìuds ayant le meilleur score.

### √âtape 5 : Binding

Une fois le n≈ìud s√©lectionn√©, le scheduler effectue une op√©ration appel√©e **binding** :
- Il met √† jour le Pod dans l'API Server en d√©finissant le champ `spec.nodeName` avec le nom du n≈ìud s√©lectionn√©
- Le kubelet du n≈ìud d√©sign√© d√©tecte ce nouveau Pod et commence √† le lancer

---

## Ressources et Requests : Impact sur l'Ordonnancement

Les sp√©cifications de ressources dans vos Pods influencent directement les d√©cisions du scheduler.

### Requests vs Limits

```yaml
resources:
  requests:
    cpu: "250m"        # Le scheduler utilise cette valeur
    memory: "512Mi"    # Le scheduler utilise cette valeur
  limits:
    cpu: "500m"        # Utilis√© par le kubelet, pas le scheduler
    memory: "1Gi"      # Utilis√© par le kubelet, pas le scheduler
```

**Point Important** : Le scheduler utilise uniquement les **requests** pour ses d√©cisions, pas les **limits**.

- Les **requests** repr√©sentent les ressources garanties dont le Pod a besoin
- Le scheduler s'assure qu'un n≈ìud a suffisamment de ressources disponibles pour honorer toutes les requests des Pods qui y tournent

### Cons√©quences d'une Mauvaise Configuration

#### Requests trop Faibles
- Le scheduler peut placer trop de Pods sur un m√™me n≈ìud
- Risque de contention des ressources et de d√©gradation des performances

#### Requests trop √âlev√©es
- Sous-utilisation des n≈ìuds
- Pods qui ne peuvent pas √™tre ordonnanc√©s m√™me si les ressources r√©elles sont disponibles

#### Pas de Requests D√©finies
- Le scheduler consid√®re que le Pod demande 0 ressource
- Tous les n≈ìuds sont potentiellement √©ligibles, m√™me ceux d√©j√† surcharg√©s

---

## Que se Passe-t-il Quand un Pod ne Peut Pas √ätre Ordonnanc√© ?

Si le scheduler ne trouve aucun n≈ìud √©ligible pour un Pod, le Pod reste dans l'√©tat **Pending**. Vous pouvez investiguer avec :

```bash
kubectl get pods
```

R√©sultat typique :
```
NAME                    READY   STATUS    RESTARTS   AGE
mon-app-xyz123-abc      0/1     Pending   0          2m
```

Pour comprendre pourquoi :

```bash
kubectl describe pod mon-app-xyz123-abc
```

Dans la section **Events**, vous verrez des messages explicatifs :

```
Events:
  Type     Reason            Age   Message
  ----     ------            ----  -------
  Warning  FailedScheduling  1m    0/3 nodes are available:
                                   3 Insufficient cpu.
```

### Raisons Courantes de Non-Ordonnancement

1. **Ressources Insuffisantes**
   - Aucun n≈ìud n'a assez de CPU ou m√©moire disponible
   - Solution : Ajouter des n≈ìuds ou r√©duire les requests

2. **Taints Non Tol√©r√©s**
   - Les n≈ìuds ont des taints que le Pod ne tol√®re pas
   - Solution : Ajouter des tolerations au Pod ou retirer les taints

3. **NodeSelector Non Satisfait**
   - Aucun n≈ìud ne poss√®de les labels requis
   - Solution : Labelliser des n≈ìuds ou modifier le nodeSelector

4. **Affinit√©s Non Respect√©es**
   - Les r√®gles d'affinit√©/anti-affinit√© ne peuvent √™tre satisfaites
   - Solution : Revoir les r√®gles ou ajouter des n≈ìuds

5. **Volumes Non Disponibles**
   - Les volumes demand√©s ne peuvent pas √™tre provisionn√©s
   - Solution : V√©rifier la configuration du stockage

---

## Le Scheduler et MicroK8s

Dans MicroK8s, le scheduler fonctionne exactement de la m√™me mani√®re que dans Kubernetes standard. Il est automatiquement d√©ploy√© et configur√© lors de l'installation de MicroK8s.

### V√©rifier l'√âtat du Scheduler

```bash
microk8s kubectl get pods -n kube-system | grep scheduler
```

Vous devriez voir quelque chose comme :
```
kube-scheduler-node1    1/1     Running   0          5d
```

### Logs du Scheduler

Si vous avez besoin de d√©boguer des probl√®mes d'ordonnancement, vous pouvez consulter les logs du scheduler :

```bash
microk8s kubectl logs -n kube-system kube-scheduler-<nom-du-pod>
```

---

## Scheduler par D√©faut vs Schedulers Personnalis√©s

### Scheduler par D√©faut

Kubernetes utilise un scheduler par d√©faut qui convient √† la plupart des cas d'usage. Il est d√©j√† optimis√© et bien test√©.

### Schedulers Personnalis√©s

Pour des besoins tr√®s sp√©cifiques, vous pouvez :
- Cr√©er votre propre scheduler personnalis√©
- Configurer un Pod pour utiliser un scheduler sp√©cifique via `spec.schedulerName`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mon-pod
spec:
  schedulerName: mon-scheduler-custom
  containers:
  - name: nginx
    image: nginx
```

**Note** : Dans un contexte d'apprentissage avec MicroK8s, le scheduler par d√©faut est largement suffisant.

---

## Points Cl√©s √† Retenir

1. **Le scheduler est automatique** : Vous n'avez g√©n√©ralement rien √† configurer, il fonctionne en arri√®re-plan

2. **Les requests sont cruciales** : Toujours d√©finir des requests r√©alistes pour CPU et m√©moire

3. **Phase de filtrage puis scoring** : Le scheduler √©limine d'abord les n≈ìuds non √©ligibles, puis choisit le meilleur parmi ceux qui restent

4. **Pending = probl√®me d'ordonnancement** : Utilisez `kubectl describe pod` pour comprendre pourquoi

5. **Influence via les contraintes** : Vous pouvez guider les d√©cisions du scheduler via nodeSelector, affinities, taints/tolerations (que nous verrons dans les sections suivantes)

---

## Bonnes Pratiques

### 1. Toujours D√©finir des Requests
Ne laissez jamais vos Pods sans `requests`. Cela permet au scheduler de prendre des d√©cisions √©clair√©es.

### 2. Requests R√©alistes
- Basez-vous sur des mesures r√©elles de consommation
- N'exag√©rez pas : cela gaspille des ressources
- N'minimisez pas : cela cause des probl√®mes de performance

### 3. Monitorer les Pods Pending
- Surveillez r√©guli√®rement les Pods qui ne peuvent pas √™tre ordonnanc√©s
- Analysez les causes et agissez rapidement

### 4. Comprendre la Capacit√© du Cluster
- Connaissez les ressources totales de votre cluster
- Planifiez l'ajout de n≈ìuds avant d'atteindre la saturation

### 5. Utiliser les Contraintes avec Parcimonie
- Trop de contraintes (affinit√©s, nodeSelectors) peuvent compliquer l'ordonnancement
- N'ajoutez des contraintes que quand c'est vraiment n√©cessaire

---

## Conclusion

Le scheduler Kubernetes est un composant sophistiqu√© mais transparent pour l'utilisateur dans la plupart des cas. Il prend automatiquement les meilleures d√©cisions possibles en fonction des ressources disponibles et des contraintes d√©finies.

En comprenant son fonctionnement, vous √™tes mieux √©quip√© pour :
- Configurer vos Pods de mani√®re optimale
- Diagnostiquer les probl√®mes d'ordonnancement
- Utiliser les m√©canismes avanc√©s comme les affinit√©s et les taints (sections suivantes)

Dans les prochaines sections, nous d√©couvrirons comment influencer les d√©cisions du scheduler gr√¢ce √† des m√©canismes comme les **Node Selectors**, les **Affinit√©s** et les **Taints/Tolerations**.

---

## Pour Aller Plus Loin

- **Documentation officielle** : [Kubernetes Scheduler](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/)
- **Scheduler Policies** : Configuration avanc√©e du comportement du scheduler
- **Scheduler Profiles** : Diff√©rents profils de scheduling pour diff√©rents besoins
- **Scheduling Framework** : Architecture extensible pour cr√©er des plugins personnalis√©s

---

**Prochain chapitre** : 20.2 Node Selectors - Comment diriger vos Pods vers des n≈ìuds sp√©cifiques

‚è≠Ô∏è [Node Selectors](/20-ordonnancement-avance/02-node-selectors.md)
