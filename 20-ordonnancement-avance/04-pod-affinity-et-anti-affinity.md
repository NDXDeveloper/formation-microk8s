üîù Retour au [Sommaire](/SOMMAIRE.md)

# 20.4 Pod Affinity et Anti-Affinity

## Introduction

Jusqu'√† pr√©sent, nous avons vu comment contr√¥ler le placement des Pods par rapport aux **n≈ìuds** avec les Node Selectors et Node Affinity. Mais que faire si vous voulez contr√¥ler le placement des Pods **les uns par rapport aux autres** ?

C'est exactement ce que permettent les **Pod Affinity** et **Pod Anti-Affinity**. Ces m√©canismes vous permettent de dire des choses comme :
- "Ce Pod doit √™tre sur le m√™me n≈ìud que ces autres Pods"
- "Ce Pod doit √™tre dans la m√™me zone que ces autres Pods"
- "Ce Pod ne doit jamais √™tre sur le m√™me n≈ìud que ces autres Pods"

Dans cette section, nous allons explorer ces concepts puissants qui ouvrent de nouvelles possibilit√©s pour orchestrer vos applications.

---

## Diff√©rence Fondamentale : Node vs Pod Affinity

### Node Affinity
- **Relation** : Pod ‚Üí N≈ìud
- **Question** : "Sur quel **type de n≈ìud** ce Pod doit-il tourner ?"
- **Exemple** : "Ce Pod doit √™tre sur un n≈ìud avec GPU"

### Pod Affinity
- **Relation** : Pod ‚Üí Autres Pods
- **Question** : "Ce Pod doit-il √™tre **proche** ou **√©loign√©** d'autres Pods ?"
- **Exemple** : "Ce Pod doit √™tre sur le m√™me n≈ìud que les Pods du cache Redis"

---

## Pourquoi Utiliser Pod Affinity/Anti-Affinity ?

### Cas d'Usage pour Pod Affinity (Rapprocher les Pods)

#### 1. **Colocalisation pour la Performance**
Placer une application web et son cache sur le m√™me n≈ìud pour minimiser la latence r√©seau.

```
N≈ìud 1: [Web App] + [Redis Cache] ‚Üê Faible latence
N≈ìud 2: [Other App]
```

#### 2. **Partage de Donn√©es Locales**
Plusieurs Pods qui partagent un volume local doivent √™tre sur le m√™me n≈ìud.

#### 3. **Batch Processing**
Placer les workers d'un job de traitement pr√®s de leur source de donn√©es.

### Cas d'Usage pour Pod Anti-Affinity (√âloigner les Pods)

#### 1. **Haute Disponibilit√©**
Distribuer les r√©plicas d'une application sur diff√©rents n≈ìuds pour √©viter qu'une panne de n≈ìud ne coupe tout le service.

```
N≈ìud 1: [App Replica 1]
N≈ìud 2: [App Replica 2]
N≈ìud 3: [App Replica 3]
```

#### 2. **√âviter la Contention de Ressources**
Emp√™cher que plusieurs applications gourmandes en CPU tournent sur le m√™me n≈ìud.

#### 3. **Isolation de S√©curit√©**
S'assurer que certaines applications critiques ne tournent jamais ensemble.

#### 4. **Distribution G√©ographique**
R√©partir les Pods sur diff√©rentes zones de disponibilit√©.

---

## Les Deux Types de Pod Affinity

Comme pour les Node Affinity, il existe deux types :

### 1. Required (Requis)

**Nom complet** : `requiredDuringSchedulingIgnoredDuringExecution`

**Comportement** :
- Le Pod **doit** √™tre plac√© selon la r√®gle
- Si impossible, le Pod reste **Pending**

### 2. Preferred (Pr√©f√©r√©)

**Nom complet** : `preferredDuringSchedulingIgnoredDuringExecution`

**Comportement** :
- Le scheduler **pr√©f√®re** placer le Pod selon la r√®gle
- Si impossible, le Pod est plac√© ailleurs
- Utilise un syst√®me de **poids** (weight) comme pour Node Affinity

---

## Concept Cl√© : La Topologie (topologyKey)

Le **topologyKey** est le concept le plus important √† comprendre pour les Pod Affinity/Anti-Affinity.

### Qu'est-ce qu'une Topologie ?

Une topologie d√©finit un **niveau de s√©paration** dans votre infrastructure. Elle est repr√©sent√©e par un label sur les n≈ìuds.

### Exemples de Topologies Courantes

#### 1. `kubernetes.io/hostname`
- **Signification** : Le n≈ìud lui-m√™me
- **Utilisation** : "Sur le m√™me n≈ìud" ou "Sur des n≈ìuds diff√©rents"

```
N≈ìud 1 (hostname=node1)
N≈ìud 2 (hostname=node2)
N≈ìud 3 (hostname=node3)
```

#### 2. `topology.kubernetes.io/zone`
- **Signification** : La zone de disponibilit√© (datacenter, rack, etc.)
- **Utilisation** : "Dans la m√™me zone" ou "Dans des zones diff√©rentes"

```
Zone A: [N≈ìud 1, N≈ìud 2]
Zone B: [N≈ìud 3, N≈ìud 4]
Zone C: [N≈ìud 5, N≈ìud 6]
```

#### 3. `topology.kubernetes.io/region`
- **Signification** : La r√©gion g√©ographique
- **Utilisation** : "Dans la m√™me r√©gion" ou "Dans des r√©gions diff√©rentes"

```
R√©gion EU: [Zone EU-West, Zone EU-East]
R√©gion US: [Zone US-West, Zone US-East]
```

### Comment Fonctionne le topologyKey ?

Le topologyKey d√©finit **√† quel niveau** les Pods doivent √™tre ensemble ou s√©par√©s.

**Exemple avec Pod Affinity** :
- `topologyKey: kubernetes.io/hostname` ‚Üí "Sur le **m√™me n≈ìud**"
- `topologyKey: topology.kubernetes.io/zone` ‚Üí "Dans la **m√™me zone**"

**Exemple avec Pod Anti-Affinity** :
- `topologyKey: kubernetes.io/hostname` ‚Üí "Sur des **n≈ìuds diff√©rents**"
- `topologyKey: topology.kubernetes.io/zone` ‚Üí "Dans des **zones diff√©rentes**"

---

## Syntaxe de Base

### Structure G√©n√©rale - Pod Affinity

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mon-pod
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - cache
        topologyKey: kubernetes.io/hostname
  containers:
  - name: mon-conteneur
    image: nginx
```

### Structure G√©n√©rale - Pod Anti-Affinity

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mon-pod
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - mon-app
        topologyKey: kubernetes.io/hostname
  containers:
  - name: mon-conteneur
    image: nginx
```

---

## Pod Affinity - Exemples Pratiques

### Exemple 1 : Application Web et Cache sur le M√™me N≈ìud

Vous voulez que votre application web soit toujours sur le m√™me n≈ìud que Redis pour minimiser la latence.

**D√©ploiement du Cache Redis** :
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-cache
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: cache
  template:
    metadata:
      labels:
        app: redis
        role: cache
    spec:
      containers:
      - name: redis
        image: redis:7
        ports:
        - containerPort: 6379
```

**D√©ploiement de l'Application Web** :
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - redis
              - key: role
                operator: In
                values:
                - cache
            topologyKey: kubernetes.io/hostname
      containers:
      - name: nginx
        image: nginx:latest
```

**Ce que fait ce manifeste** :
- Les Pods `web-app` **doivent** √™tre sur le m√™me n≈ìud (`topologyKey: kubernetes.io/hostname`) que les Pods ayant les labels `app=redis` **et** `role=cache`
- Si Redis est sur `node1`, toutes les r√©plicas de web-app iront sur `node1`

### Exemple 2 : Dans la M√™me Zone (Plus Flexible)

Au lieu d'exiger le m√™me n≈ìud, vous voulez juste √™tre dans la m√™me zone.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app-flexible
spec:
  replicas: 5
  selector:
    matchLabels:
      app: web-flexible
  template:
    metadata:
      labels:
        app: web-flexible
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - redis
              topologyKey: topology.kubernetes.io/zone
      containers:
      - name: nginx
        image: nginx:latest
```

**Ce que fait ce manifeste** :
- Le scheduler **pr√©f√®re** placer web-app dans la m√™me **zone** que Redis
- Si impossible (zone pleine), les Pods iront dans une autre zone
- Les Pods peuvent √™tre sur des n≈ìuds diff√©rents, tant qu'ils sont dans la m√™me zone

### Exemple 3 : Colocalisation de Plusieurs Services

Un frontend, un backend et un cache qui doivent tous √™tre ensemble.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
  labels:
    tier: frontend
    app: shop
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      # Doit √™tre avec le backend
      - labelSelector:
          matchLabels:
            tier: backend
            app: shop
        topologyKey: kubernetes.io/hostname
      # Doit √™tre avec le cache
      - labelSelector:
          matchLabels:
            tier: cache
            app: shop
        topologyKey: kubernetes.io/hostname
  containers:
  - name: nginx
    image: nginx
```

**Ce que fait ce manifeste** :
- Le frontend doit √™tre sur le m√™me n≈ìud que le backend **ET** le cache
- Tous les composants de l'application `shop` seront colocalis√©s

---

## Pod Anti-Affinity - Exemples Pratiques

### Exemple 1 : Haute Disponibilit√© - R√©plicas sur Diff√©rents N≈ìuds

C'est l'usage le plus courant de Pod Anti-Affinity.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-ha
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mon-app-ha
  template:
    metadata:
      labels:
        app: mon-app-ha
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - mon-app-ha
            topologyKey: kubernetes.io/hostname
      containers:
      - name: app
        image: mon-app:latest
```

**Ce que fait ce manifeste** :
- Chaque r√©plica de `mon-app-ha` **doit** √™tre sur un n≈ìud diff√©rent
- Si vous avez 3 n≈ìuds : un Pod par n≈ìud
- Si vous avez 2 n≈ìuds : seulement 2 Pods pourront √™tre ordonnanc√©s (le 3e restera Pending)

**Diagramme** :
```
N≈ìud 1: [mon-app-ha Pod 1]
N≈ìud 2: [mon-app-ha Pod 2]
N≈ìud 3: [mon-app-ha Pod 3]
```

### Exemple 2 : Haute Disponibilit√© - Pr√©f√©rence Douce

Version plus flexible qui pr√©f√®re distribuer mais ne bloque pas le d√©ploiement.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-ha-flexible
spec:
  replicas: 5
  selector:
    matchLabels:
      app: mon-app-flexible
  template:
    metadata:
      labels:
        app: mon-app-flexible
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - mon-app-flexible
              topologyKey: kubernetes.io/hostname
      containers:
      - name: app
        image: mon-app:latest
```

**Ce que fait ce manifeste** :
- Le scheduler **pr√©f√®re** distribuer les Pods sur diff√©rents n≈ìuds
- Si vous avez 3 n≈ìuds, il mettra id√©alement 2 Pods sur certains n≈ìuds
- Tous les 5 Pods seront d√©ploy√©s m√™me si vous n'avez qu'un seul n≈ìud

### Exemple 3 : Distribution par Zone

Pour une vraie haute disponibilit√©, distribuez sur diff√©rentes zones.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-multi-zone
spec:
  replicas: 3
  selector:
    matchLabels:
      app: app-critique
  template:
    metadata:
      labels:
        app: app-critique
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - app-critique
            topologyKey: topology.kubernetes.io/zone
      containers:
      - name: app
        image: app-critique:v1
```

**Ce que fait ce manifeste** :
- Chaque r√©plica doit √™tre dans une **zone diff√©rente**
- Protection contre la panne d'une zone enti√®re

**Diagramme** :
```
Zone A: [app-critique Pod 1]
Zone B: [app-critique Pod 2]
Zone C: [app-critique Pod 3]
```

### Exemple 4 : Isoler les Environnements

Emp√™cher que les Pods de production et de test tournent ensemble.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mon-app
      env: production
  template:
    metadata:
      labels:
        app: mon-app
        env: production
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: env
                operator: In
                values:
                - development
                - test
            topologyKey: kubernetes.io/hostname
      containers:
      - name: app
        image: mon-app:prod
```

**Ce que fait ce manifeste** :
- Les Pods de production ne seront **jamais** sur le m√™me n≈ìud que les Pods de dev ou test
- Garantit l'isolation des environnements

---

## Combinaison Pod Affinity + Anti-Affinity

Vous pouvez combiner affinity et anti-affinity pour cr√©er des r√®gles sophistiqu√©es.

### Exemple : Application Web Optimis√©e

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-optimized
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-optimized
  template:
    metadata:
      labels:
        app: web-optimized
        tier: frontend
    spec:
      affinity:
        # Affinity : √ätre proche du cache
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - redis
              topologyKey: kubernetes.io/hostname
        # Anti-Affinity : R√©plicas sur diff√©rents n≈ìuds
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 90
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - web-optimized
              topologyKey: kubernetes.io/hostname
      containers:
      - name: nginx
        image: nginx:latest
```

**Ce que fait ce manifeste** :
- **Pr√©f√©rence 1 (poids 100)** : √ätre sur le m√™me n≈ìud que Redis
- **Pr√©f√©rence 2 (poids 90)** : Les r√©plicas sur des n≈ìuds diff√©rents
- Le scheduler √©quilibrera ces deux objectifs

---

## Combinaison avec Node Affinity

Vous pouvez √©galement combiner Pod Affinity/Anti-Affinity avec Node Affinity.

### Exemple : Architecture Multi-Contraintes

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-complexe
spec:
  replicas: 3
  selector:
    matchLabels:
      app: complexe
  template:
    metadata:
      labels:
        app: complexe
    spec:
      affinity:
        # Node Affinity : Seulement sur des n≈ìuds production avec SSD
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: environment
                operator: In
                values:
                - production
              - key: disktype
                operator: In
                values:
                - ssd
        # Pod Affinity : Proche du cache
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - cache
              topologyKey: kubernetes.io/hostname
        # Pod Anti-Affinity : R√©plicas distribu√©s
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 70
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - complexe
              topologyKey: kubernetes.io/hostname
      containers:
      - name: app
        image: mon-app:latest
```

**Ce manifeste combine** :
1. **Node Affinity** : Doit √™tre sur n≈ìud production + SSD
2. **Pod Affinity** : Pr√©f√®re √™tre avec le cache
3. **Pod Anti-Affinity** : Pr√©f√®re √™tre distribu√©

---

## Cas d'Usage Pratiques avec MicroK8s

### Cas 1 : Cluster de Base de Donn√©es avec Haute Disponibilit√©

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-cluster
spec:
  serviceName: postgres
  replicas: 3
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - postgres
            topologyKey: kubernetes.io/hostname
      containers:
      - name: postgres
        image: postgres:14
        env:
        - name: POSTGRES_PASSWORD
          value: "password"
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
```

**R√©sultat** :
- Chaque instance PostgreSQL sur un n≈ìud diff√©rent
- Si un n≈ìud tombe, les 2 autres instances continuent
- Protection contre la d√©faillance d'un n≈ìud

### Cas 2 : Application avec Workers et Coordinateur

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coordinator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: job-system
      role: coordinator
  template:
    metadata:
      labels:
        app: job-system
        role: coordinator
    spec:
      containers:
      - name: coordinator
        image: coordinator:latest
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: workers
spec:
  replicas: 5
  selector:
    matchLabels:
      app: job-system
      role: worker
  template:
    metadata:
      labels:
        app: job-system
        role: worker
    spec:
      affinity:
        # Les workers pr√©f√®rent √™tre avec le coordinateur
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: role
                  operator: In
                  values:
                  - coordinator
              topologyKey: kubernetes.io/hostname
        # Mais les workers pr√©f√®rent √™tre distribu√©s entre eux
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 50
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: role
                  operator: In
                  values:
                  - worker
              topologyKey: kubernetes.io/hostname
      containers:
      - name: worker
        image: worker:latest
```

**R√©sultat** :
- Workers pr√©f√®rent √™tre sur le m√™me n≈ìud que le coordinateur (faible latence)
- Mais ils pr√©f√®rent aussi √™tre distribu√©s entre eux (r√©silience)
- Le scheduler √©quilibre ces deux objectifs

### Cas 3 : Stack Monitoring (Prometheus + Grafana)

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
      component: monitoring
  template:
    metadata:
      labels:
        app: prometheus
        component: monitoring
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus:latest
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
      component: monitoring
  template:
    metadata:
      labels:
        app: grafana
        component: monitoring
    spec:
      affinity:
        # Grafana pr√©f√®re √™tre avec Prometheus
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - prometheus
              topologyKey: kubernetes.io/hostname
      containers:
      - name: grafana
        image: grafana/grafana:latest
```

**R√©sultat** :
- Grafana pr√©f√®re √™tre sur le m√™me n≈ìud que Prometheus
- R√©duit la latence des requ√™tes de donn√©es
- Si impossible, Grafana sera quand m√™me d√©ploy√© ailleurs

---

## Namespace et Pod Affinity

Par d√©faut, les Pod Affinity/Anti-Affinity s'appliquent aux Pods du **m√™me namespace**.

### Sp√©cifier des Namespaces Diff√©rents

```yaml
affinity:
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app
          operator: In
          values:
          - cache
      topologyKey: kubernetes.io/hostname
      namespaces:
      - production
      - staging
```

**Ce que fait ce manifeste** :
- Cherche les Pods avec `app=cache` dans les namespaces `production` **et** `staging`

### Tous les Namespaces

Pour chercher dans tous les namespaces, utilisez un selector vide :

```yaml
namespaceSelector: {}
```

---

## Limitations et Contraintes

### 1. Performance du Scheduler

Les r√®gles Pod Affinity/Anti-Affinity sont co√ªteuses en calcul :
- Le scheduler doit examiner tous les Pods existants
- Plus vous avez de Pods, plus c'est lent
- Limit√© √† environ 5000 Pods dans un cluster

### 2. Required Anti-Affinity peut Bloquer des D√©ploiements

```yaml
# ‚ö†Ô∏è Attention avec cette configuration
spec:
  replicas: 10  # Vous voulez 10 r√©plicas
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app: mon-app
        topologyKey: kubernetes.io/hostname
```

**Probl√®me** :
- Si vous n'avez que 5 n≈ìuds, seulement 5 Pods seront d√©ploy√©s
- Les 5 autres resteront Pending ind√©finiment

**Solution** : Utilisez `preferred` au lieu de `required`.

### 3. TopologyKey Obligatoire

Le `topologyKey` est toujours obligatoire pour Pod Affinity/Anti-Affinity.

```yaml
# ‚ùå Ne fonctionne pas
podAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchLabels:
        app: cache
  # topologyKey manquant !
```

```yaml
# ‚úÖ Correct
podAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchLabels:
        app: cache
    topologyKey: kubernetes.io/hostname
```

---

## D√©bogage et Diagnostics

### Pod en Pending √† Cause de l'Affinity

```bash
kubectl describe pod <pod-name>
```

√âv√©nements typiques :
```
Events:
  Type     Reason            Message
  ----     ------            -------
  Warning  FailedScheduling  0/3 nodes are available:
                             3 node(s) didn't match pod affinity rules.
```

Ou pour anti-affinity :
```
  Warning  FailedScheduling  0/3 nodes are available:
                             3 node(s) didn't match pod anti-affinity rules.
```

### V√©rifier les Labels des Pods Existants

```bash
# Voir tous les Pods avec leurs labels
kubectl get pods --show-labels

# Filtrer par label
kubectl get pods -l app=redis

# Dans un namespace sp√©cifique
kubectl get pods -n production -l app=cache
```

### Voir la Distribution des Pods

```bash
# Voir sur quels n≈ìuds les Pods sont d√©ploy√©s
kubectl get pods -o wide

# Compter les Pods par n≈ìud
kubectl get pods -o wide --no-headers | awk '{print $7}' | sort | uniq -c
```

---

## Bonnes Pratiques

### 1. Pr√©f√©rez Preferred √† Required

```yaml
# ‚úÖ Recommand√© : Flexible
podAntiAffinity:
  preferredDuringSchedulingIgnoredDuringExecution:
  - weight: 100
    podAffinityTerm:
      labelSelector:
        matchLabels:
          app: mon-app
      topologyKey: kubernetes.io/hostname
```

```yaml
# ‚ö†Ô∏è √Ä √©viter : Peut bloquer des d√©ploiements
podAntiAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchLabels:
        app: mon-app
    topologyKey: kubernetes.io/hostname
```

### 2. Utilisez des Labels Pr√©cis

```yaml
# ‚úÖ Bon : Labels sp√©cifiques
labelSelector:
  matchLabels:
    app: mon-app
    version: v2
    tier: frontend
```

```yaml
# ‚ùå Mauvais : Labels trop g√©n√©riques
labelSelector:
  matchLabels:
    app: app  # Trop vague
```

### 3. Planifiez pour la Croissance

Si vous utilisez `required` anti-affinity, assurez-vous d'avoir assez de n≈ìuds :

- 3 r√©plicas avec anti-affinity ‚Üí minimum 3 n≈ìuds
- 5 r√©plicas avec anti-affinity ‚Üí minimum 5 n≈ìuds

### 4. Surveillez les Pods Pending

```bash
# Script de surveillance
watch 'kubectl get pods --all-namespaces | grep Pending'
```

Si vous voyez beaucoup de Pods Pending √† cause d'affinity, revoyez vos r√®gles.

### 5. Documentez vos R√®gles

```yaml
metadata:
  name: app-web
  annotations:
    description: "Pod Affinity with Redis for low latency"
    affinity-rules: "Prefers same node as Redis, distributed replicas"
spec:
  affinity:
    # ...
```

### 6. Testez en Environnement de Dev

Avant de d√©ployer en production, testez vos r√®gles d'affinity/anti-affinity en dev :

```bash
# Cr√©er des n≈ìuds fictifs avec labels
kubectl label nodes node1 topology.kubernetes.io/zone=zone-a
kubectl label nodes node2 topology.kubernetes.io/zone=zone-b
kubectl label nodes node3 topology.kubernetes.io/zone=zone-c

# D√©ployer et v√©rifier
kubectl apply -f mon-deployment.yaml
kubectl get pods -o wide
```

### 7. Attention aux Combinaisons Contradictoires

```yaml
# ‚ö†Ô∏è R√®gles potentiellement contradictoires
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app: cache
        topologyKey: kubernetes.io/hostname  # Doit √™tre avec cache
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app: cache
        topologyKey: kubernetes.io/hostname  # Ne doit pas √™tre avec cache !?
```

Cela cr√©era une situation impossible √† satisfaire.

---

## Tableau R√©capitulatif

| Type | Usage | TopologyKey Commun | Cas d'Usage |
|------|-------|-------------------|-------------|
| **Pod Affinity** | Rapprocher les Pods | `kubernetes.io/hostname` | Performance, partage de donn√©es |
| **Pod Anti-Affinity** | √âloigner les Pods | `kubernetes.io/hostname` | Haute disponibilit√© |
| **Pod Anti-Affinity** | Distribuer par zone | `topology.kubernetes.io/zone` | R√©silience g√©ographique |
| **Required** | R√®gle obligatoire | Tout | Contraintes strictes |
| **Preferred** | R√®gle souple | Tout | Pr√©f√©rences flexibles |

---

## Exemple Complet : Application E-Commerce

Voici une architecture compl√®te d'une application e-commerce avec toutes les bonnes pratiques.

### Architecture

```
Frontend (3 r√©plicas) ‚îÄ‚îê
                       ‚îú‚îÄ> Redis Cache (1 instance)
Backend API (5 reps)  ‚îÄ‚îò
                       ‚îî‚îÄ> PostgreSQL (3 instances HA)
```

### 1. Redis Cache

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-cache
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      tier: cache
      system: ecommerce
  template:
    metadata:
      labels:
        app: redis
        tier: cache
        system: ecommerce
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
```

### 2. Frontend

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
      tier: web
      system: ecommerce
  template:
    metadata:
      labels:
        app: frontend
        tier: web
        system: ecommerce
    spec:
      affinity:
        # Affinity : Proche du cache Redis
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: redis
                  tier: cache
              topologyKey: kubernetes.io/hostname
        # Anti-Affinity : R√©plicas distribu√©s
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: frontend
              topologyKey: kubernetes.io/hostname
      containers:
      - name: nginx
        image: nginx:alpine
        resources:
          requests:
            cpu: "200m"
            memory: "256Mi"
```

### 3. Backend API

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-api
spec:
  replicas: 5
  selector:
    matchLabels:
      app: backend
      tier: api
      system: ecommerce
  template:
    metadata:
      labels:
        app: backend
        tier: api
        system: ecommerce
    spec:
      affinity:
        # Affinity : Proche du cache Redis
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 90
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: redis
              topologyKey: kubernetes.io/hostname
        # Anti-Affinity : R√©plicas distribu√©s
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: backend
              topologyKey: kubernetes.io/hostname
      containers:
      - name: api
        image: mon-api:v1
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
```

### 4. PostgreSQL (Haute Disponibilit√©)

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres
  replicas: 3
  selector:
    matchLabels:
      app: postgres
      tier: database
      system: ecommerce
  template:
    metadata:
      labels:
        app: postgres
        tier: database
        system: ecommerce
    spec:
      affinity:
        # Anti-Affinity : Instances sur diff√©rents n≈ìuds
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: postgres
            topologyKey: kubernetes.io/hostname
      containers:
      - name: postgres
        image: postgres:14
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            cpu: "1000m"
            memory: "2Gi"
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 20Gi
```

---

## Points Cl√©s √† Retenir

1. **Pod Affinity** = Rapprocher les Pods (m√™me n≈ìud, m√™me zone)
2. **Pod Anti-Affinity** = √âloigner les Pods (n≈ìuds diff√©rents, zones diff√©rentes)
3. **TopologyKey** = D√©finit le niveau de s√©paration
4. **Required vs Preferred** = Obligatoire vs Souple
5. **Haute Disponibilit√©** = Anti-affinity + `topologyKey: kubernetes.io/hostname`
6. **Performance** = Affinity pour colocaliser les services li√©s
7. **Pr√©f√©rez Preferred** = Plus flexible et r√©silient

---

## Conclusion

Les Pod Affinity et Anti-Affinity sont des outils puissants qui vous permettent de contr√¥ler finement comment vos Pods sont distribu√©s dans votre cluster. Ils compl√®tent parfaitement les Node Affinity en ajoutant une dimension relationnelle entre les Pods.

**Cas d'usage principaux** :
- **Pod Affinity** : Optimiser les performances en colocalisant les services
- **Pod Anti-Affinity** : Garantir la haute disponibilit√© en distribuant les r√©plicas

En combinant intelligemment ces m√©canismes avec Node Affinity et les resource requests, vous pouvez cr√©er des architectures Kubernetes sophistiqu√©es, performantes et r√©silientes.

Dans la prochaine section, nous d√©couvrirons les **Taints et Tolerations**, un autre m√©canisme d'ordonnancement qui fonctionne dans le sens inverse : au lieu de dire o√π les Pods **doivent** aller, les Taints disent o√π ils **ne peuvent pas** aller (sauf tol√©rance sp√©ciale).

---

## Pour Aller Plus Loin

- **Documentation officielle** : [Pod Affinity and Anti-Affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity)
- **Topology Spread Constraints** : Un m√©canisme plus r√©cent et plus simple pour distribuer les Pods
- **Descheduler** : Pour r√©√©quilibrer les Pods qui tournent d√©j√†

---

**Prochain chapitre** : 20.5 Taints et Tolerations - Marquer les n≈ìuds comme sp√©ciaux et contr√¥ler qui peut y acc√©der

‚è≠Ô∏è [Taints et Tolerations](/20-ordonnancement-avance/05-taints-et-tolerations.md)
