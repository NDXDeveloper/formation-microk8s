üîù Retour au [Sommaire](/SOMMAIRE.md)

# 20. Ordonnancement Avanc√©

## Introduction au Chapitre

L'**ordonnancement** (scheduling) est l'un des aspects les plus puissants et les plus sophistiqu√©s de Kubernetes. C'est le processus par lequel Kubernetes d√©cide **sur quel n≈ìud** un Pod doit √™tre ex√©cut√©. Bien que ce processus se fasse automatiquement et de mani√®re transparente, comprendre et ma√Ætriser les m√©canismes d'ordonnancement vous permet de :

- **Optimiser les performances** de vos applications
- **Garantir la haute disponibilit√©** de vos services
- **Contr√¥ler les co√ªts** en utilisant efficacement vos ressources
- **Isoler les workloads** pour la s√©curit√© et la conformit√©
- **Cr√©er des architectures sophistiqu√©es** adapt√©es √† vos besoins sp√©cifiques

Dans ce chapitre, nous allons explorer en profondeur tous les m√©canismes qui vous permettent d'influencer et de contr√¥ler comment vos Pods sont plac√©s dans votre cluster.

---

## Qu'est-ce que l'Ordonnancement ?

### Le Concept de Base

Lorsque vous cr√©ez un Pod dans Kubernetes (directement ou via un Deployment, StatefulSet, etc.), voici ce qui se passe :

```
1. Vous cr√©ez un Pod
        ‚Üì
2. Le Pod est cr√©√© dans l'√©tat "Pending" (en attente)
        ‚Üì
3. Le Scheduler examine le Pod
        ‚Üì
4. Le Scheduler √©value tous les n≈ìuds disponibles
        ‚Üì
5. Le Scheduler choisit le meilleur n≈ìud
        ‚Üì
6. Le Pod est assign√© √† ce n≈ìud
        ‚Üì
7. Le Kubelet du n≈ìud d√©marre le Pod
        ‚Üì
8. Le Pod passe √† l'√©tat "Running"
```

### Pourquoi l'Ordonnancement est Important ?

#### 1. **Performance et Latence**

Placer une application web pr√®s de son cache Redis peut r√©duire la latence de millisecondes √† microsecondes.

```
‚ùå Mauvais placement :
N≈ìud 1: [Application Web]
N≈ìud 2: [Cache Redis]
‚îî‚îÄ Latence r√©seau entre n≈ìuds

‚úÖ Bon placement :
N≈ìud 1: [Application Web] + [Cache Redis]
‚îî‚îÄ Communication locale ultra-rapide
```

#### 2. **Haute Disponibilit√©**

Distribuer les r√©plicas d'une application sur diff√©rents n≈ìuds garantit que votre service reste disponible m√™me en cas de panne.

```
‚ùå Pas de contr√¥le :
N≈ìud 1: [App Replica 1, App Replica 2, App Replica 3]
‚îî‚îÄ Si ce n≈ìud tombe, tout le service est down

‚úÖ Distribution contr√¥l√©e :
N≈ìud 1: [App Replica 1]
N≈ìud 2: [App Replica 2]
N≈ìud 3: [App Replica 3]
‚îî‚îÄ Le service reste disponible m√™me si un n≈ìud tombe
```

#### 3. **Optimisation des Ressources**

Certaines applications n√©cessitent du mat√©riel sp√©cifique (GPU, SSD, beaucoup de m√©moire). L'ordonnancement permet de placer ces applications sur les bons n≈ìuds.

```
N≈ìud GPU: [Applications de Machine Learning]
N≈ìud SSD: [Bases de donn√©es]
N≈ìud Standard: [Applications web classiques]
```

#### 4. **Isolation et S√©curit√©**

S√©parer les environnements de production et de d√©veloppement, ou isoler les donn√©es sensibles.

```
N≈ìuds Production: [Apps Production uniquement]
N≈ìuds Dev/Test: [Apps Dev/Test uniquement]
‚îî‚îÄ Isolation compl√®te
```

#### 5. **Gestion des Co√ªts**

Dans un environnement cloud, utiliser des instances spot (moins ch√®res mais peuvent √™tre interrompues) pour les workloads non-critiques.

```
Instances Premium: [Applications critiques]
Instances Spot: [Jobs batch, traitements non urgents]
‚îî‚îÄ √âconomies importantes
```

---

## L'Ordonnancement par D√©faut

Par d√©faut, si vous ne sp√©cifiez rien, le scheduler de Kubernetes fait un excellent travail :

```yaml
# Pod simple sans contraintes d'ordonnancement
apiVersion: v1
kind: Pod
metadata:
  name: mon-pod
spec:
  containers:
  - name: nginx
    image: nginx
```

Le scheduler va :
1. **Filtrer** les n≈ìuds qui ne peuvent pas h√©berger le Pod (pas assez de ressources, taints, etc.)
2. **Noter** les n≈ìuds restants selon plusieurs crit√®res
3. **Choisir** le n≈ìud avec le meilleur score

**Crit√®res par d√©faut** :
- √âquilibrage de la charge entre n≈ìuds
- Ressources disponibles (CPU, m√©moire)
- Pr√©sence de l'image Docker (pour √©viter de t√©l√©charger)
- R√©partition des Pods d'un m√™me Service

Pour la plupart des applications simples, **cela suffit amplement**.

---

## Quand Faut-il Intervenir ?

Vous devriez envisager de contr√¥ler l'ordonnancement dans ces situations :

### ‚úÖ Vous Devriez Intervenir Si :

1. **Haute disponibilit√© critique** : Vos r√©plicas doivent √™tre sur des n≈ìuds diff√©rents
2. **Mat√©riel sp√©cifique** : Vous avez besoin de GPU, SSD, ou haute m√©moire
3. **Performance importante** : Certains Pods doivent √™tre proches les uns des autres
4. **Isolation requise** : Production vs Dev, ou multi-tenant
5. **Co√ªts √† optimiser** : Utilisation d'instances diverses (premium, standard, spot)
6. **Conformit√©** : Donn√©es qui doivent rester dans certaines zones g√©ographiques

### ‚ùå Vous N'Avez Pas Besoin d'Intervenir Si :

1. **Application simple** : Une application web standard sans besoins sp√©cifiques
2. **Cluster homog√®ne** : Tous vos n≈ìuds sont identiques
3. **Pas d'exigences strictes** : Les performances actuelles vous conviennent
4. **Environnement de test** : Vous exp√©rimentez simplement avec Kubernetes

---

## Les M√©canismes d'Ordonnancement Disponibles

Kubernetes offre plusieurs outils pour contr√¥ler l'ordonnancement, du plus simple au plus sophistiqu√© :

### 1. **Node Selectors** (Le Plus Simple)

**Concept** : "Ce Pod doit aller sur un n≈ìud avec ce label"

**Syntaxe** :
```yaml
nodeSelector:
  disktype: ssd
```

**Quand l'utiliser** : Besoins simples, un seul crit√®re

---

### 2. **Node Affinity** (Plus Puissant)

**Concept** : "Ce Pod doit/pr√©f√®re aller sur des n≈ìuds qui correspondent √† ces r√®gles"

**Syntaxe** :
```yaml
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: disktype
          operator: In
          values:
          - ssd
          - nvme
```

**Quand l'utiliser** : R√®gles complexes (OR, NOT), pr√©f√©rences

---

### 3. **Pod Affinity** (Relations Entre Pods)

**Concept** : "Ce Pod doit/pr√©f√®re √™tre proche d'autres Pods"

**Syntaxe** :
```yaml
affinity:
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchLabels:
          app: cache
      topologyKey: kubernetes.io/hostname
```

**Quand l'utiliser** : Colocalisation pour performance

---

### 4. **Pod Anti-Affinity** (S√©paration de Pods)

**Concept** : "Ce Pod doit/pr√©f√®re √™tre √©loign√© d'autres Pods"

**Syntaxe** :
```yaml
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchLabels:
          app: mon-app
      topologyKey: kubernetes.io/hostname
```

**Quand l'utiliser** : Haute disponibilit√©, isolation

---

### 5. **Taints et Tolerations** (Restrictions d'Acc√®s)

**Concept** : "Ce n≈ìud est r√©serv√©/sp√©cial, seuls certains Pods peuvent y acc√©der"

**Syntaxe** :
```yaml
# Sur le n≈ìud (taint)
kubectl taint nodes node1 gpu=true:NoSchedule

# Sur le Pod (toleration)
tolerations:
- key: "gpu"
  operator: "Equal"
  value: "true"
  effect: "NoSchedule"
```

**Quand l'utiliser** : R√©server des n≈ìuds, maintenance, isolation stricte

---

### 6. **Topology Spread Constraints** (Distribution Uniforme)

**Concept** : "Distribuer mes Pods uniform√©ment √† travers les n≈ìuds/zones"

**Syntaxe** :
```yaml
topologySpreadConstraints:
- maxSkew: 1
  topologyKey: topology.kubernetes.io/zone
  whenUnsatisfiable: DoNotSchedule
  labelSelector:
    matchLabels:
      app: mon-app
```

**Quand l'utiliser** : Haute disponibilit√©, √©quilibrage de charge

---

## Comparaison Rapide des M√©canismes

| M√©canisme | Complexit√© | Flexibilit√© | Usage Principal |
|-----------|------------|-------------|-----------------|
| **Node Selector** | ‚≠ê Tr√®s simple | ‚≠ê‚≠ê Limit√©e | Placement basique |
| **Node Affinity** | ‚≠ê‚≠ê Moyenne | ‚≠ê‚≠ê‚≠ê‚≠ê √âlev√©e | R√®gles avanc√©es sur n≈ìuds |
| **Pod Affinity** | ‚≠ê‚≠ê‚≠ê Moyenne-√âlev√©e | ‚≠ê‚≠ê‚≠ê‚≠ê √âlev√©e | Colocalisation |
| **Pod Anti-Affinity** | ‚≠ê‚≠ê‚≠ê Moyenne-√âlev√©e | ‚≠ê‚≠ê‚≠ê‚≠ê √âlev√©e | S√©paration, HA |
| **Taints/Tolerations** | ‚≠ê‚≠ê Moyenne | ‚≠ê‚≠ê‚≠ê Moyenne | R√©servation, isolation |
| **Topology Spread** | ‚≠ê‚≠ê‚≠ê Moyenne-√âlev√©e | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Tr√®s √©lev√©e | Distribution uniforme |

---

## Direction des Contraintes

Il est important de comprendre la "direction" de chaque m√©canisme :

### Pod ‚Üí N≈ìud (Attraction)

Ces m√©canismes permettent aux **Pods d'attirer** vers certains n≈ìuds :

- **Node Selector** : "Je veux aller sur un n≈ìud avec ce label"
- **Node Affinity** : "Je veux/pr√©f√®re aller sur des n≈ìuds qui correspondent"

### Pod ‚Üî Pod (Relations)

Ces m√©canismes d√©finissent des **relations entre Pods** :

- **Pod Affinity** : "Je veux √™tre proche de ces Pods"
- **Pod Anti-Affinity** : "Je veux √™tre loin de ces Pods"
- **Topology Spread** : "Je veux √™tre distribu√© uniform√©ment avec mes pairs"

### N≈ìud ‚Üí Pod (R√©pulsion)

Ce m√©canisme permet aux **n≈ìuds de repousser** certains Pods :

- **Taints/Tolerations** : "Ce n≈ìud repousse tous les Pods, sauf ceux qui tol√®rent"

---

## Combinaison des M√©canismes

La vraie puissance vient de la **combinaison** de ces m√©canismes. Vous pouvez utiliser plusieurs contraintes sur un m√™me Pod :

```yaml
spec:
  # Node Affinity : Doit √™tre sur un n≈ìud SSD
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
    # Pod Affinity : Pr√©f√®re √™tre avec Redis
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app: redis
          topologyKey: kubernetes.io/hostname
    # Pod Anti-Affinity : R√©plicas sur n≈ìuds diff√©rents
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app: mon-app
        topologyKey: kubernetes.io/hostname
  # Topology Spread : Distribution par zone
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app: mon-app
  # Tolerations : Acc√®s √† des n≈ìuds sp√©ciaux
  tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "database"
    effect: "NoSchedule"
```

---

## Architecture de ce Chapitre

Ce chapitre est organis√© de mani√®re progressive, du plus simple au plus complexe :

### **20.1 Scheduler Kubernetes**
Comprendre le cerveau de l'ordonnancement : comment fonctionne le scheduler, son processus de d√©cision, et pourquoi il fait certains choix.

### **20.2 Node Selectors**
Le m√©canisme le plus simple pour diriger vos Pods vers des n≈ìuds sp√©cifiques en utilisant des labels.

### **20.3 Node Affinity et Anti-Affinity**
Version √©volu√©e des Node Selectors avec des r√®gles sophistiqu√©es (OR, NOT, pr√©f√©rences).

### **20.4 Pod Affinity et Anti-Affinity**
Contr√¥ler le placement des Pods **les uns par rapport aux autres** : colocalisation ou s√©paration.

### **20.5 Taints et Tolerations**
Marquer des n≈ìuds comme "sp√©ciaux" et contr√¥ler qui peut y acc√©der : r√©servation et isolation.

### **20.6 Topology Spread Constraints**
Le m√©canisme moderne pour distribuer uniform√©ment vos Pods √† travers votre infrastructure.

### **20.7 Cas d'Usage Pratiques**
Exemples concrets et complets qui combinent tous les m√©canismes pour cr√©er des architectures sophistiqu√©es.

---

## Pr√©requis pour ce Chapitre

Pour tirer le meilleur parti de ce chapitre, vous devriez avoir :

### Connaissances Requises

‚úÖ **Concepts de base Kubernetes** :
- Pods, Deployments, Services
- Namespaces
- Labels et Selectors

‚úÖ **Commandes kubectl essentielles** :
- `kubectl get`, `kubectl describe`
- `kubectl apply`, `kubectl delete`
- `kubectl logs`

‚úÖ **Architecture Kubernetes de base** :
- Control plane vs Worker nodes
- Kubelet, Scheduler, API Server

### Environnement

‚úÖ **Cluster MicroK8s fonctionnel**
- De pr√©f√©rence avec plusieurs n≈ìuds (pour tester la distribution)
- Si vous n'avez qu'un seul n≈ìud, vous pourrez quand m√™me apprendre les concepts

‚úÖ **Acc√®s kubectl configur√©**

### Pour les Sections Avanc√©es (optionnel)

Pour exp√©rimenter pleinement avec les m√©canismes avanc√©s, id√©alement :

- **Cluster multi-node** (3+ n≈ìuds) pour tester la distribution
- **Labels de topologie** configur√©s (zones, r√©gions)
- **Plusieurs types de n≈ìuds** (si possible) pour tester les affinit√©s

Mais ne vous inqui√©tez pas : m√™me avec un cluster simple, vous pourrez comprendre tous les concepts !

---

## Approche P√©dagogique de ce Chapitre

### 1. **Th√©orie Avant Pratique**

Chaque section commence par expliquer **pourquoi** et **comment** fonctionne le m√©canisme, avec des analogies et des sch√©mas.

### 2. **Exemples Progressifs**

Nous partons d'exemples simples et construisons progressivement vers des configurations complexes.

### 3. **Cas d'Usage R√©els**

Chaque m√©canisme est illustr√© avec des cas d'usage concrets que vous rencontrerez dans la vraie vie.

### 4. **Bonnes Pratiques**

Nous partageons les bonnes pratiques et les pi√®ges √† √©viter, bas√©s sur l'exp√©rience.

### 5. **Comparaisons**

Nous comparons les diff√©rents m√©canismes pour vous aider √† choisir le bon outil pour chaque situation.

---

## Vocabulaire Important

Avant de commencer, voici les termes cl√©s que vous rencontrerez :

- **Scheduler** : Le composant qui d√©cide o√π placer les Pods
- **Node** : Une machine (physique ou virtuelle) dans le cluster
- **Label** : Une paire cl√©-valeur attach√©e √† un objet (n≈ìud, Pod, etc.)
- **Selector** : Un filtre bas√© sur les labels
- **Affinity** : Attirance (rapprochement)
- **Anti-Affinity** : R√©pulsion (√©loignement)
- **Taint** : Une "souillure" sur un n≈ìud qui repousse les Pods
- **Toleration** : Une tol√©rance qui permet √† un Pod d'aller sur un n≈ìud avec taint
- **Topology** : La structure de votre infrastructure (n≈ìuds, zones, r√©gions)
- **Skew** : L'√©cart entre diff√©rents domaines de topologie
- **Pending** : √âtat d'un Pod qui attend d'√™tre ordonnanc√©
- **Eviction** : Expulsion d'un Pod d'un n≈ìud

---

## Principes Fondamentaux

Avant de plonger dans les d√©tails, voici les principes fondamentaux √† garder en t√™te :

### 1. **L'Ordonnancement est une D√©cision Ponctuelle**

Une fois qu'un Pod est plac√© sur un n≈ìud et qu'il d√©marre, il y reste. Le scheduler ne d√©place **pas** les Pods d√©j√† en cours d'ex√©cution (sauf cas particuliers avec taints `NoExecute`).

### 2. **Les Contraintes sont √âvalu√©es au Moment de l'Ordonnancement**

Les r√®gles d'affinity, node affinity, etc. sont v√©rifi√©es uniquement quand le Pod est cr√©√©, pas apr√®s.

### 3. **Plusieurs Contraintes = ET Logique**

Si vous sp√©cifiez plusieurs contraintes, elles doivent **toutes** √™tre satisfaites simultan√©ment.

### 4. **Le Scheduler Cherche le Meilleur Compromis**

Avec des contraintes "preferred" (souples), le scheduler essaie de les respecter au mieux, mais ne bloque pas le d√©ploiement.

### 5. **Pr√©f√©rez la Simplicit√©**

N'ajoutez des contraintes que si vous en avez vraiment besoin. Plus c'est complexe, plus c'est difficile √† d√©boguer.

---

## Philosophie de l'Ordonnancement

### Le Bon √âquilibre

L'ordonnancement est un √©quilibre entre plusieurs objectifs parfois contradictoires :

```
Performance       ‚Üê‚Üí  Co√ªt
    ‚Üï                  ‚Üï
Simplicit√©       ‚Üê‚Üí  Contr√¥le
    ‚Üï                  ‚Üï
Flexibilit√©      ‚Üê‚Üí  Garanties strictes
```

**Exemples de compromis** :

- **Performance vs Co√ªt** : Utiliser des n≈ìuds GPU (performants) co√ªte cher
- **Contr√¥le vs Simplicit√©** : Plus vous contr√¥lez, plus c'est complexe
- **Garanties vs Flexibilit√©** : Des contraintes strictes peuvent bloquer des d√©ploiements

### Notre Recommandation

1. **Commencez simple** : Laissez le scheduler faire son travail
2. **Ajoutez des contraintes progressivement** : Uniquement quand n√©cessaire
3. **Mesurez l'impact** : V√©rifiez que vos contraintes am√©liorent vraiment les choses
4. **Documentez** : Expliquez pourquoi vous avez fait certains choix

---

## Comment Utiliser ce Chapitre

### Pour les D√©butants

Si vous d√©couvrez l'ordonnancement :

1. Lisez **20.1 Scheduler** pour comprendre les bases
2. Commencez par **20.2 Node Selectors** (le plus simple)
3. Passez √† **20.6 Topology Spread Constraints** (moderne et simple)
4. Explorez les autres sections selon vos besoins

### Pour les Utilisateurs Interm√©diaires

Si vous connaissez d√©j√† les bases :

1. Survolez **20.1** et **20.2**
2. Concentrez-vous sur **20.3 √† 20.6** pour les m√©canismes avanc√©s
3. √âtudiez **20.7** pour les architectures compl√®tes

### Pour les Experts

Si vous ma√Ætrisez d√©j√† les concepts :

1. Allez directement √† **20.7 Cas d'Usage Pratiques**
2. Utilisez les autres sections comme r√©f√©rence
3. Adaptez les exemples √† vos besoins sp√©cifiques

---

## √Ä Retenir

1. **L'ordonnancement est automatique** par d√©faut et fonctionne bien pour la majorit√© des cas
2. **Vous pouvez l'influencer** avec diff√©rents m√©canismes selon vos besoins
3. **Commencez simple** et complexifiez uniquement si n√©cessaire
4. **Testez vos contraintes** avant de les d√©ployer en production
5. **Documentez vos choix** pour votre √©quipe et votre futur vous-m√™me

---

## Pr√™t √† Commencer ?

Maintenant que vous avez une vue d'ensemble de l'ordonnancement dans Kubernetes et de ce qui vous attend dans ce chapitre, vous √™tes pr√™t √† plonger dans les d√©tails !

Dans la prochaine section (**20.1 Scheduler Kubernetes**), nous allons d√©couvrir le fonctionnement interne du scheduler : comment il prend ses d√©cisions, quels crit√®res il utilise, et comment il choisit le meilleur n≈ìud pour chaque Pod.

---

**Bonne lecture et bon apprentissage !** üöÄ

L'ordonnancement est l'une des fonctionnalit√©s les plus √©l√©gantes de Kubernetes. En la ma√Ætrisant, vous pourrez cr√©er des architectures sophistiqu√©es, performantes et r√©silientes qui tirent pleinement parti de la puissance de l'orchestration de conteneurs.

‚è≠Ô∏è [Scheduler Kubernetes](/20-ordonnancement-avance/01-scheduler-kubernetes.md)
