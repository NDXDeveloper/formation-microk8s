üîù Retour au [Sommaire](/SOMMAIRE.md)

# 20.6 Topology Spread Constraints

## Introduction

Vous avez d√©couvert la **Pod Anti-Affinity** pour distribuer vos Pods sur diff√©rents n≈ìuds ou zones. Mais cette approche peut √™tre complexe et verbeuse. Kubernetes propose un m√©canisme plus moderne et plus simple : les **Topology Spread Constraints** (contraintes de distribution topologique).

Les Topology Spread Constraints permettent de contr√¥ler comment vos Pods sont r√©partis √† travers votre infrastructure (n≈ìuds, zones, r√©gions) de mani√®re **uniforme** et **√©quilibr√©e**, avec une syntaxe beaucoup plus simple et intuitive.

### Analogie Simple

Imaginez que vous distribuez des cartes √† des joueurs :
- **Pod Anti-Affinity** : "Cette carte ne peut pas √™tre √† c√¥t√© d'une autre carte identique"
- **Topology Spread Constraints** : "Distribuez les cartes de mani√®re √©quitable entre tous les joueurs, avec maximum 1 carte d'√©cart"

---

## Pourquoi Topology Spread Constraints ?

### Limitations de Pod Anti-Affinity

Pod Anti-Affinity est puissant mais pr√©sente des d√©fis :

```yaml
# ‚ùå Complexe et verbeux
podAntiAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchLabels:
        app: mon-app
    topologyKey: kubernetes.io/hostname
```

**Probl√®mes** :
- Syntaxe verbeuse
- Distribution "tout ou rien" (required) ou "au mieux" (preferred)
- Difficile de contr√¥ler finement l'√©quilibre
- Pas de notion de "max d'√©cart" entre les n≈ìuds

### Avantages de Topology Spread Constraints

```yaml
# ‚úÖ Plus simple et plus puissant
topologySpreadConstraints:
- maxSkew: 1
  topologyKey: kubernetes.io/hostname
  whenUnsatisfiable: DoNotSchedule
  labelSelector:
    matchLabels:
      app: mon-app
```

**Avantages** :
- Syntaxe plus claire et concise
- Contr√¥le fin de l'√©quilibre avec `maxSkew`
- Distribution vraiment uniforme
- Plus flexible et pr√©visible

---

## Concepts Fondamentaux

### 1. Topologie (Topology)

Une **topologie** d√©finit un niveau de s√©paration dans votre infrastructure, repr√©sent√© par un label sur les n≈ìuds.

**Exemples** :
- `kubernetes.io/hostname` ‚Üí N≈ìud individuel
- `topology.kubernetes.io/zone` ‚Üí Zone de disponibilit√©
- `topology.kubernetes.io/region` ‚Üí R√©gion g√©ographique

### 2. Domaine de Topologie (Topology Domain)

Un **domaine** est un ensemble de n≈ìuds partageant la m√™me valeur pour une cl√© de topologie.

**Exemple** :
```
topologyKey: topology.kubernetes.io/zone

Zone A: [node1, node2]     ‚Üê Un domaine
Zone B: [node3, node4]     ‚Üê Un autre domaine
Zone C: [node5]            ‚Üê Encore un autre domaine
```

### 3. Skew (√âcart)

Le **skew** est la diff√©rence entre le nombre de Pods dans diff√©rents domaines.

**Exemple** :
```
Zone A: 3 Pods
Zone B: 2 Pods
Zone C: 1 Pod

Skew maximum = 3 - 1 = 2
```

### 4. MaxSkew (√âcart Maximum)

Le **maxSkew** d√©finit l'√©cart maximum tol√©r√© entre le domaine le plus charg√© et le moins charg√©.

---

## Structure de Base

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mon-pod
spec:
  topologySpreadConstraints:
  - maxSkew: <nombre>
    topologyKey: <cl√©-de-topologie>
    whenUnsatisfiable: <DoNotSchedule|ScheduleAnyway>
    labelSelector:
      matchLabels:
        <label>: <valeur>
  containers:
  - name: mon-conteneur
    image: nginx
```

### Param√®tres Essentiels

#### 1. maxSkew (Obligatoire)

D√©finit l'√©cart maximum autoris√© entre les domaines.

- `maxSkew: 1` ‚Üí Distribution tr√®s uniforme (diff√©rence de 1 Pod max)
- `maxSkew: 2` ‚Üí Distribution moins stricte (diff√©rence de 2 Pods max)
- `maxSkew: 5` ‚Üí Distribution l√¢che

**R√®gle** : Plus le nombre est petit, plus la distribution est uniforme.

#### 2. topologyKey (Obligatoire)

La cl√© de label qui d√©finit les domaines.

**Valeurs courantes** :
- `kubernetes.io/hostname` ‚Üí Distribution par n≈ìud
- `topology.kubernetes.io/zone` ‚Üí Distribution par zone
- `topology.kubernetes.io/region` ‚Üí Distribution par r√©gion

#### 3. whenUnsatisfiable (Obligatoire)

D√©finit que faire si la contrainte ne peut pas √™tre satisfaite.

**DoNotSchedule** (Strict) :
- Le Pod ne sera **pas** ordonnanc√© si cela viole la contrainte
- Comportement "hard" (obligatoire)

**ScheduleAnyway** (Flexible) :
- Le Pod sera ordonnanc√© m√™me si cela viole la contrainte
- Le scheduler favorise quand m√™me une distribution √©quilibr√©e
- Comportement "soft" (pr√©f√©rence)

#### 4. labelSelector (Obligatoire)

Identifie les Pods √† consid√©rer pour le calcul de la distribution.

```yaml
labelSelector:
  matchLabels:
    app: mon-app
    version: v1
```

---

## Exemples Pratiques

### Exemple 1 : Distribution Uniforme sur les N≈ìuds

Vous voulez distribuer vos Pods √©quitablement sur tous les n≈ìuds.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 6
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: web
      containers:
      - name: nginx
        image: nginx:latest
```

**Comportement avec 3 n≈ìuds** :
```
N≈ìud 1: 2 Pods
N≈ìud 2: 2 Pods
N≈ìud 3: 2 Pods
```

**Avec maxSkew: 1** :
- Diff√©rence maximale entre n≈ìuds = 1 Pod
- Distribution tr√®s √©quilibr√©e

**Si vous aviez 7 r√©plicas** :
```
N≈ìud 1: 3 Pods  ‚Üê Un de plus
N≈ìud 2: 2 Pods
N≈ìud 3: 2 Pods
Skew = 3 - 2 = 1 ‚úÖ Respecte maxSkew: 1
```

### Exemple 2 : Distribution par Zone

Distribuer les Pods √©quitablement entre les zones de disponibilit√©.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-multi-zone
spec:
  replicas: 9
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: api
      containers:
      - name: api
        image: api:v1
```

**Comportement avec 3 zones** :
```
Zone A: 3 Pods
Zone B: 3 Pods
Zone C: 3 Pods
```

**Haute disponibilit√© garantie** : Chaque zone a le m√™me nombre de Pods (¬±1).

### Exemple 3 : Distribution Flexible

Si vous voulez une distribution √©quilibr√©e mais sans bloquer le d√©ploiement.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-flexible
spec:
  replicas: 10
  selector:
    matchLabels:
      app: worker
  template:
    metadata:
      labels:
        app: worker
    spec:
      topologySpreadConstraints:
      - maxSkew: 2
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: worker
      containers:
      - name: worker
        image: worker:latest
```

**Comportement** :
- Le scheduler **essaie** de respecter maxSkew: 2
- Si impossible (ex: pas assez de n≈ìuds), les Pods seront quand m√™me d√©ploy√©s
- `ScheduleAnyway` = pr√©f√©rence, pas obligation

**Avec 3 n≈ìuds et 10 r√©plicas** :
```
Id√©al avec maxSkew: 2
N≈ìud 1: 4 Pods
N≈ìud 2: 3 Pods
N≈ìud 3: 3 Pods
Skew = 4 - 3 = 1 ‚úÖ
```

### Exemple 4 : Plusieurs Contraintes

Vous pouvez d√©finir plusieurs contraintes simultan√©ment.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-multi-contraintes
spec:
  replicas: 12
  selector:
    matchLabels:
      app: critical-app
  template:
    metadata:
      labels:
        app: critical-app
    spec:
      topologySpreadConstraints:
      # Contrainte 1 : Distribution par zone
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: critical-app
      # Contrainte 2 : Distribution par n≈ìud
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: critical-app
      containers:
      - name: app
        image: critical-app:v1
```

**Comportement** :
- **Obligatoire** : Distribution √©quilibr√©e par zone (DoNotSchedule)
- **Pr√©f√©r√©** : Distribution √©quilibr√©e par n≈ìud (ScheduleAnyway)

**Exemple avec 3 zones de 2 n≈ìuds chacune** :
```
Zone A:
  N≈ìud 1: 2 Pods
  N≈ìud 2: 2 Pods
Zone B:
  N≈ìud 3: 2 Pods
  N≈ìud 4: 2 Pods
Zone C:
  N≈ìud 5: 2 Pods
  N≈ìud 6: 2 Pods
```

---

## Comparaison : maxSkew et whenUnsatisfiable

### Impact de maxSkew

| maxSkew | Distribution | Flexibilit√© |
|---------|--------------|-------------|
| 1 | Tr√®s uniforme | Tr√®s strict |
| 2 | √âquilibr√©e | Mod√©r√© |
| 3-5 | Acceptable | Flexible |
| >5 | L√¢che | Tr√®s flexible |

**Exemple avec 10 Pods et 4 n≈ìuds** :

```
maxSkew: 1
N≈ìud 1: 3 Pods
N≈ìud 2: 3 Pods
N≈ìud 3: 2 Pods
N≈ìud 4: 2 Pods
√âcart = 3 - 2 = 1 ‚úÖ

maxSkew: 2
N≈ìud 1: 4 Pods
N≈ìud 2: 3 Pods
N≈ìud 3: 2 Pods
N≈ìud 4: 1 Pod
√âcart = 4 - 1 = 3 ‚ùå Viole la contrainte avec DoNotSchedule
```

### Impact de whenUnsatisfiable

| whenUnsatisfiable | Comportement | Usage |
|-------------------|--------------|-------|
| DoNotSchedule | Pod reste Pending si violation | Applications critiques |
| ScheduleAnyway | Pod d√©ploy√© m√™me si violation | Applications flexibles |

---

## Cas d'Usage Pratiques avec MicroK8s

### Cas 1 : Application Web Haute Disponibilit√©

```bash
# Labelliser les n≈ìuds par zone
kubectl label nodes node1 topology.kubernetes.io/zone=zone-a
kubectl label nodes node2 topology.kubernetes.io/zone=zone-b
kubectl label nodes node3 topology.kubernetes.io/zone=zone-c
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-ha
spec:
  replicas: 6
  selector:
    matchLabels:
      app: frontend
      tier: web
  template:
    metadata:
      labels:
        app: frontend
        tier: web
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: frontend
      containers:
      - name: nginx
        image: nginx:alpine
        resources:
          requests:
            cpu: "200m"
            memory: "256Mi"
```

**R√©sultat** :
- 2 Pods par zone garantis
- Protection contre la panne d'une zone enti√®re
- Service continue m√™me si une zone est down

### Cas 2 : Workers Distribu√©s

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: workers
spec:
  replicas: 8
  selector:
    matchLabels:
      app: worker
      type: batch
  template:
    metadata:
      labels:
        app: worker
        type: batch
    spec:
      topologySpreadConstraints:
      # Distribution √©quilibr√©e sur les n≈ìuds
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: worker
            type: batch
      containers:
      - name: worker
        image: worker:v2
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
```

**Avec 4 n≈ìuds** :
```
N≈ìud 1: 2 Workers
N≈ìud 2: 2 Workers
N≈ìud 3: 2 Workers
N≈ìud 4: 2 Workers
```

**Avantages** :
- Charge CPU √©quilibr√©e
- Pas de n≈ìud surcharg√©
- Performance optimale

### Cas 3 : Base de Donn√©es R√©pliqu√©e

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-replica
spec:
  serviceName: postgres
  replicas: 3
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      topologySpreadConstraints:
      # Une instance par n≈ìud obligatoire
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: postgres
      # Distribution par zone pr√©f√©r√©e
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: postgres
      containers:
      - name: postgres
        image: postgres:14
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 20Gi
```

**R√©sultat** :
- Chaque instance PostgreSQL sur un n≈ìud diff√©rent
- Id√©alement dans des zones diff√©rentes
- R√©silience maximale

### Cas 4 : Micro-services avec D√©pendances

```yaml
---
# Service Cache
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-cache
spec:
  replicas: 3
  selector:
    matchLabels:
      app: redis
      role: cache
  template:
    metadata:
      labels:
        app: redis
        role: cache
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: redis
      containers:
      - name: redis
        image: redis:7-alpine
---
# Service API
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-backend
spec:
  replicas: 6
  selector:
    matchLabels:
      app: api
      tier: backend
  template:
    metadata:
      labels:
        app: api
        tier: backend
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: api
      affinity:
        # Pr√©f√®re √™tre avec Redis
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: redis
              topologyKey: kubernetes.io/hostname
      containers:
      - name: api
        image: api:v2
```

**R√©sultat** :
- Redis distribu√© uniform√©ment
- API distribu√©e uniform√©ment ET proche de Redis
- √âquilibre entre performance et r√©silience

---

## Comparaison : Topology Spread vs Pod Anti-Affinity

### Distribution de 6 Pods sur 3 N≈ìuds

#### Avec Pod Anti-Affinity

```yaml
# Complexe et limit√©
podAntiAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchLabels:
        app: mon-app
    topologyKey: kubernetes.io/hostname
```

**R√©sultat** :
```
N≈ìud 1: 1 Pod
N≈ìud 2: 1 Pod
N≈ìud 3: 1 Pod
3 autres Pods restent Pending ‚ùå
```

**Probl√®me** : Required anti-affinity = maximum 1 Pod par n≈ìud.

#### Avec Topology Spread Constraints

```yaml
# Simple et intelligent
topologySpreadConstraints:
- maxSkew: 1
  topologyKey: kubernetes.io/hostname
  whenUnsatisfiable: DoNotSchedule
  labelSelector:
    matchLabels:
      app: mon-app
```

**R√©sultat** :
```
N≈ìud 1: 2 Pods
N≈ìud 2: 2 Pods
N≈ìud 3: 2 Pods
Tous les Pods d√©ploy√©s ‚úÖ
```

**Avantage** : Distribution uniforme ET tous les Pods d√©ploy√©s.

### Tableau Comparatif

| Aspect | Pod Anti-Affinity | Topology Spread Constraints |
|--------|-------------------|----------------------------|
| **Syntaxe** | Verbeuse | Concise |
| **Distribution** | Tout ou rien | Uniforme avec maxSkew |
| **Flexibilit√©** | Limit√©e | Excellente |
| **Contr√¥le fin** | Non | Oui (maxSkew) |
| **Cas d'usage** | S√©paration stricte | Distribution √©quilibr√©e |
| **Complexit√©** | √âlev√©e | Moyenne |

**Recommandation** : Pr√©f√©rez Topology Spread Constraints pour distribuer uniform√©ment vos Pods.

---

## MinDomains (Kubernetes 1.25+)

Une fonctionnalit√© plus r√©cente permet de sp√©cifier un nombre minimum de domaines.

```yaml
topologySpreadConstraints:
- maxSkew: 1
  minDomains: 3
  topologyKey: topology.kubernetes.io/zone
  whenUnsatisfiable: DoNotSchedule
  labelSelector:
    matchLabels:
      app: mon-app
```

**Comportement** :
- Exige que les Pods soient r√©partis sur **au moins 3 zones**
- Si seulement 2 zones sont disponibles, les Pods restent Pending

**Usage** : Pour garantir une distribution g√©ographique minimale.

---

## Combinaison avec d'Autres M√©canismes

### Topology Spread + Node Affinity

```yaml
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app: mon-app
  containers:
  - name: app
    image: mon-app:latest
```

**Comportement** :
- Pods **uniquement** sur n≈ìuds SSD (Node Affinity)
- Distribution **uniforme** par zone (Topology Spread)

### Topology Spread + Taints

```yaml
spec:
  tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "production"
    effect: "NoSchedule"
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app: mon-app
  containers:
  - name: app
    image: mon-app:latest
```

**Comportement** :
- Pods peuvent aller sur n≈ìuds production (Toleration)
- Distribution uniforme sur ces n≈ìuds (Topology Spread)

---

## D√©bogage et Diagnostics

### Pod en Pending

```bash
kubectl describe pod <pod-name>
```

**√âv√©nements typiques** :
```
Events:
  Type     Reason            Message
  ----     ------            -------
  Warning  FailedScheduling  0/3 nodes are available:
                             3 node(s) didn't match pod topology spread constraints.
```

**Causes possibles** :
1. maxSkew trop strict (essayez d'augmenter)
2. Pas assez de n≈ìuds/zones disponibles
3. whenUnsatisfiable: DoNotSchedule trop restrictif

### V√©rifier la Distribution Actuelle

```bash
# Voir sur quels n≈ìuds les Pods tournent
kubectl get pods -l app=mon-app -o wide

# Compter les Pods par n≈ìud
kubectl get pods -l app=mon-app -o wide --no-headers | \
  awk '{print $7}' | sort | uniq -c
```

**Exemple de sortie** :
```
3 node1
3 node2
2 node3
```

Skew = 3 - 2 = 1

### V√©rifier les Labels de Topologie

```bash
# Voir les zones des n≈ìuds
kubectl get nodes -L topology.kubernetes.io/zone

# Voir tous les labels
kubectl get nodes --show-labels
```

---

## Bonnes Pratiques

### 1. Commencez avec maxSkew: 1

Pour une distribution vraiment √©quilibr√©e :

```yaml
# ‚úÖ Recommand√©
topologySpreadConstraints:
- maxSkew: 1
  topologyKey: kubernetes.io/hostname
  whenUnsatisfiable: ScheduleAnyway
  labelSelector:
    matchLabels:
      app: mon-app
```

### 2. Utilisez ScheduleAnyway pour la Flexibilit√©

√Ä moins que la distribution parfaite soit critique :

```yaml
# ‚úÖ Flexible
whenUnsatisfiable: ScheduleAnyway

# ‚ö†Ô∏è Strict - peut bloquer des d√©ploiements
whenUnsatisfiable: DoNotSchedule
```

### 3. Labellisez Correctement vos N≈ìuds

```bash
# ‚úÖ Labels de topologie clairs
kubectl label nodes node1 topology.kubernetes.io/zone=zone-a
kubectl label nodes node1 topology.kubernetes.io/rack=rack-1
kubectl label nodes node1 topology.kubernetes.io/datacenter=dc-west
```

### 4. Testez vos Contraintes

Avant de d√©ployer en production :

```bash
# D√©ployer avec 1 r√©plica
kubectl scale deployment mon-app --replicas=1

# Augmenter progressivement
kubectl scale deployment mon-app --replicas=3
kubectl scale deployment mon-app --replicas=6

# V√©rifier la distribution √† chaque √©tape
kubectl get pods -l app=mon-app -o wide
```

### 5. Documentez vos Topologies

```yaml
metadata:
  name: app-production
  annotations:
    topology-strategy: "Uniform distribution across zones (maxSkew: 1)"
    high-availability: "true"
```

### 6. Combinez Plusieurs Niveaux de Topologie

```yaml
topologySpreadConstraints:
# Niveau 1 : Zone (strict)
- maxSkew: 1
  topologyKey: topology.kubernetes.io/zone
  whenUnsatisfiable: DoNotSchedule
  labelSelector:
    matchLabels:
      app: mon-app
# Niveau 2 : N≈ìud (flexible)
- maxSkew: 1
  topologyKey: kubernetes.io/hostname
  whenUnsatisfiable: ScheduleAnyway
  labelSelector:
    matchLabels:
      app: mon-app
```

### 7. Adaptez maxSkew au Nombre de R√©plicas

| R√©plicas | Domaines | maxSkew Recommand√© |
|----------|----------|-------------------|
| 3 | 3 zones | 1 (distribution parfaite) |
| 5 | 3 zones | 1 (2-2-1) |
| 10 | 4 n≈ìuds | 1 (3-3-2-2) |
| 20 | 5 zones | 2 (acceptable) |

---

## Exemple Complet : Application E-Commerce

Architecture compl√®te utilisant Topology Spread Constraints.

### Infrastructure

```bash
# Zone A
kubectl label nodes node1 topology.kubernetes.io/zone=zone-a
kubectl label nodes node2 topology.kubernetes.io/zone=zone-a

# Zone B
kubectl label nodes node3 topology.kubernetes.io/zone=zone-b
kubectl label nodes node4 topology.kubernetes.io/zone=zone-b

# Zone C
kubectl label nodes node5 topology.kubernetes.io/zone=zone-c
kubectl label nodes node6 topology.kubernetes.io/zone=zone-c
```

### 1. Frontend (9 r√©plicas)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  labels:
    app: ecommerce
    tier: frontend
spec:
  replicas: 9
  selector:
    matchLabels:
      app: ecommerce
      tier: frontend
  template:
    metadata:
      labels:
        app: ecommerce
        tier: frontend
    spec:
      topologySpreadConstraints:
      # Distribution par zone
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: ecommerce
            tier: frontend
      # Distribution par n≈ìud
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: ecommerce
            tier: frontend
      containers:
      - name: nginx
        image: nginx:alpine
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
```

**Distribution attendue** :
```
Zone A:
  node1: 2 Pods
  node2: 1 Pod
Zone B:
  node3: 2 Pods
  node4: 1 Pod
Zone C:
  node5: 2 Pods
  node6: 1 Pod
```

### 2. Backend API (12 r√©plicas)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-api
  labels:
    app: ecommerce
    tier: backend
spec:
  replicas: 12
  selector:
    matchLabels:
      app: ecommerce
      tier: backend
  template:
    metadata:
      labels:
        app: ecommerce
        tier: backend
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: ecommerce
            tier: backend
      containers:
      - name: api
        image: ecommerce-api:v2
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
```

**Distribution attendue** :
```
Zone A: 4 Pods
Zone B: 4 Pods
Zone C: 4 Pods
```

### 3. Cache Redis (3 r√©plicas)

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis
  labels:
    app: ecommerce
    tier: cache
spec:
  serviceName: redis
  replicas: 3
  selector:
    matchLabels:
      app: ecommerce
      tier: cache
  template:
    metadata:
      labels:
        app: ecommerce
        tier: cache
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: ecommerce
            tier: cache
      containers:
      - name: redis
        image: redis:7-alpine
        resources:
          requests:
            cpu: "200m"
            memory: "512Mi"
```

**Distribution attendue** :
```
Zone A: 1 Pod (redis-0)
Zone B: 1 Pod (redis-1)
Zone C: 1 Pod (redis-2)
```

---

## Sc√©narios Avanc√©s

### Sc√©nario 1 : Rolling Update et Topology Spread

Lors d'un rolling update, Kubernetes respecte les Topology Spread Constraints.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  replicas: 6
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: mon-app
  template:
    metadata:
      labels:
        app: mon-app
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: mon-app
      containers:
      - name: app
        image: mon-app:v2
```

**Comportement** :
- Les nouveaux Pods respectent la distribution
- Les anciens Pods sont supprim√©s progressivement
- La distribution reste √©quilibr√©e pendant la mise √† jour

### Sc√©nario 2 : Scaling et Topology Spread

```bash
# Scale up
kubectl scale deployment mon-app --replicas=12

# Kubernetes distribue automatiquement les nouveaux Pods
```

**Avec 4 n≈ìuds et maxSkew: 1** :
```
Avant (6 Pods):
node1: 2, node2: 2, node3: 1, node4: 1

Apr√®s (12 Pods):
node1: 3, node2: 3, node3: 3, node4: 3
```

---

## Points Cl√©s √† Retenir

1. **Plus simple que Pod Anti-Affinity** pour la distribution uniforme
2. **maxSkew** contr√¥le l'√©cart maximum entre domaines
3. **topologyKey** d√©finit le niveau de distribution (n≈ìud, zone, r√©gion)
4. **whenUnsatisfiable** d√©finit le comportement (strict ou flexible)
5. **Distribution intelligente** : Kubernetes calcule automatiquement
6. **Combinable** avec Node Affinity, Taints, etc.
7. **Pr√©f√©rez ScheduleAnyway** pour √©viter les Pods Pending

---

## Conclusion

Les **Topology Spread Constraints** repr√©sentent une √©volution majeure dans l'ordonnancement Kubernetes. Ils offrent une mani√®re simple, √©l√©gante et puissante de distribuer uniform√©ment vos Pods √† travers votre infrastructure.

**Avantages principaux** :
- Syntaxe claire et concise
- Distribution vraiment uniforme
- Contr√¥le fin avec maxSkew
- Plus flexible que Pod Anti-Affinity
- Meilleure utilisation des ressources

**Quand les utiliser** :
- Haute disponibilit√© (distribution par zone)
- √âquilibrage de charge (distribution par n≈ìud)
- Optimisation des ressources
- R√©silience g√©ographique

Les Topology Spread Constraints sont le m√©canisme moderne et recommand√© pour distribuer vos Pods. Ils remplacent avantageusement Pod Anti-Affinity dans la majorit√© des cas d'usage.

Dans la prochaine section, nous conclurons ce chapitre sur l'ordonnancement avec des **cas d'usage pratiques** qui combinent tous les m√©canismes que nous avons appris.

---

## Pour Aller Plus Loin

- **Documentation officielle** : [Pod Topology Spread Constraints](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/)
- **Blog Kubernetes** : [Introducing PodTopologySpread](https://kubernetes.io/blog/2020/05/introducing-podtopologyspread/)
- **√âvolutions** : Suivre les nouvelles fonctionnalit√©s (minDomains, nodeAffinityPolicy, etc.)

---

**Prochain chapitre** : 20.7 Cas d'Usage Pratiques - Combinaison de tous les m√©canismes d'ordonnancement

‚è≠Ô∏è [Cas d'usage pratiques](/20-ordonnancement-avance/07-cas-dusage-pratiques.md)
