ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 20.7 Cas d'Usage Pratiques

## Introduction

Nous avons explorÃ© individuellement les diffÃ©rents mÃ©canismes d'ordonnancement de Kubernetes :
- **Scheduler** : Le cerveau qui prend les dÃ©cisions
- **Node Selectors** : Direction simple vers des nÅ“uds spÃ©cifiques
- **Node Affinity** : RÃ¨gles avancÃ©es pour le placement sur les nÅ“uds
- **Pod Affinity/Anti-Affinity** : Relations entre Pods
- **Taints/Tolerations** : Restriction d'accÃ¨s aux nÅ“uds
- **Topology Spread Constraints** : Distribution uniforme

Mais la vraie puissance vient de la **combinaison** de ces mÃ©canismes. Dans cette section, nous allons voir des cas d'usage pratiques et rÃ©alistes qui montrent comment orchestrer intelligemment vos applications.

---

## Vue d'Ensemble des MÃ©canismes

### Tableau de DÃ©cision Rapide

| Besoin | MÃ©canisme RecommandÃ© |
|--------|---------------------|
| "Ce Pod doit aller sur un type de nÅ“ud spÃ©cifique" | Node Selector ou Node Affinity |
| "Ce Pod doit Ãªtre proche d'autres Pods" | Pod Affinity |
| "Ce Pod doit Ãªtre Ã©loignÃ© d'autres Pods" | Pod Anti-Affinity ou Topology Spread |
| "RÃ©server un nÅ“ud pour un usage spÃ©cial" | Taints + Tolerations |
| "Distribuer uniformÃ©ment mes Pods" | Topology Spread Constraints |
| "Haute disponibilitÃ© garantie" | Topology Spread + Node Affinity |
| "Isolation stricte" | Taints + Node Affinity |

---

## Cas d'Usage 1 : Application Web Classique (3 Tiers)

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Load Balancer                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend (NGINX) - 6 rÃ©plicas              â”‚
â”‚  - Distribution uniforme par zone           â”‚
â”‚  - NÅ“uds standards                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend API - 9 rÃ©plicas                   â”‚
â”‚  - Distribution uniforme                    â”‚
â”‚  - Proche du cache pour performance         â”‚
â”‚  - NÅ“uds avec plus de CPU                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redis Cache       â”‚  PostgreSQL            â”‚
â”‚  - 3 rÃ©plicas      â”‚  - 3 instances HA      â”‚
â”‚  - Une par zone    â”‚  - Une par zone        â”‚
â”‚  - SSD requis      â”‚  - SSD + haute mÃ©moire â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configuration des NÅ“uds

```bash
# Zone A - NÅ“uds standards
kubectl label nodes node1 topology.kubernetes.io/zone=zone-a
kubectl label nodes node1 node-type=standard cpu-class=medium

kubectl label nodes node2 topology.kubernetes.io/zone=zone-a
kubectl label nodes node2 node-type=standard cpu-class=medium

# Zone B - NÅ“uds standards
kubectl label nodes node3 topology.kubernetes.io/zone=zone-b
kubectl label nodes node3 node-type=standard cpu-class=medium

kubectl label nodes node4 topology.kubernetes.io/zone=zone-b
kubectl label nodes node4 node-type=standard cpu-class=medium

# Zone C - NÅ“uds pour donnÃ©es (SSD + haute mÃ©moire)
kubectl label nodes node5 topology.kubernetes.io/zone=zone-c
kubectl label nodes node5 node-type=data disktype=ssd memory-class=high

kubectl label nodes node6 topology.kubernetes.io/zone=zone-c
kubectl label nodes node6 node-type=data disktype=ssd memory-class=high

# Taints sur les nÅ“uds data (rÃ©servÃ©s)
kubectl taint nodes node5 workload=data:NoSchedule
kubectl taint nodes node6 workload=data:NoSchedule
```

### 1. DÃ©ploiement Frontend

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: webapp
  labels:
    app: webapp
    tier: frontend
spec:
  replicas: 6
  selector:
    matchLabels:
      app: webapp
      tier: frontend
  template:
    metadata:
      labels:
        app: webapp
        tier: frontend
    spec:
      # Topology Spread : Distribution uniforme par zone
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: webapp
            tier: frontend
      # Node Affinity : PrÃ©fÃ¨re les nÅ“uds standards
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: node-type
                operator: In
                values:
                - standard
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "200m"
            memory: "256Mi"
```

**RÃ©sultat attendu** :
```
Zone A: 2 Pods frontend (node1, node2)
Zone B: 2 Pods frontend (node3, node4)
Zone C: 2 Pods frontend (rÃ©partis si possible, sinon ailleurs)
```

### 2. DÃ©ploiement Backend API

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-api
  namespace: webapp
  labels:
    app: webapp
    tier: backend
spec:
  replicas: 9
  selector:
    matchLabels:
      app: webapp
      tier: backend
  template:
    metadata:
      labels:
        app: webapp
        tier: backend
    spec:
      # Topology Spread : Distribution uniforme
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: webapp
            tier: backend
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: webapp
            tier: backend
      # Pod Affinity : Proche du cache pour performance
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: webapp
                  tier: cache
              topologyKey: kubernetes.io/hostname
        # Node Affinity : CPU moyen minimum
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: cpu-class
                operator: In
                values:
                - medium
                - high
      containers:
      - name: api
        image: webapp-api:v2.1.0
        ports:
        - containerPort: 8080
        env:
        - name: REDIS_HOST
          value: "redis-service.webapp.svc.cluster.local"
        - name: DB_HOST
          value: "postgres-service.webapp.svc.cluster.local"
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
```

**RÃ©sultat attendu** :
```
3 Pods par zone, idÃ©alement proches de Redis
Performance optimale grÃ¢ce Ã  la colocalisation avec le cache
```

### 3. Redis Cache

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis
  namespace: webapp
  labels:
    app: webapp
    tier: cache
spec:
  serviceName: redis
  replicas: 3
  selector:
    matchLabels:
      app: webapp
      tier: cache
  template:
    metadata:
      labels:
        app: webapp
        tier: cache
    spec:
      # Tolerations : AccÃ¨s aux nÅ“uds data
      tolerations:
      - key: "workload"
        operator: "Equal"
        value: "data"
        effect: "NoSchedule"
      # Topology Spread : Une instance par zone
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: webapp
            tier: cache
      # Pod Anti-Affinity : Instances sur nÅ“uds diffÃ©rents
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: webapp
                tier: cache
            topologyKey: kubernetes.io/hostname
        # Node Affinity : SSD requis
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        resources:
          requests:
            cpu: "200m"
            memory: "512Mi"
          limits:
            cpu: "500m"
            memory: "1Gi"
        volumeMounts:
        - name: data
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
```

### 4. PostgreSQL

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: webapp
  labels:
    app: webapp
    tier: database
spec:
  serviceName: postgres
  replicas: 3
  selector:
    matchLabels:
      app: webapp
      tier: database
  template:
    metadata:
      labels:
        app: webapp
        tier: database
    spec:
      # Tolerations : AccÃ¨s aux nÅ“uds data
      tolerations:
      - key: "workload"
        operator: "Equal"
        value: "data"
        effect: "NoSchedule"
      # Topology Spread : Distribution par zone
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: webapp
            tier: database
      # Pod Anti-Affinity : Instances sÃ©parÃ©es
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: webapp
                tier: database
            topologyKey: kubernetes.io/hostname
        # Node Affinity : SSD + Haute mÃ©moire
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
              - key: memory-class
                operator: In
                values:
                - high
      containers:
      - name: postgres
        image: postgres:14
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        - name: POSTGRES_DB
          value: webapp
        resources:
          requests:
            cpu: "1000m"
            memory: "4Gi"
          limits:
            cpu: "2000m"
            memory: "8Gi"
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 50Gi
```

### Services

```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
  namespace: webapp
spec:
  type: LoadBalancer
  selector:
    app: webapp
    tier: frontend
  ports:
  - port: 80
    targetPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: backend-service
  namespace: webapp
spec:
  type: ClusterIP
  selector:
    app: webapp
    tier: backend
  ports:
  - port: 8080
    targetPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
  namespace: webapp
spec:
  type: ClusterIP
  clusterIP: None  # Headless service for StatefulSet
  selector:
    app: webapp
    tier: cache
  ports:
  - port: 6379
    targetPort: 6379
---
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
  namespace: webapp
spec:
  type: ClusterIP
  clusterIP: None  # Headless service for StatefulSet
  selector:
    app: webapp
    tier: database
  ports:
  - port: 5432
    targetPort: 5432
```

### RÃ©sumÃ© des Choix d'Ordonnancement

| Composant | MÃ©canismes UtilisÃ©s | Justification |
|-----------|---------------------|---------------|
| **Frontend** | Topology Spread, Node Affinity (preferred) | Distribution uniforme pour HA, prÃ©fÃ¨re nÅ“uds standards |
| **Backend** | Topology Spread, Pod Affinity, Node Affinity | Distribution + colocalisation avec cache + CPU suffisant |
| **Redis** | Taints/Tolerations, Topology Spread, Anti-Affinity, Node Affinity | RÃ©servÃ© sur nÅ“uds data SSD, une par zone |
| **PostgreSQL** | Taints/Tolerations, Topology Spread, Anti-Affinity, Node Affinity | RÃ©servÃ© sur nÅ“uds data SSD haute mÃ©moire, HA stricte |

---

## Cas d'Usage 2 : Plateforme ML/AI

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Jupyter Notebooks - 2 rÃ©plicas              â”‚
â”‚  - NÅ“uds GPU requis                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Training Jobs - Variables                   â”‚
â”‚  - GPU requis, tolÃ¨re interruptions          â”‚
â”‚  - Peut utiliser nÅ“uds spot                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Serving API - 3 rÃ©plicas              â”‚
â”‚  - CPU uniquement                            â”‚
â”‚  - Haute disponibilitÃ©                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Storage (MinIO) - 3 instances          â”‚
â”‚  - SSD requis                                â”‚
â”‚  - Distribution par zone                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configuration des NÅ“uds

```bash
# NÅ“uds GPU (onÃ©reux, rÃ©servÃ©s)
kubectl label nodes gpu-node1 hardware=gpu gpu-type=v100
kubectl taint nodes gpu-node1 hardware=gpu:NoSchedule
kubectl label nodes gpu-node2 hardware=gpu gpu-type=v100
kubectl taint nodes gpu-node2 hardware=gpu:NoSchedule

# NÅ“uds Spot (moins chers, peuvent Ãªtre interrompus)
kubectl label nodes spot-node1 instance-type=spot
kubectl label nodes spot-node1 hardware=gpu gpu-type=t4
kubectl taint nodes spot-node1 spot=true:PreferNoSchedule

# NÅ“uds CPU standards
kubectl label nodes cpu-node1 hardware=cpu
kubectl label nodes cpu-node2 hardware=cpu
kubectl label nodes cpu-node3 hardware=cpu

# NÅ“uds storage
kubectl label nodes storage-node1 disktype=ssd topology.kubernetes.io/zone=zone-a
kubectl label nodes storage-node2 disktype=ssd topology.kubernetes.io/zone=zone-b
kubectl label nodes storage-node3 disktype=ssd topology.kubernetes.io/zone=zone-c
```

### 1. Jupyter Notebooks (DÃ©veloppement)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jupyter
  namespace: ml-platform
spec:
  replicas: 2
  selector:
    matchLabels:
      app: jupyter
  template:
    metadata:
      labels:
        app: jupyter
    spec:
      # Toleration : AccÃ¨s aux GPU
      tolerations:
      - key: "hardware"
        operator: "Equal"
        value: "gpu"
        effect: "NoSchedule"
      # Node Affinity : GPU requis
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: hardware
                operator: In
                values:
                - gpu
              - key: gpu-type
                operator: In
                values:
                - v100  # GPU premium uniquement
      containers:
      - name: jupyter
        image: jupyter/tensorflow-notebook:latest
        ports:
        - containerPort: 8888
        resources:
          requests:
            cpu: "2000m"
            memory: "8Gi"
            nvidia.com/gpu: 1
          limits:
            cpu: "4000m"
            memory: "16Gi"
            nvidia.com/gpu: 1
```

### 2. Training Jobs (Batch)

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: model-training
  namespace: ml-platform
spec:
  parallelism: 3
  completions: 3
  template:
    metadata:
      labels:
        app: training
        job-type: batch
    spec:
      restartPolicy: OnFailure
      # Tolerations : Peut utiliser GPU standard ET spot
      tolerations:
      - key: "hardware"
        operator: "Equal"
        value: "gpu"
        effect: "NoSchedule"
      - key: "spot"
        operator: "Equal"
        value: "true"
        effect: "PreferNoSchedule"
      # Node Affinity : GPU requis, prÃ©fÃ¨re spot pour Ã©conomiser
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            # Option 1 : GPU standard
            - matchExpressions:
              - key: hardware
                operator: In
                values:
                - gpu
          preferredDuringSchedulingIgnoredDuringExecution:
          # PrÃ©fÃ¨re les instances spot (moins chÃ¨res)
          - weight: 100
            preference:
              matchExpressions:
              - key: instance-type
                operator: In
                values:
                - spot
      containers:
      - name: trainer
        image: ml-training:latest
        command: ["python", "train.py"]
        resources:
          requests:
            cpu: "4000m"
            memory: "16Gi"
            nvidia.com/gpu: 1
          limits:
            nvidia.com/gpu: 1
```

### 3. Model Serving API

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: model-serving
  namespace: ml-platform
spec:
  replicas: 3
  selector:
    matchLabels:
      app: model-serving
  template:
    metadata:
      labels:
        app: model-serving
    spec:
      # Topology Spread : Distribution pour HA
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: model-serving
      # Node Affinity : CPU uniquement (pas de GPU gaspillÃ©)
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: hardware
                operator: In
                values:
                - cpu
      containers:
      - name: api
        image: model-serving-api:v1
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: "1000m"
            memory: "2Gi"
          limits:
            cpu: "2000m"
            memory: "4Gi"
```

### 4. Data Storage (MinIO)

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: minio
  namespace: ml-platform
spec:
  serviceName: minio
  replicas: 3
  selector:
    matchLabels:
      app: minio
  template:
    metadata:
      labels:
        app: minio
    spec:
      # Topology Spread : Une instance par zone
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: minio
      # Pod Anti-Affinity : Instances sÃ©parÃ©es
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: minio
            topologyKey: kubernetes.io/hostname
        # Node Affinity : SSD requis
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
      containers:
      - name: minio
        image: minio/minio:latest
        args:
        - server
        - /data
        ports:
        - containerPort: 9000
        resources:
          requests:
            cpu: "500m"
            memory: "2Gi"
          limits:
            cpu: "1000m"
            memory: "4Gi"
        volumeMounts:
        - name: data
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 100Gi
```

### RÃ©sumÃ© des Choix

| Composant | StratÃ©gie | Ã‰conomie vs Performance |
|-----------|-----------|------------------------|
| **Jupyter** | GPU premium uniquement | Performance (dÃ©veloppement) |
| **Training** | GPU spot prÃ©fÃ©rÃ© | Ã‰conomie (jobs batch) |
| **Serving** | CPU, distribution HA | EfficacitÃ© (pas de GPU gaspillÃ©) |
| **Storage** | SSD, distribution gÃ©ographique | FiabilitÃ© + Performance |

---

## Cas d'Usage 3 : Multi-Tenant SaaS Platform

### Architecture

Plusieurs clients (tenants) sur la mÃªme infrastructure, avec isolation et garanties.

```
Tenant A (Premium)    Tenant B (Standard)    Tenant C (Free)
     â†“                      â†“                      â†“
NÅ“uds dÃ©diÃ©s          NÅ“uds partagÃ©s         NÅ“uds best-effort
SSD + haute mÃ©moire   Standards              Spot instances
```

### Configuration des NÅ“uds

```bash
# NÅ“uds Premium (dÃ©diÃ©s, haute performance)
kubectl label nodes premium-node1 tier=premium disktype=nvme memory-class=high
kubectl taint nodes premium-node1 tier=premium:NoSchedule

kubectl label nodes premium-node2 tier=premium disktype=nvme memory-class=high
kubectl taint nodes premium-node2 tier=premium:NoSchedule

# NÅ“uds Standard (partagÃ©s)
kubectl label nodes standard-node1 tier=standard disktype=ssd
kubectl label nodes standard-node2 tier=standard disktype=ssd
kubectl label nodes standard-node3 tier=standard disktype=ssd

# NÅ“uds Free (best-effort, spot)
kubectl label nodes free-node1 tier=free instance-type=spot
kubectl taint nodes free-node1 tier=free:PreferNoSchedule
```

### 1. Application Tenant Premium

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tenant-a-app
  namespace: tenant-a
  labels:
    tenant: tenant-a
    tier: premium
spec:
  replicas: 3
  selector:
    matchLabels:
      tenant: tenant-a
      app: webapp
  template:
    metadata:
      labels:
        tenant: tenant-a
        app: webapp
        tier: premium
    spec:
      # Toleration : AccÃ¨s aux nÅ“uds premium
      tolerations:
      - key: "tier"
        operator: "Equal"
        value: "premium"
        effect: "NoSchedule"
      # Topology Spread : Distribution uniforme
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            tenant: tenant-a
      # Pod Anti-Affinity : Isolation des autres tenants
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          # Ne jamais Ãªtre avec d'autres tenants
          - labelSelector:
              matchExpressions:
              - key: tenant
                operator: NotIn
                values:
                - tenant-a
            topologyKey: kubernetes.io/hostname
        # Node Affinity : NÅ“uds premium uniquement
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: tier
                operator: In
                values:
                - premium
              - key: disktype
                operator: In
                values:
                - nvme
      # Priority Class : Haute prioritÃ©
      priorityClassName: high-priority
      containers:
      - name: app
        image: webapp:premium
        resources:
          requests:
            cpu: "2000m"
            memory: "4Gi"
          limits:
            cpu: "4000m"
            memory: "8Gi"
```

### 2. Application Tenant Standard

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tenant-b-app
  namespace: tenant-b
  labels:
    tenant: tenant-b
    tier: standard
spec:
  replicas: 2
  selector:
    matchLabels:
      tenant: tenant-b
      app: webapp
  template:
    metadata:
      labels:
        tenant: tenant-b
        app: webapp
        tier: standard
    spec:
      # Topology Spread : Distribution Ã©quilibrÃ©e
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            tenant: tenant-b
      # Node Affinity : NÅ“uds standard
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: tier
                operator: In
                values:
                - standard
      # Priority Class : PrioritÃ© moyenne
      priorityClassName: medium-priority
      containers:
      - name: app
        image: webapp:standard
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
```

### 3. Application Tenant Free

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tenant-c-app
  namespace: tenant-c
  labels:
    tenant: tenant-c
    tier: free
spec:
  replicas: 1
  selector:
    matchLabels:
      tenant: tenant-c
      app: webapp
  template:
    metadata:
      labels:
        tenant: tenant-c
        app: webapp
        tier: free
    spec:
      # Toleration : Accepte les nÅ“uds free
      tolerations:
      - key: "tier"
        operator: "Equal"
        value: "free"
        effect: "PreferNoSchedule"
      # Node Affinity : PrÃ©fÃ¨re les nÅ“uds free
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: tier
                operator: In
                values:
                - free
      # Priority Class : Basse prioritÃ©
      priorityClassName: low-priority
      containers:
      - name: app
        image: webapp:free
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "200m"
            memory: "512Mi"
```

### PriorityClasses

```yaml
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000
globalDefault: false
description: "High priority for premium tenants"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: medium-priority
value: 500
globalDefault: false
description: "Medium priority for standard tenants"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 100
globalDefault: false
description: "Low priority for free tenants"
```

### RÃ©sumÃ© de l'Isolation

| Tenant | NÅ“uds | Isolation | Garanties |
|--------|-------|-----------|-----------|
| **Premium** | DÃ©diÃ©s + Taints | Stricte (Anti-Affinity) | Ressources garanties, haute performance |
| **Standard** | PartagÃ©s | ModÃ©rÃ©e | Ressources raisonnables |
| **Free** | Best-effort/Spot | Aucune | Aucune garantie, peut Ãªtre Ã©vincÃ© |

---

## Cas d'Usage 4 : Environnements Dev/Staging/Production

### Architecture par Environnement

```
Production        Staging          Development
    â†“                â†“                  â†“
NÅ“uds dÃ©diÃ©s    NÅ“uds partagÃ©s    NÅ“uds partagÃ©s
Haute perf      Performance       Best-effort
Taints strict   Taints modÃ©rÃ©     Pas de taints
```

### Configuration des NÅ“uds

```bash
# Production - NÅ“uds dÃ©diÃ©s, haute qualitÃ©
kubectl label nodes prod-node1 environment=production disktype=ssd
kubectl taint nodes prod-node1 environment=production:NoSchedule

kubectl label nodes prod-node2 environment=production disktype=ssd
kubectl taint nodes prod-node2 environment=production:NoSchedule

# Staging - NÅ“uds dÃ©diÃ©s, qualitÃ© moyenne
kubectl label nodes staging-node1 environment=staging
kubectl taint nodes staging-node1 environment=staging:PreferNoSchedule

# Dev - NÅ“uds partagÃ©s, best-effort
kubectl label nodes dev-node1 environment=development
kubectl label nodes dev-node2 environment=development
```

### DÃ©ploiement Multi-Environnement

```yaml
---
# Production
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api
  namespace: production
spec:
  replicas: 5
  selector:
    matchLabels:
      app: api
      env: production
  template:
    metadata:
      labels:
        app: api
        env: production
    spec:
      tolerations:
      - key: "environment"
        operator: "Equal"
        value: "production"
        effect: "NoSchedule"
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: api
            env: production
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: env
                operator: In
                values:
                - staging
                - development
            topologyKey: kubernetes.io/hostname
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: environment
                operator: In
                values:
                - production
              - key: disktype
                operator: In
                values:
                - ssd
      priorityClassName: production-priority
      containers:
      - name: api
        image: api:v2.1.0
        resources:
          requests:
            cpu: "1000m"
            memory: "2Gi"
          limits:
            cpu: "2000m"
            memory: "4Gi"
---
# Staging
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api
  namespace: staging
spec:
  replicas: 2
  selector:
    matchLabels:
      app: api
      env: staging
  template:
    metadata:
      labels:
        app: api
        env: staging
    spec:
      tolerations:
      - key: "environment"
        operator: "Equal"
        value: "staging"
        effect: "PreferNoSchedule"
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  env: production
              topologyKey: kubernetes.io/hostname
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: environment
                operator: In
                values:
                - staging
      priorityClassName: staging-priority
      containers:
      - name: api
        image: api:v2.1.0-staging
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
---
# Development
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api
  namespace: development
spec:
  replicas: 1
  selector:
    matchLabels:
      app: api
      env: development
  template:
    metadata:
      labels:
        app: api
        env: development
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: environment
                operator: In
                values:
                - development
      priorityClassName: dev-priority
      containers:
      - name: api
        image: api:latest
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "1Gi"
```

---

## Bonnes Pratiques GÃ©nÃ©rales

### 1. Commencez Simple, Complexifiez Progressivement

```yaml
# âœ… Phase 1 : Simple
nodeSelector:
  disktype: ssd

# âœ… Phase 2 : Plus sophistiquÃ© si nÃ©cessaire
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: disktype
          operator: In
          values:
          - ssd
          - nvme

# âœ… Phase 3 : Complet si vraiment nÃ©cessaire
affinity:
  nodeAffinity: ...
  podAntiAffinity: ...
topologySpreadConstraints: ...
tolerations: ...
```

### 2. Documentez vos Choix

```yaml
metadata:
  name: critical-app
  annotations:
    scheduling-strategy: |
      - Node Affinity: SSD required for database performance
      - Topology Spread: maxSkew=1 for high availability across zones
      - Pod Anti-Affinity: Ensure replicas on different nodes
      - Tolerations: Access to dedicated database nodes
    last-reviewed: "2024-01-15"
    owner: "platform-team"
```

### 3. Utilisez des Valeurs CohÃ©rentes

```yaml
# âœ… Bon : Labels et valeurs standardisÃ©s
labels:
  environment: production    # production, staging, development
  tier: backend             # frontend, backend, cache, database
  sensitivity: high         # high, medium, low
```

### 4. PrÃ©fÃ©rez ScheduleAnyway sauf NÃ©cessitÃ©

```yaml
# âœ… Flexible - RecommandÃ©
topologySpreadConstraints:
- maxSkew: 1
  whenUnsatisfiable: ScheduleAnyway

# âš ï¸ Strict - Uniquement si vraiment nÃ©cessaire
topologySpreadConstraints:
- maxSkew: 1
  whenUnsatisfiable: DoNotSchedule
```

### 5. Testez dans un Environnement Non-Critique

Avant de dÃ©ployer en production :

```bash
# 1. CrÃ©er un namespace de test
kubectl create namespace scheduling-test

# 2. DÃ©ployer avec vos rÃ¨gles
kubectl apply -f deployment.yaml -n scheduling-test

# 3. VÃ©rifier la distribution
kubectl get pods -n scheduling-test -o wide

# 4. Simuler une panne
kubectl drain node1 --ignore-daemonsets

# 5. VÃ©rifier le rebalancement
kubectl get pods -n scheduling-test -o wide
```

### 6. Surveillez et Ajustez

```bash
# Pods en Pending (signe de contraintes trop strictes)
kubectl get pods --all-namespaces | grep Pending

# Distribution actuelle
kubectl get pods -l app=mon-app -o wide | \
  awk '{print $7}' | sort | uniq -c

# Ã‰vÃ©nements de scheduling
kubectl get events --sort-by='.lastTimestamp' | \
  grep FailedScheduling
```

---

## Checklist de DÃ©cision

### Pour Chaque Application, Posez-vous :

**1. Haute DisponibilitÃ© ?**
- âœ… Oui â†’ Topology Spread Constraints + Pod Anti-Affinity
- âŒ Non â†’ Pas nÃ©cessaire

**2. Performance Critique ?**
- âœ… Oui â†’ Node Affinity (SSD, GPU, haute mÃ©moire) + Pod Affinity (colocalisation)
- âŒ Non â†’ Node Selector simple suffit

**3. Isolation NÃ©cessaire ?**
- âœ… Oui â†’ Taints + Tolerations + Pod Anti-Affinity
- âŒ Non â†’ Partage OK

**4. CoÃ»t Important ?**
- âœ… Oui â†’ Utiliser Spot instances avec Tolerations appropriÃ©es
- âŒ Non â†’ NÅ“uds standards

**5. Multi-Zone ?**
- âœ… Oui â†’ Topology Spread avec topologyKey: zone
- âŒ Non â†’ Topology Spread par hostname suffit

---

## Tableau RÃ©capitulatif : Quand Utiliser Quoi

| ScÃ©nario | Solution RecommandÃ©e |
|----------|---------------------|
| **HA simple** | Topology Spread Constraints (maxSkew: 1) |
| **HA stricte** | Topology Spread + Pod Anti-Affinity |
| **Performance** | Node Affinity (SSD, GPU) + Pod Affinity (colocalisation) |
| **Isolation** | Taints + Tolerations |
| **Multi-tenant** | Namespaces + Taints + Node Affinity + Priority |
| **Ã‰conomie** | Spot instances + Tolerations PreferNoSchedule |
| **Dev vs Prod** | Taints diffÃ©rents + Node Affinity |
| **DonnÃ©es critiques** | Node Affinity (SSD) + Topology Spread (zones) |

---

## Conclusion

L'ordonnancement avancÃ© dans Kubernetes est comme un puzzle : chaque piÃ¨ce (Node Selector, Affinity, Taints, Topology Spread) a son rÃ´le, et c'est en les combinant intelligemment que vous crÃ©ez une architecture robuste, performante et Ã©conomique.

### Points ClÃ©s Ã  Retenir

1. **Commencez simple** : Node Selector pour dÃ©buter
2. **Complexifiez si nÃ©cessaire** : Ajoutez Affinity, Taints, etc. selon vos besoins
3. **Documentez** : Expliquez vos choix pour l'Ã©quipe
4. **Testez** : Validez dans un environnement non-critique
5. **Surveillez** : VÃ©rifiez que vos contraintes ne bloquent pas les dÃ©ploiements
6. **Ajustez** : Ã‰voluez vos stratÃ©gies avec l'expÃ©rience

### HiÃ©rarchie de ComplexitÃ©

```
Node Selector (simple)
    â†“
Node Affinity (flexible)
    â†“
+ Topology Spread (distribution)
    â†“
+ Pod Affinity/Anti-Affinity (relations)
    â†“
+ Taints/Tolerations (restrictions)
    â†“
Architecture ComplÃ¨te (tous ensemble)
```

En maÃ®trisant ces mÃ©canismes et en les combinant judicieusement, vous pouvez orchestrer vos applications Kubernetes de maniÃ¨re optimale, en Ã©quilibrant haute disponibilitÃ©, performance, coÃ»t et isolation selon vos besoins spÃ©cifiques.

---

## Pour Aller Plus Loin

- **ExpÃ©rimentez** : CrÃ©ez votre propre lab MicroK8s multi-node
- **Mesurez** : Utilisez des outils de monitoring pour valider vos choix
- **Partagez** : Documentez vos patterns pour votre Ã©quipe
- **Ã‰voluez** : Kubernetes Ã©volue, restez Ã  jour sur les nouvelles fonctionnalitÃ©s

---

**Fin du chapitre 20 : Ordonnancement AvancÃ©**

Vous avez maintenant une comprÃ©hension complÃ¨te de l'ordonnancement dans Kubernetes, de la thÃ©orie du scheduler aux cas d'usage pratiques complexes. Ces connaissances vous permettront de concevoir et dÃ©ployer des architectures Kubernetes sophistiquÃ©es et adaptÃ©es Ã  vos besoins.

â­ï¸ [Multi-Node et Haute DisponibilitÃ©](/21-multi-node-et-haute-disponibilite/README.md)
