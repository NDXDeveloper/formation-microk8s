üîù Retour au [Sommaire](/SOMMAIRE.md)

# 20.2 Node Selectors

## Introduction

Maintenant que vous comprenez comment fonctionne le scheduler Kubernetes, il est temps de d√©couvrir comment vous pouvez **influencer** ses d√©cisions. Les **Node Selectors** sont le m√©canisme le plus simple et le plus direct pour indiquer au scheduler sur quel type de n≈ìud vous souhaitez que vos Pods soient ex√©cut√©s.

Dans cette section, nous allons apprendre √† utiliser les Node Selectors pour contr√¥ler le placement de vos Pods de mani√®re explicite et pr√©visible.

---

## Qu'est-ce qu'un Node Selector ?

Un **Node Selector** est une contrainte simple que vous pouvez ajouter √† vos Pods pour sp√©cifier qu'ils doivent √™tre ordonnanc√©s **uniquement** sur des n≈ìuds poss√©dant certains labels (√©tiquettes).

### Analogie Simple

Imaginez que vous organisez une f√™te et que vous devez assigner des invit√©s √† des tables :
- Certaines tables sont √©tiquet√©es "VIP"
- D'autres sont √©tiquet√©es "Famille"
- Certaines sont √©tiquet√©es "Amis"

Un Node Selector, c'est comme dire : "Cette personne doit absolument √™tre assise √† une table √©tiquet√©e 'VIP'". Si aucune table VIP n'est disponible, la personne ne pourra pas √™tre plac√©e.

---

## Pourquoi Utiliser des Node Selectors ?

Les Node Selectors sont utiles dans plusieurs situations :

### 1. **N≈ìuds avec du Mat√©riel Sp√©cifique**
Vous avez des n≈ìuds √©quip√©s de GPU et vous voulez que seules les applications de machine learning tournent dessus.

### 2. **S√©paration par Environnement**
Vous souhaitez que les applications de production tournent sur certains n≈ìuds et les applications de d√©veloppement sur d'autres.

### 3. **Optimisation des Co√ªts**
Dans un cloud, vous avez des instances de diff√©rents types (performance √©lev√©e vs standard) et vous voulez optimiser le placement.

### 4. **Conformit√© et S√©curit√©**
Certaines applications sensibles doivent tourner sur des n≈ìuds d√©di√©s et isol√©s.

### 5. **Diff√©rences de Stockage**
Vous avez des n≈ìuds avec des SSD rapides et d'autres avec des disques classiques, et vous voulez placer vos bases de donn√©es sur les SSD.

---

## Labels sur les N≈ìuds

Pour utiliser des Node Selectors, vous devez d'abord comprendre les **labels** des n≈ìuds.

### Qu'est-ce qu'un Label ?

Un label est une paire **cl√©=valeur** attach√©e √† un objet Kubernetes (ici, un n≈ìud). Par exemple :
- `type=gpu`
- `environment=production`
- `disktype=ssd`
- `zone=us-east-1a`

### Labels par D√©faut

Kubernetes ajoute automatiquement certains labels √† vos n≈ìuds lors de leur cr√©ation. Par exemple :
- `kubernetes.io/hostname` : nom d'h√¥te du n≈ìud
- `kubernetes.io/os` : syst√®me d'exploitation (linux, windows)
- `kubernetes.io/arch` : architecture (amd64, arm64, etc.)

### Voir les Labels d'un N≈ìud

Pour afficher tous les labels d'un n≈ìud :

```bash
kubectl get nodes --show-labels
```

R√©sultat exemple :
```
NAME     STATUS   ROLES    AGE   VERSION   LABELS
node1    Ready    <none>   5d    v1.28.0   kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux
```

Pour voir les labels de mani√®re plus lisible :

```bash
kubectl describe node node1
```

Dans la sortie, vous verrez une section **Labels** :
```
Labels:       kubernetes.io/arch=amd64
              kubernetes.io/hostname=node1
              kubernetes.io/os=linux
```

---

## Ajouter des Labels Personnalis√©s aux N≈ìuds

Vous pouvez ajouter vos propres labels aux n≈ìuds pour les cat√©goriser selon vos besoins.

### Syntaxe de Base

```bash
kubectl label nodes <nom-du-noeud> <cl√©>=<valeur>
```

### Exemples Pratiques

#### Exemple 1 : Identifier un N≈ìud avec GPU

```bash
kubectl label nodes node1 hardware=gpu
```

#### Exemple 2 : Marquer un Environnement

```bash
kubectl label nodes node2 environment=production
kubectl label nodes node3 environment=development
```

#### Exemple 3 : Type de Stockage

```bash
kubectl label nodes node1 disktype=ssd
kubectl label nodes node2 disktype=hdd
```

#### Exemple 4 : Zone G√©ographique

```bash
kubectl label nodes node1 zone=europe
kubectl label nodes node2 zone=us
```

### V√©rifier que le Label est Ajout√©

```bash
kubectl get nodes node1 --show-labels
```

Ou :

```bash
kubectl describe node node1 | grep Labels -A 5
```

---

## Utiliser un Node Selector dans un Pod

Une fois que vos n≈ìuds sont labellis√©s, vous pouvez utiliser un Node Selector dans vos Pods pour contr√¥ler leur placement.

### Syntaxe de Base

Dans le fichier YAML de votre Pod, ajoutez la section `nodeSelector` :

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mon-pod
spec:
  nodeSelector:
    cl√©: valeur
  containers:
  - name: mon-conteneur
    image: nginx
```

### Exemple Complet : Pod sur un N≈ìud GPU

Supposons que vous ayez labellis√© un n≈ìud avec `hardware=gpu`. Voici comment cr√©er un Pod qui s'ex√©cutera uniquement sur ce n≈ìud :

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ml-training-pod
  labels:
    app: machine-learning
spec:
  nodeSelector:
    hardware: gpu
  containers:
  - name: tensorflow
    image: tensorflow/tensorflow:latest-gpu
    resources:
      limits:
        nvidia.com/gpu: 1
```

**Ce que fait ce manifeste :**
- Le scheduler cherchera **uniquement** les n≈ìuds ayant le label `hardware=gpu`
- Si aucun n≈ìud avec ce label n'existe ou n'a de ressources disponibles, le Pod restera en √©tat **Pending**

### Exemple Complet : Pod en Production

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: api-production
spec:
  nodeSelector:
    environment: production
  containers:
  - name: api
    image: mon-api:v1.2.3
    resources:
      requests:
        cpu: "500m"
        memory: "1Gi"
```

---

## Node Selectors avec les Deployments

Les Node Selectors fonctionnent √©galement avec les Deployments, ce qui est plus courant en pratique.

### Exemple : Deployment avec Node Selector

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      nodeSelector:
        disktype: ssd
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
```

**R√©sultat :**
- Les 3 r√©plicas du Pod seront ordonnanc√©s uniquement sur les n≈ìuds ayant le label `disktype=ssd`
- Si vous avez plusieurs n≈ìuds avec ce label, les Pods seront r√©partis entre eux par le scheduler

---

## Plusieurs Crit√®res dans un Node Selector

Vous pouvez sp√©cifier plusieurs paires cl√©-valeur dans un Node Selector. Le Pod sera ordonnanc√© **uniquement** sur les n≈ìuds qui ont **tous** les labels sp√©cifi√©s.

### Exemple : Plusieurs Labels

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-exigeant
spec:
  nodeSelector:
    environment: production
    disktype: ssd
    zone: europe
  containers:
  - name: app
    image: mon-app:latest
```

**Comportement :**
- Le Pod n√©cessite un n≈ìud ayant les **trois** labels :
  - `environment=production`
  - `disktype=ssd`
  - `zone=europe`
- Si aucun n≈ìud ne poss√®de ces trois labels simultan√©ment, le Pod reste **Pending**

---

## Que se Passe-t-il si Aucun N≈ìud ne Correspond ?

Si vous d√©finissez un Node Selector mais qu'aucun n≈ìud n'a les labels correspondants, le Pod restera ind√©finiment en √©tat **Pending**.

### Diagnostic

```bash
kubectl get pods
```

R√©sultat :
```
NAME            READY   STATUS    RESTARTS   AGE
mon-pod         0/1     Pending   0          5m
```

Pour comprendre le probl√®me :

```bash
kubectl describe pod mon-pod
```

Dans les √©v√©nements, vous verrez :
```
Events:
  Type     Reason            Message
  ----     ------            -------
  Warning  FailedScheduling  0/3 nodes are available:
                             3 node(s) didn't match Pod's node affinity/selector.
```

### Solutions

1. **V√©rifier les labels des n≈ìuds**
   ```bash
   kubectl get nodes --show-labels
   ```

2. **Ajouter le label manquant**
   ```bash
   kubectl label nodes node1 <cl√©>=<valeur>
   ```

3. **Modifier ou retirer le nodeSelector du Pod**

---

## Modifier ou Supprimer un Label

### Modifier un Label Existant

Pour changer la valeur d'un label :

```bash
kubectl label nodes node1 environment=staging --overwrite
```

L'option `--overwrite` est n√©cessaire si le label existe d√©j√†.

### Supprimer un Label

Pour retirer un label d'un n≈ìud, ajoutez un `-` √† la fin du nom du label :

```bash
kubectl label nodes node1 disktype-
```

---

## Cas d'Usage Pratiques avec MicroK8s

### Cas 1 : Cluster Multi-Node avec R√¥les Diff√©rents

Imaginons que vous ayez un cluster MicroK8s avec 3 n≈ìuds :
- `node1` : n≈ìud puissant pour les applications critiques
- `node2` : n≈ìud standard pour les applications normales
- `node3` : n≈ìud de test pour le d√©veloppement

**Configuration :**

```bash
# Labelliser les n≈ìuds
kubectl label nodes node1 tier=critical
kubectl label nodes node2 tier=standard
kubectl label nodes node3 tier=development

# V√©rification
kubectl get nodes --show-labels
```

**D√©ploiement d'une application critique :**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-critique
spec:
  replicas: 2
  selector:
    matchLabels:
      app: critique
  template:
    metadata:
      labels:
        app: critique
    spec:
      nodeSelector:
        tier: critical
      containers:
      - name: app
        image: mon-app-critique:latest
```

### Cas 2 : Isolation des Bases de Donn√©es

Vous voulez que vos bases de donn√©es tournent sur un n≈ìud d√©di√© avec des disques SSD.

**Configuration :**

```bash
# Labelliser le n≈ìud database
kubectl label nodes node1 workload=database
kubectl label nodes node1 disktype=ssd
```

**D√©ploiement PostgreSQL :**

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      nodeSelector:
        workload: database
        disktype: ssd
      containers:
      - name: postgres
        image: postgres:14
        env:
        - name: POSTGRES_PASSWORD
          value: "motdepasse"
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
```

### Cas 3 : Environnement de Test Isol√©

Vous voulez isoler votre environnement de test sur un n≈ìud sp√©cifique.

**Configuration :**

```bash
kubectl label nodes node3 env=test
```

**D√©ploiement :**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: test
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-test
  namespace: test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      nodeSelector:
        env: test
      containers:
      - name: app
        image: mon-app:dev
```

---

## Limitations des Node Selectors

Bien que les Node Selectors soient simples et efficaces, ils ont certaines limitations :

### 1. **Contrainte Rigide (Hard Constraint)**
Un Node Selector est une contrainte **obligatoire**. Si aucun n≈ìud ne correspond, le Pod ne sera jamais ordonnanc√©. Il n'y a pas de notion de "pr√©f√©rence".

### 2. **Logique Simple (ET uniquement)**
- Vous ne pouvez sp√©cifier que des conditions "ET" (AND)
- Impossible d'exprimer des conditions "OU" (OR) comme "environment=prod OU environment=staging"
- Impossible d'exprimer des n√©gations comme "PAS environment=test"

### 3. **Pas de Relations Entre Pods**
Les Node Selectors ne permettent pas d'exprimer des r√®gles comme :
- "Ce Pod doit √™tre sur le m√™me n≈ìud que cet autre Pod"
- "Ce Pod doit √™tre sur un n≈ìud diff√©rent de cet autre Pod"

### Solution pour des Besoins Plus Complexes

Pour des besoins plus avanc√©s, Kubernetes offre les **Node Affinity** et **Pod Affinity/Anti-Affinity** que nous verrons dans les prochaines sections. Ces m√©canismes sont plus puissants mais aussi plus complexes.

---

## Bonnes Pratiques

### 1. **Nommage Coh√©rent des Labels**

Utilisez des conventions de nommage claires et coh√©rentes :

‚úÖ **Bon :**
```
environment=production
disktype=ssd
hardware=gpu
zone=europe-west1
```

‚ùå **Mauvais :**
```
env=prod
Disk_Type=SSD
HW=gpu
loc=ew1
```

### 2. **Documentation des Labels**

Maintenez une documentation des labels utilis√©s dans votre cluster :

```
# Labels Standard du Cluster
- environment: production, staging, development
- disktype: ssd, hdd, nvme
- hardware: standard, gpu, high-memory
- zone: europe, us, asia
- tier: critical, standard, low-priority
```

### 3. **√âviter la Sur-Contrainte**

N'ajoutez des Node Selectors que lorsque c'est vraiment n√©cessaire. Trop de contraintes peuvent :
- Compliquer l'ordonnancement
- R√©duire la flexibilit√©
- Causer des probl√®mes si un n≈ìud tombe en panne

### 4. **Tester les Node Selectors**

Avant de d√©ployer en production, testez vos Node Selectors :

```bash
# Cr√©er un Pod de test
kubectl run test-pod --image=nginx --dry-run=client -o yaml > test-pod.yaml

# Ajouter le nodeSelector dans test-pod.yaml
# Puis appliquer
kubectl apply -f test-pod.yaml

# V√©rifier le placement
kubectl get pod test-pod -o wide
```

### 5. **Combiner avec des Resource Requests**

Toujours d√©finir des `requests` m√™me avec des Node Selectors :

```yaml
spec:
  nodeSelector:
    disktype: ssd
  containers:
  - name: app
    image: mon-app
    resources:
      requests:
        cpu: "500m"
        memory: "1Gi"
```

### 6. **Pr√©voir les Sc√©narios de Panne**

Que se passe-t-il si votre unique n≈ìud `environment=production` tombe en panne ? Ayez toujours plusieurs n≈ìuds avec les m√™mes labels critiques.

---

## Node Selectors vs Node Affinity

Vous entendrez souvent parler de **Node Affinity** comme alternative aux Node Selectors. Voici une comparaison rapide :

| Aspect | Node Selector | Node Affinity |
|--------|---------------|---------------|
| **Simplicit√©** | Tr√®s simple | Plus complexe |
| **Flexibilit√©** | Limit√©e (AND uniquement) | Grande (OR, NOT, pr√©f√©rences) |
| **Contrainte** | Hard (obligatoire) | Hard ou Soft (pr√©f√©rence) |
| **Syntaxe** | Concise | Plus verbeuse |
| **Cas d'usage** | Besoins simples | Besoins avanc√©s |

**Recommandation :** Commencez par les Node Selectors. Si vous avez besoin de plus de flexibilit√©, explorez les Node Affinity (section 20.3).

---

## V√©rifier l'Impact des Node Selectors

### Voir sur Quel N≈ìud un Pod Tourne

```bash
kubectl get pods -o wide
```

La colonne `NODE` indique sur quel n≈ìud chaque Pod s'ex√©cute.

### Filtrer les Pods par N≈ìud

```bash
kubectl get pods --field-selector spec.nodeName=node1
```

### Voir Tous les Pods d'un N≈ìud

```bash
kubectl get pods --all-namespaces -o wide | grep node1
```

---

## Commandes Utiles - R√©capitulatif

```bash
# Afficher les labels des n≈ìuds
kubectl get nodes --show-labels

# Ajouter un label √† un n≈ìud
kubectl label nodes <node> <key>=<value>

# Modifier un label existant
kubectl label nodes <node> <key>=<value> --overwrite

# Supprimer un label
kubectl label nodes <node> <key>-

# Afficher les d√©tails d'un n≈ìud (incluant labels)
kubectl describe node <node>

# Voir les Pods avec leur n≈ìud
kubectl get pods -o wide

# D√©crire un Pod (voir les events de scheduling)
kubectl describe pod <pod-name>

# Filtrer les n≈ìuds par label
kubectl get nodes -l <key>=<value>
```

---

## Exemple Complet : Mise en Place d'un Environnement Multi-Tiers

Imaginons un sc√©nario complet avec 3 n≈ìuds dans MicroK8s.

### 1. Configuration des N≈ìuds

```bash
# N≈ìud 1 : Frontend (SSD, performances √©lev√©es)
kubectl label nodes node1 tier=frontend
kubectl label nodes node1 disktype=ssd

# N≈ìud 2 : Backend (standard)
kubectl label nodes node2 tier=backend
kubectl label nodes node2 disktype=ssd

# N≈ìud 3 : Database (SSD, haute m√©moire)
kubectl label nodes node3 tier=database
kubectl label nodes node3 disktype=ssd
kubectl label nodes node3 memory=high
```

### 2. D√©ploiement Frontend

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      nodeSelector:
        tier: frontend
      containers:
      - name: nginx
        image: nginx:latest
        resources:
          requests:
            cpu: "200m"
            memory: "256Mi"
```

### 3. D√©ploiement Backend

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
    spec:
      nodeSelector:
        tier: backend
      containers:
      - name: api
        image: mon-api:v1
        resources:
          requests:
            cpu: "500m"
            memory: "512Mi"
```

### 4. D√©ploiement Database

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database
spec:
  serviceName: db
  replicas: 1
  selector:
    matchLabels:
      app: database
  template:
    metadata:
      labels:
        app: database
    spec:
      nodeSelector:
        tier: database
        memory: high
      containers:
      - name: postgres
        image: postgres:14
        resources:
          requests:
            cpu: "1000m"
            memory: "2Gi"
```

---

## Points Cl√©s √† Retenir

1. **Les Node Selectors sont simples** : Une mani√®re directe et facile de contr√¥ler le placement des Pods

2. **Les labels sont la cl√©** : Organisez vos n≈ìuds avec des labels significatifs et coh√©rents

3. **Contrainte obligatoire** : Un Node Selector est une r√®gle stricte - le Pod ne s'ex√©cutera que sur les n≈ìuds correspondants

4. **Plusieurs labels = ET logique** : Tous les labels sp√©cifi√©s doivent √™tre pr√©sents sur le n≈ìud

5. **Diagnostic** : Utilisez `kubectl describe pod` pour comprendre les probl√®mes d'ordonnancement

6. **Commencez simple** : Pour des besoins simples, les Node Selectors suffisent amplement

---

## Conclusion

Les Node Selectors sont votre premier outil pour influencer les d√©cisions du scheduler Kubernetes. Ils offrent un √©quilibre parfait entre simplicit√© et utilit√© pour la plupart des cas d'usage courants.

En ma√Ætrisant les Node Selectors, vous pouvez :
- Optimiser l'utilisation de votre mat√©riel
- Isoler les environnements
- Garantir que les applications critiques tournent sur les bons n≈ìuds
- Am√©liorer les performances en pla√ßant les Pods strat√©giquement

Dans la prochaine section, nous d√©couvrirons les **Node Affinity**, qui offrent encore plus de flexibilit√© et de puissance pour des sc√©narios d'ordonnancement plus complexes.

---

## Pour Aller Plus Loin

- **Documentation officielle** : [Assign Pods to Nodes](https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/)
- **Labels et Selectors** : [Labels and Selectors](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
- **Node Affinity** : Une alternative plus puissante (prochaine section)

---

**Prochain chapitre** : 20.3 Node Affinity et Anti-Affinity - Pour des r√®gles d'ordonnancement plus sophistiqu√©es

‚è≠Ô∏è [Node Affinity et Anti-Affinity](/20-ordonnancement-avance/03-node-affinity-et-anti-affinity.md)
