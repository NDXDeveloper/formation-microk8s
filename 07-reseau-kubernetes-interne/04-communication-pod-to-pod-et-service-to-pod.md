ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 7.4 Communication Pod-to-Pod et Service-to-Pod

## Introduction

Dans les chapitres prÃ©cÃ©dents, nous avons vu les fondations du rÃ©seau Kubernetes : les concepts de base, le CNI (Calico), et le DNS (CoreDNS). Maintenant, il est temps de voir comment tout cela fonctionne ensemble pour permettre aux applications de communiquer.

Dans ce chapitre, nous allons explorer concrÃ¨tement comment les Pods communiquent entre eux, comment les Services facilitent cette communication, et quels sont les diffÃ©rents patterns de communication que vous rencontrerez.

## Les trois modes de communication dans Kubernetes

Il existe trois faÃ§ons principales pour les applications de communiquer dans Kubernetes :

### 1. Communication Pod-to-Pod directe

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pod A   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Pod B   â”‚
â”‚10.1.1.5  â”‚          â”‚10.1.1.8  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Le Pod A connaÃ®t l'IP du Pod B et communique directement avec lui.

**Quand l'utiliser ?**
- Rarement en pratique
- Principalement pour des cas spÃ©ciaux (debugging, tests)
- Dans des StatefulSets oÃ¹ chaque Pod a une identitÃ© stable

**InconvÃ©nient majeur :** Si le Pod B redÃ©marre, il change d'IP, et Pod A ne peut plus le joindre.

### 2. Communication via Service (le mode recommandÃ©)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pod A   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Service â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Pod B   â”‚
â”‚10.1.1.5  â”‚          â”‚  (DNS)  â”‚          â”‚10.1.1.8  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Le Pod A utilise le nom DNS du Service, qui redirige vers un ou plusieurs Pods backend.

**Pourquoi c'est recommandÃ© ?**
- **Abstraction** : Pod A ne connaÃ®t pas l'IP de Pod B
- **RÃ©silience** : Si Pod B change d'IP, le Service s'adapte automatiquement
- **Load balancing** : Si plusieurs Pods backend, le trafic est distribuÃ©
- **DÃ©couplage** : Pod A et Pod B peuvent Ã©voluer indÃ©pendamment

### 3. Communication via Ingress (pour l'accÃ¨s externe)

```
Internet â”€â”€â–¶ Ingress â”€â”€â–¶ Service â”€â”€â–¶ Pods
```

Pour exposer des applications Ã  l'extÃ©rieur du cluster (nous verrons Ã§a dans les chapitres suivants).

## Communication Pod-to-Pod directe

MÃªme si ce n'est pas le mode recommandÃ©, comprenons d'abord comment fonctionne la communication directe entre Pods.

### ScÃ©nario 1 : Deux Pods sur le mÃªme Node

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Node 1                        â”‚
â”‚                 192.168.1.10                     â”‚
â”‚                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚   Pod A      â”‚           â”‚   Pod B      â”‚    â”‚
â”‚   â”‚  10.1.1.5    â”‚           â”‚  10.1.1.8    â”‚    â”‚
â”‚   â”‚              â”‚           â”‚              â”‚    â”‚
â”‚   â”‚  eth0        â”‚           â”‚  eth0        â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚          â”‚                          â”‚            â”‚
â”‚          â”‚                          â”‚            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚ cali123456   â”‚           â”‚ cali789012   â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚          â”‚                          â”‚            â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                     â”‚                            â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚            â”‚  Linux Bridge   â”‚                   â”‚
â”‚            â”‚  (ou routing)   â”‚                   â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ã‰tapes de communication :**

1. **Pod A crÃ©e un paquet** : Source=10.1.1.5, Destination=10.1.1.8
2. **Le paquet sort par eth0** du Pod A vers l'interface cali123456 du Node
3. **Le kernel Linux route** : "10.1.1.8 est local, envoyer via cali789012"
4. **Le paquet entre dans Pod B** via son eth0

**CaractÃ©ristiques :**
- âœ… **TrÃ¨s rapide** : Pas de traversÃ©e rÃ©seau physique
- âœ… **Faible latence** : Communication locale uniquement
- âœ… **Pas de NAT** : L'IP source reste 10.1.1.5

### ScÃ©nario 2 : Deux Pods sur des Nodes diffÃ©rents

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Node 1             â”‚      â”‚       Node 2             â”‚
â”‚    192.168.1.10          â”‚      â”‚    192.168.1.11          â”‚
â”‚                          â”‚      â”‚                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚      â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Pod A      â”‚        â”‚      â”‚        â”‚   Pod B      â”‚  â”‚
â”‚  â”‚  10.1.1.5    â”‚        â”‚      â”‚        â”‚  10.1.2.8    â”‚  â”‚
â”‚  â”‚              â”‚        â”‚      â”‚        â”‚              â”‚  â”‚
â”‚  â”‚  eth0        â”‚        â”‚      â”‚        â”‚  eth0        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚      â”‚        â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                â”‚      â”‚               â”‚          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”        â”‚      â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ cali123456   â”‚        â”‚      â”‚        â”‚ cali456789   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚      â”‚        â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                â”‚      â”‚               â”‚          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”        â”‚      â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Routing    â”‚        â”‚      â”‚        â”‚   Routing    â”‚  â”‚
â”‚  â”‚    Table     â”‚        â”‚      â”‚        â”‚    Table     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚      â”‚        â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                â”‚      â”‚               â”‚          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”        â”‚      â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚     eth0     â”‚        â”‚      â”‚        â”‚     eth0     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚      â”‚        â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                       â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RÃ©seau physique â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    (Switch / Router)
```

**Ã‰tapes de communication :**

1. **Pod A crÃ©e un paquet** : Source=10.1.1.5, Destination=10.1.2.8
2. **Le paquet sort par eth0** du Pod A vers cali123456
3. **Node 1 consulte sa table de routage** :
   ```
   Destination 10.1.2.8 ?
   â†’ Route apprise via BGP : 10.1.2.0/24 via 192.168.1.11
   â†’ Envoyer vers Node 2
   ```
4. **Le paquet traverse le rÃ©seau physique** avec :
   - IP Source : **10.1.1.5** (toujours l'IP du Pod A, pas de NAT !)
   - IP Destination : **10.1.2.8** (IP du Pod B)
5. **Node 2 reÃ§oit le paquet** et consulte sa table de routage :
   ```
   Destination 10.1.2.8 ?
   â†’ Route locale : 10.1.2.8 dev cali456789
   â†’ C'est un de mes Pods !
   ```
6. **Le paquet entre dans Pod B** via cali456789 puis eth0

**CaractÃ©ristiques :**
- âœ… **Pas de NAT** : Pod B voit l'IP rÃ©elle de Pod A (10.1.1.5)
- âœ… **Routage direct** : Pas d'encapsulation (avec Calico en mode direct routing)
- âš ï¸ **Latence rÃ©seau** : TraversÃ©e du rÃ©seau physique (quelques millisecondes)

### VÃ©rifier la communication directe

Vous pouvez tester la communication directe entre Pods :

```bash
# CrÃ©er deux Pods de test
microk8s kubectl run pod-a --image=nginx --restart=Never
microk8s kubectl run pod-b --image=busybox --restart=Never -- sleep 3600

# Obtenir l'IP de pod-a
POD_A_IP=$(microk8s kubectl get pod pod-a -o jsonpath='{.status.podIP}')

# Depuis pod-b, pinguer pod-a
microk8s kubectl exec pod-b -- ping -c 3 $POD_A_IP

# Devrait fonctionner !
```

### Pourquoi ne pas utiliser la communication directe ?

**ProblÃ¨me 1 : IPs Ã©phÃ©mÃ¨res**

```
Situation initiale :
Pod A communique avec Pod B (10.1.1.8)

Pod B plante et redÃ©marre :
Pod B a maintenant l'IP 10.1.1.23

Pod A essaie toujours de contacter 10.1.1.8 â†’ Ã‰CHEC
```

Votre application devrait dÃ©tecter le changement d'IP et se reconfigurer. Complexe et fragile !

**ProblÃ¨me 2 : Pas de load balancing**

Si vous avez 3 Pods backend, comment choisir lequel contacter ? Vous devez implÃ©menter votre propre logique de sÃ©lection.

**ProblÃ¨me 3 : Couplage fort**

Pod A doit connaÃ®tre l'existence et l'IP de Pod B. Si vous changez l'architecture (ajouter des Pods, changer de dÃ©ploiement), Pod A doit Ãªtre modifiÃ©.

**Solution : Utiliser les Services !**

## Communication via Services (le bon pattern)

Les Services rÃ©solvent tous les problÃ¨mes de la communication directe.

### Qu'est-ce qu'un Service ?

Un **Service** est une abstraction qui reprÃ©sente un ensemble de Pods avec :
- **Un nom DNS stable** : `my-service.default.svc.cluster.local`
- **Une IP stable (ClusterIP)** : `10.152.183.25`
- **Un load balancer interne** : Distribue le trafic entre les Pods backend

### Anatomie d'un Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: backend
  namespace: production
spec:
  selector:
    app: backend      # SÃ©lectionne les Pods avec ce label
  ports:
  - port: 80          # Port du Service
    targetPort: 8080  # Port du conteneur dans le Pod
  type: ClusterIP     # Type de Service (par dÃ©faut)
```

**Explication ligne par ligne :**

- **selector** : DÃ©finit quels Pods sont "backend" de ce Service
  - Tous les Pods avec le label `app=backend` sont sÃ©lectionnÃ©s
- **port: 80** : Le Service Ã©coute sur le port 80
- **targetPort: 8080** : Le trafic est redirigÃ© vers le port 8080 des Pods
- **type: ClusterIP** : Service accessible uniquement depuis le cluster

### SÃ©lection des Pods par labels

Les Services utilisent les **labels** pour trouver leurs Pods backend.

**Exemple : 3 Pods avec le label app=backend**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Service "backend"                     â”‚
â”‚         ClusterIP: 10.152.183.25                 â”‚
â”‚         selector: app=backend                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚ SÃ©lectionne tous les Pods
                 â”‚ qui ont le label app=backend
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        â”‚        â”‚
        â–¼        â–¼        â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
    â”‚Pod 1 â”‚ â”‚Pod 2 â”‚ â”‚Pod 3 â”‚
    â”‚app:  â”‚ â”‚app:  â”‚ â”‚app:  â”‚
    â”‚back  â”‚ â”‚back  â”‚ â”‚back  â”‚
    â”‚end   â”‚ â”‚end   â”‚ â”‚end   â”‚
    â”‚10.1  â”‚ â”‚10.1  â”‚ â”‚10.1  â”‚
    â”‚.1.10 â”‚ â”‚.1.11 â”‚ â”‚.1.12 â”‚
    â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
```

**Point important :** Si vous scalez (ajoutez ou supprimez des Pods), le Service s'adapte automatiquement. Pas besoin de modifier quoi que ce soit !

### Communication via Service : Ã©tape par Ã©tape

Suivons une requÃªte depuis un Pod frontend vers un Service backend.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  Ã‰tape 1 : Pod Frontend veut appeler "backend"          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚   Pod Frontend   â”‚                                   â”‚
â”‚  â”‚   10.1.1.5       â”‚                                   â”‚
â”‚  â”‚                  â”‚                                   â”‚
â”‚  â”‚  Code:           â”‚                                   â”‚
â”‚  â”‚  url = "http://backend:80/api"                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚             â”‚                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ Ã‰tape 2 : RÃ©solution DNS
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           CoreDNS                                        â”‚
â”‚                                                          â”‚
â”‚  Query: "backend"                                        â”‚
â”‚  Avec search path â†’ backend.production.svc.cluster.local â”‚
â”‚                                                          â”‚
â”‚  RÃ©ponse : 10.152.183.25 (ClusterIP du Service)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ Ã‰tape 3 : Connexion Ã  l'IP du Service
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pod Frontend envoie le paquet Ã  10.152.183.25:80       â”‚
â”‚                                                         â”‚
â”‚  Paquet : [Source: 10.1.1.5, Dest: 10.152.183.25:80]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ Ã‰tape 4 : Interception par kube-proxy/iptables
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Kube-proxy (iptables rules)                 â”‚
â”‚                                                         â”‚
â”‚  RÃ¨gle : Si destination = 10.152.183.25:80              â”‚
â”‚  Alors : Choisir un Pod backend (round-robin)           â”‚
â”‚                                                         â”‚
â”‚  Pods disponibles :                                     â”‚
â”‚    - 10.1.1.10:8080                                     â”‚
â”‚    - 10.1.1.11:8080                                     â”‚
â”‚    - 10.1.1.12:8080                                     â”‚
â”‚                                                         â”‚
â”‚  SÃ©lection : 10.1.1.11:8080 (Pod 2)                     â”‚
â”‚                                                         â”‚
â”‚  DNAT (Destination NAT) :                               â”‚
â”‚    Destination 10.152.183.25:80 â†’ 10.1.1.11:8080        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ Ã‰tape 5 : Paquet modifiÃ© routÃ© vers Pod backend
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Paquet aprÃ¨s DNAT :                                    â”‚
â”‚  [Source: 10.1.1.5, Dest: 10.1.1.11:8080]               â”‚
â”‚                                                         â”‚
â”‚  Le paquet est routÃ© normalement (Pod-to-Pod)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Pod Backend (Pod 2)                             â”‚
â”‚         10.1.1.11:8080                                  â”‚
â”‚                                                         â”‚
â”‚  Ã‰tape 6 : Pod Backend reÃ§oit le paquet                 â”‚
â”‚  Voit Source: 10.1.1.5 (Pod Frontend)                   â”‚
â”‚                                                         â”‚
â”‚  Ã‰tape 7 : Pod Backend traite la requÃªte                â”‚
â”‚  GÃ©nÃ¨re une rÃ©ponse                                     â”‚
â”‚                                                         â”‚
â”‚  Ã‰tape 8 : RÃ©ponse envoyÃ©e vers 10.1.1.5                â”‚
â”‚  [Source: 10.1.1.11:8080, Dest: 10.1.1.5]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ Ã‰tape 9 : Retour via iptables (reverse NAT)
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Kube-proxy (iptables rules)                 â”‚
â”‚                                                         â”‚
â”‚  Reverse NAT (SNAT) :                                   â”‚
â”‚    Source 10.1.1.11:8080 â†’ 10.152.183.25:80             â”‚
â”‚                                                         â”‚
â”‚  Le Frontend pense que la rÃ©ponse vient du Service      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Pod Frontend                                    â”‚
â”‚         10.1.1.5                                        â”‚
â”‚                                                         â”‚
â”‚  Ã‰tape 10 : ReÃ§oit la rÃ©ponse                           â”‚
â”‚  [Source: 10.152.183.25:80, Dest: 10.1.1.5]             â”‚
â”‚                                                         â”‚
â”‚  Frontend ne sait pas qu'il y avait plusieurs Pods !    â”‚
â”‚  Pour lui, il a juste parlÃ© avec "backend"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Points clÃ©s Ã  retenir :**

1. **DNS** : Le nom "backend" est rÃ©solu en IP du Service (10.152.183.25)
2. **ClusterIP virtuelle** : 10.152.183.25 n'existe pas vraiment sur un interface rÃ©seau
3. **Kube-proxy** : Intercepte le trafic via iptables et fait du DNAT
4. **Load balancing** : Choisit automatiquement un Pod backend
5. **Transparence** : Le Frontend ne sait pas quel Pod a traitÃ© sa requÃªte

### Le rÃ´le de kube-proxy

**Kube-proxy** est un composant Kubernetes qui s'exÃ©cute sur chaque Node. Il est responsable de l'implÃ©mentation des Services.

**Ses responsabilitÃ©s :**

1. **Surveiller** l'API Kubernetes pour dÃ©tecter les changements de Services/Endpoints
2. **Configurer iptables** (ou IPVS) pour rediriger le trafic vers les Pods
3. **GÃ©rer le load balancing** entre les Pods backend

**Architecture :**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Node                            â”‚
â”‚                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Kube-proxy                         â”‚  â”‚
â”‚  â”‚                                              â”‚  â”‚
â”‚  â”‚  1. Watch API Server                         â”‚  â”‚
â”‚  â”‚     â†“                                        â”‚  â”‚
â”‚  â”‚  2. Service/Endpoints changent ?             â”‚  â”‚
â”‚  â”‚     â†“                                        â”‚  â”‚
â”‚  â”‚  3. Mettre Ã  jour les rÃ¨gles iptables        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                              â”‚
â”‚                     â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           iptables / IPVS                    â”‚  â”‚
â”‚  â”‚                                              â”‚  â”‚
â”‚  â”‚  RÃ¨gles pour tous les Services :             â”‚  â”‚
â”‚  â”‚  - Service A â†’ Pods A1, A2, A3               â”‚  â”‚
â”‚  â”‚  - Service B â†’ Pods B1, B2                   â”‚  â”‚
â”‚  â”‚  - ...                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                    â”‚
â”‚  Tous les paquets rÃ©seau passent par iptables      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Les modes de kube-proxy

Kube-proxy peut fonctionner en plusieurs modes :

#### 1. iptables (mode par dÃ©faut dans MicroK8s)

**Avantages :**
- Mature et stable
- Fonctionne partout

**InconvÃ©nients :**
- Performance dÃ©gradÃ©e avec beaucoup de Services (>1000)
- Load balancing basique (round-robin alÃ©atoire)

#### 2. IPVS (mode avancÃ©)

**Avantages :**
- TrÃ¨s performant mÃªme avec des milliers de Services
- Algorithmes de load balancing avancÃ©s (round-robin, least connection, etc.)
- Utilise moins de ressources CPU

**InconvÃ©nients :**
- NÃ©cessite le module kernel IPVS
- LÃ©gÃ¨rement plus complexe Ã  debugger

### Endpoints : le lien entre Services et Pods

Quand vous crÃ©ez un Service, Kubernetes crÃ©e automatiquement un objet **Endpoints** qui liste les IPs des Pods backend.

```bash
# Voir les Endpoints d'un Service
microk8s kubectl get endpoints backend

# Exemple de sortie :
NAME      ENDPOINTS                                    AGE
backend   10.1.1.10:8080,10.1.1.11:8080,10.1.1.12:8080   5m
```

**Fonctionnement :**

```
Service "backend" (selector: app=backend)
    â†“
Kubernetes trouve tous les Pods avec app=backend
    â†“
CrÃ©e/Met Ã  jour l'objet Endpoints avec leurs IPs
    â†“
Kube-proxy lit les Endpoints
    â†“
Configure iptables pour rediriger vers ces IPs
```

**Important :** Si un Pod tombe ou dÃ©marre, l'objet Endpoints est automatiquement mis Ã  jour, et kube-proxy reconfigure iptables. Tout est automatique !

## Patterns de communication courants

Voyons maintenant des scÃ©narios rÃ©els de communication entre applications.

### Pattern 1 : Architecture 3-tiers classique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Frontend â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚    API   â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚   DB   â”‚  â”‚
â”‚  â”‚  (nginx) â”‚        â”‚  (node)  â”‚       â”‚(postgres  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                     â”‚
â”‚  Service:            Service:          Service:     â”‚
â”‚  "frontend"          "api"             "database"   â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Communication :**

1. **Frontend â†’ API** :
   ```javascript
   // Dans le frontend
   fetch('http://api:8080/users')
   ```

2. **API â†’ Database** :
   ```javascript
   // Dans l'API
   const db = new Client({
     host: 'database',
     port: 5432,
     database: 'myapp'
   });
   ```

**Avantages :**
- Chaque tier peut scaler indÃ©pendamment
- Abstraction totale des IPs
- RÃ©silience aux redÃ©marrages

### Pattern 2 : Microservices

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                  â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚     â”Œâ”€â”€â”€â”‚  Gateway â”‚â”€â”€â”€â”                         â”‚
â”‚     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                         â”‚
â”‚     â”‚                  â”‚                         â”‚
â”‚     â–¼                  â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚ Users  â”‚      â”‚ Orders   â”‚                    â”‚
â”‚  â”‚Service â”‚      â”‚ Service  â”‚                    â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚      â”‚                â”‚                          â”‚
â”‚      â”‚                â–¼                          â”‚
â”‚      â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚Payments  â”‚                      â”‚
â”‚                â”‚Service   â”‚                      â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                  â”‚
â”‚  Chaque boÃ®te est un Service avec ses Pods       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Communication :**

```javascript
// Gateway appelle Users service
const response = await fetch('http://users-service:8080/user/123');

// Orders service appelle Payments service
const payment = await fetch('http://payments-service:8080/charge', {
  method: 'POST',
  body: JSON.stringify(orderData)
});
```

**Avantages :**
- Chaque service est indÃ©pendant
- Peut utiliser des technologies diffÃ©rentes
- ScalabilitÃ© fine par service

### Pattern 3 : Workers et Queue

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   API    â”‚â”€â”€â”€â”€â”€â–¶â”‚  Redis   â”‚â—€â”€â”€â”€â”€â”€â”‚  Worker  â”‚   â”‚
â”‚  â”‚          â”‚ PUT  â”‚  (Queue) â”‚ GET  â”‚          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                         (x3 Pods)   â”‚
â”‚  Service:          Service:            Service:     â”‚
â”‚  "api"             "redis"             "worker"     â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Communication :**

```python
# API envoie un job dans la queue
redis_client = Redis(host='redis', port=6379)
redis_client.lpush('jobs', json.dumps(job_data))

# Workers rÃ©cupÃ¨rent les jobs
while True:
    job = redis_client.brpop('jobs')
    process_job(job)
```

**Avantages :**
- Traitement asynchrone
- RÃ©silience : si un worker tombe, un autre prend le relais
- ScalabilitÃ© : ajoutez des workers selon la charge

### Pattern 4 : Communication cross-namespace

Parfois, des applications dans diffÃ©rents namespaces doivent communiquer.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Namespace: production                   â”‚
â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚  â”‚   API    â”‚â”€â”€â”€â”€â”€â”                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                            â”‚
â”‚                   â”‚ Appel cross-namespace      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Namespace: shared                       â”‚
â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚  â”‚  Monitoring  â”‚                              â”‚
â”‚  â”‚   Service    â”‚                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Communication :**

```python
# Depuis namespace "production", appeler service dans "shared"
monitoring_url = 'http://monitoring.shared.svc.cluster.local:9090/metrics'
```

**Format DNS cross-namespace :**
```
<service>.<namespace>.svc.cluster.local
```

**Important :** Par dÃ©faut, les Pods peuvent communiquer entre namespaces. Si vous voulez restreindre cela, utilisez des **Network Policies** (chapitre sÃ©curitÃ©).

## Types de Services

Nous avons vu le type ClusterIP jusqu'ici, mais il existe d'autres types de Services.

### 1. ClusterIP (par dÃ©faut)

```yaml
type: ClusterIP
```

**CaractÃ©ristiques :**
- IP interne au cluster uniquement
- Accessible depuis tous les Pods
- Non accessible depuis l'extÃ©rieur

**Cas d'usage :**
- Communication entre microservices
- Bases de donnÃ©es internes
- APIs internes

### 2. NodePort

```yaml
type: NodePort
ports:
- port: 80
  targetPort: 8080
  nodePort: 30080  # Port sur chaque Node (30000-32767)
```

**CaractÃ©ristiques :**
- Ouvre un port sur **tous les Nodes** du cluster
- Accessible via `<IP-du-Node>:30080`
- CrÃ©e automatiquement un ClusterIP aussi

**SchÃ©ma :**

```
Internet
   â”‚
   â–¼
Node IP:30080 â”€â”€â–¶ Service (ClusterIP) â”€â”€â–¶ Pods
```

**Cas d'usage :**
- Exposition simple pour dÃ©veloppement/test
- AccÃ¨s externe basique

**InconvÃ©nients :**
- Ports dans une plage limitÃ©e (30000-32767)
- Pas de nom de domaine automatique
- Pas de SSL/TLS automatique

### 3. LoadBalancer

```yaml
type: LoadBalancer
ports:
- port: 80
  targetPort: 8080
```

**CaractÃ©ristiques :**
- CrÃ©e un load balancer externe (nÃ©cessite un cloud provider ou MetalLB)
- Obtient une IP externe
- CrÃ©e automatiquement NodePort et ClusterIP

**SchÃ©ma :**

```
Internet
   â”‚
   â–¼
Load Balancer (IP externe) â”€â”€â–¶ NodePort â”€â”€â–¶ Service (ClusterIP) â”€â”€â–¶ Pods
```

**Cas d'usage :**
- Exposition production
- Applications publiques

**Note :** Dans MicroK8s, vous devez activer MetalLB pour que ce type fonctionne (voir chapitre MetalLB).

### 4. ExternalName

```yaml
type: ExternalName
externalName: api.example.com
```

**CaractÃ©ristiques :**
- CrÃ©e un alias DNS vers un nom externe
- Pas de ClusterIP
- Pas de proxying

**Cas d'usage :**
- RÃ©fÃ©rencer des services externes (ex: base de donnÃ©es managÃ©e en dehors du cluster)
- Migration progressive vers Kubernetes

**Exemple :**

```python
# Votre code appelle "database"
db_host = 'database'

# Mais "database" est un ExternalName qui pointe vers "prod-db.rds.amazonaws.com"
# Transparent pour l'application !
```

## Load Balancing : distribution du trafic

Quand un Service a plusieurs Pods backend, comment le trafic est-il distribuÃ© ?

### Algorithme : Random Selection (sÃ©lection alÃ©atoire)

Dans le mode iptables (par dÃ©faut), kube-proxy utilise une **sÃ©lection alÃ©atoire** avec probabilitÃ©s Ã©gales.

**Exemple avec 3 Pods :**

```
Service "api" avec 3 Pods backend :
- Pod 1 : 10.1.1.10
- Pod 2 : 10.1.1.11
- Pod 3 : 10.1.1.12

RÃ¨gles iptables gÃ©nÃ©rÃ©es :
33% de chance â†’ Pod 1
33% de chance â†’ Pod 2
33% de chance â†’ Pod 3
```

**ConsÃ©quence :** Sur une grande quantitÃ© de requÃªtes, la distribution est approximativement Ã©gale.

### Session Affinity (affinitÃ© de session)

Par dÃ©faut, chaque requÃªte peut aller vers un Pod diffÃ©rent. Parfois, vous voulez que toutes les requÃªtes d'un mÃªme client aillent au mÃªme Pod.

**Configuration :**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: api
spec:
  sessionAffinity: ClientIP  # â† Active l'affinitÃ© par IP client
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800  # 3 heures
  selector:
    app: api
  ports:
  - port: 80
    targetPort: 8080
```

**Comportement :**

```
Client avec IP 192.168.1.100 envoie une requÃªte
    â†“
Kube-proxy : "PremiÃ¨re requÃªte de cette IP, choisir Pod 2"
    â†“
Toutes les requÃªtes suivantes de 192.168.1.100 vont vers Pod 2
(pendant 3 heures)
```

**Cas d'usage :**
- Applications avec sessions cÃ´tÃ© serveur
- WebSockets
- Connexions longues

**Attention :** L'affinitÃ© est basÃ©e sur l'IP **source**. Si plusieurs clients partagent la mÃªme IP (NAT), ils iront tous au mÃªme Pod (dÃ©sÃ©quilibre possible).

## Ports : mapping et traduction

Comprendre comment les ports sont mappÃ©s est crucial.

### Port du Service vs TargetPort

```yaml
ports:
- port: 80          # Port du Service
  targetPort: 8080  # Port du conteneur
```

**Traduction :**

```
Client appelle â†’ Service:80
                    â†“
             RedirigÃ© vers Pod:8080
```

**Avantage :** Vous pouvez standardiser le port du Service (ex: toujours 80 pour HTTP) mÃªme si vos applications Ã©coutent sur des ports diffÃ©rents.

**Exemple multi-applications :**

```yaml
# Service frontend
ports:
- port: 80
  targetPort: 3000  # React dev server

# Service API
ports:
- port: 80
  targetPort: 8080  # Spring Boot

# Service admin
ports:
- port: 80
  targetPort: 5000  # Flask
```

Tous exposent le port 80, mais les applications backend utilisent leurs propres ports.

### Named Ports

Vous pouvez donner des noms aux ports dans les Pods :

```yaml
# DÃ©finition du Pod
spec:
  containers:
  - name: app
    ports:
    - name: http
      containerPort: 8080
    - name: metrics
      containerPort: 9090
```

Puis rÃ©fÃ©rencer ces noms dans le Service :

```yaml
# Service
spec:
  ports:
  - name: web
    port: 80
    targetPort: http  # â† RÃ©fÃ©rence au port nommÃ©
  - name: monitoring
    port: 9090
    targetPort: metrics
```

**Avantages :**
- Plus lisible
- Plus flexible (changer le port dans le Pod ne nÃ©cessite pas de changer le Service)

### Multiple Ports

Un Service peut exposer plusieurs ports :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: fullstack-app
spec:
  selector:
    app: fullstack
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: https
    port: 443
    targetPort: 8443
  - name: metrics
    port: 9090
    targetPort: 9090
```

**Usage :**

```python
# Connexion HTTP
http_url = 'http://fullstack-app:80'

# Connexion HTTPS
https_url = 'https://fullstack-app:443'

# MÃ©triques Prometheus
metrics_url = 'http://fullstack-app:9090/metrics'
```

## Protocoles supportÃ©s

Les Services Kubernetes supportent plusieurs protocoles.

### TCP (par dÃ©faut)

```yaml
ports:
- port: 80
  protocol: TCP  # Peut Ãªtre omis (dÃ©faut)
```

**Usage :** HTTP, HTTPS, bases de donnÃ©es, APIs REST, etc.

### UDP

```yaml
ports:
- port: 53
  protocol: UDP
```

**Usage :** DNS, protocoles de streaming, jeux vidÃ©o

### SCTP (Stream Control Transmission Protocol)

```yaml
ports:
- port: 9090
  protocol: SCTP
```

**Usage :** TÃ©lÃ©communications, protocoles industriels (rarement utilisÃ©)

## Readiness et Liveness : santÃ© des Pods

Les Services ne redirigent le trafic que vers des Pods **sains** (ready).

### Readiness Probe

Une **Readiness Probe** indique si un Pod est prÃªt Ã  recevoir du trafic.

```yaml
spec:
  containers:
  - name: app
    image: myapp:latest
    readinessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
```

**Fonctionnement :**

```
Pod dÃ©marre
    â†“
Kubernetes attend 5 secondes (initialDelaySeconds)
    â†“
Envoie une requÃªte GET http://pod-ip:8080/health
    â†“
Si rÃ©ponse 200 OK â†’ Pod marquÃ© "Ready"
    â†“
Kube-proxy ajoute le Pod aux Endpoints du Service
    â†“
Le Pod commence Ã  recevoir du trafic
```

**Si la probe Ã©choue :**

```
Pod rÃ©pond 500 Internal Server Error
    â†“
Kubernetes marque le Pod comme "Not Ready"
    â†“
Kube-proxy retire le Pod des Endpoints
    â†“
Plus aucun trafic n'est envoyÃ© vers ce Pod
```

**Importance :** Sans Readiness Probe, un Pod pourrait recevoir du trafic avant d'Ãªtre prÃªt (ex: avant que la base de donnÃ©es soit connectÃ©e), causant des erreurs 500.

### Liveness Probe

Une **Liveness Probe** indique si un Pod est vivant ou doit Ãªtre redÃ©marrÃ©.

```yaml
spec:
  containers:
  - name: app
    image: myapp:latest
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
```

**DiffÃ©rence avec Readiness :**

- **Readiness** : "Es-tu prÃªt Ã  recevoir du trafic ?"
  - Non â†’ Retire des Endpoints, mais ne redÃ©marre pas
- **Liveness** : "Es-tu vivant ?"
  - Non â†’ RedÃ©marre le conteneur

**Cas d'usage Liveness :**
- Deadlocks (application bloquÃ©e)
- Fuites mÃ©moire (application trop lente)
- Corruption interne

**Bonnes pratiques :**
- Utilisez **toujours** une Readiness Probe
- Utilisez une Liveness Probe si votre app peut se bloquer
- Ne faites pas de checks trop lourds (risque de faux positifs)

## DÃ©couverte de services : patterns avancÃ©s

### Service Discovery manuel

Si vous avez vraiment besoin de connaÃ®tre tous les Pods d'un Service :

```python
import socket

# RÃ©soudre le nom DNS du Service
service_name = "backend.production.svc.cluster.local"
try:
    ips = socket.getaddrinfo(service_name, 8080, socket.AF_INET)
    pod_ips = [ip[4][0] for ip in ips]
    print(f"Pods backend : {pod_ips}")
except socket.gaierror:
    print("Service not found")
```

**Note :** Cela ne fonctionne qu'avec des **Headless Services** (clusterIP: None).

### Variables d'environnement (legacy)

Kubernetes injecte automatiquement des variables d'environnement pour chaque Service dans les Pods.

**Exemple :**

Si un Service "database" existe, les Pods verront :

```bash
DATABASE_SERVICE_HOST=10.152.183.25
DATABASE_SERVICE_PORT=5432
```

**Usage :**

```python
import os

db_host = os.environ.get('DATABASE_SERVICE_HOST')
db_port = os.environ.get('DATABASE_SERVICE_PORT')
```

**InconvÃ©nient :** Les variables ne sont injectÃ©es qu'au dÃ©marrage du Pod. Si vous crÃ©ez un Service aprÃ¨s un Pod, ce Pod ne verra pas les variables.

**Recommandation :** Utilisez plutÃ´t le DNS (plus fiable et dynamique).

## Performance et optimisation

### LocalitÃ© du trafic

Par dÃ©faut, un Service peut envoyer le trafic vers n'importe quel Pod, mÃªme sur un autre Node.

**Avec 2 Nodes :**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Node 1      â”‚      â”‚     Node 2      â”‚
â”‚                 â”‚      â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚Frontend  â”‚â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â–¶â”‚ Backend  â”‚   â”‚
â”‚  â”‚          â”‚   â”‚      â”‚  â”‚  Pod     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚      â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚      â”‚                 â”‚
â”‚  â”‚ Backend  â”‚â—„â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€Trafic peut    â”‚
â”‚  â”‚  Pod     â”‚   â”‚      â”‚  venir de Node2 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚      â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Impact :** Latence rÃ©seau supplÃ©mentaire quand le trafic traverse les Nodes.

**Solution : topologyKeys (dÃ©prÃ©ciÃ©) ou Service Internal Traffic Policy**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: backend
spec:
  internalTrafficPolicy: Local  # â† Trafic local uniquement
  selector:
    app: backend
  ports:
  - port: 80
```

**Comportement :**

Avec `internalTrafficPolicy: Local`, le trafic est envoyÃ© uniquement aux Pods **sur le mÃªme Node** que le client.

**Avantages :**
- Latence rÃ©duite
- Pas de trafic rÃ©seau inter-Node

**InconvÃ©nients :**
- DÃ©sÃ©quilibre possible si les Pods ne sont pas uniformÃ©ment distribuÃ©s
- Si aucun Pod local, la requÃªte Ã©choue

## Debugging de la communication

Quand la communication ne fonctionne pas, voici comment diagnostiquer.

### ProblÃ¨me 1 : "Connection refused" ou "Connection timeout"

**Diagnostic :**

**Ã‰tape 1 : VÃ©rifier que le Service existe**

```bash
microk8s kubectl get svc backend

# Devrait afficher le Service avec une ClusterIP
```

**Ã‰tape 2 : VÃ©rifier les Endpoints**

```bash
microk8s kubectl get endpoints backend

# Devrait lister les IPs des Pods
# Si vide â†’ Aucun Pod ne correspond au selector
```

**Ã‰tape 3 : VÃ©rifier les Pods**

```bash
microk8s kubectl get pods -l app=backend

# VÃ©rifier que des Pods existent et sont en "Running" et "Ready"
```

**Ã‰tape 4 : Tester depuis un Pod**

```bash
# CrÃ©er un Pod de test
microk8s kubectl run test --image=busybox --restart=Never -- sleep 3600

# Tester la rÃ©solution DNS
microk8s kubectl exec test -- nslookup backend

# Tester la connexion
microk8s kubectl exec test -- wget -O- http://backend:80

# Devrait retourner une rÃ©ponse
```

### ProblÃ¨me 2 : DNS ne rÃ©sout pas

**Voir le chapitre 7.3 sur CoreDNS pour le debugging DNS complet.**

**Quick check :**

```bash
# Depuis un Pod, vÃ©rifier /etc/resolv.conf
microk8s kubectl exec test -- cat /etc/resolv.conf

# Devrait contenir : nameserver 10.152.183.10
```

### ProblÃ¨me 3 : Le Service rÃ©sout, mais connexion Ã©choue

**Cause probable : Port incorrect**

```yaml
# Service expose le port 80
ports:
- port: 80
  targetPort: 8080

# Mais votre application Ã©coute sur 3000, pas 8080 !
```

**Solution :** Corriger le `targetPort` pour qu'il corresponde au port de l'application.

**VÃ©rifier le port d'Ã©coute du Pod :**

```bash
# Voir les logs du Pod
microk8s kubectl logs <pod-name>

# Chercher une ligne comme : "Listening on port 3000"
```

### ProblÃ¨me 4 : Certaines requÃªtes Ã©chouent, d'autres rÃ©ussissent

**Cause probable : Un Pod backend est dÃ©faillant**

```bash
# VÃ©rifier l'Ã©tat de tous les Pods
microk8s kubectl get pods -l app=backend

# VÃ©rifier les logs du Pod dÃ©faillant
microk8s kubectl logs <pod-name>

# VÃ©rifier les events
microk8s kubectl describe pod <pod-name>
```

**Solution :** Ajouter ou amÃ©liorer les **Readiness Probes** pour que les Pods dÃ©faillants soient automatiquement retirÃ©s.

## Bonnes pratiques

### 1. Toujours utiliser des Services (pas d'IPs en dur)

âŒ **Mauvais :**
```python
api_url = "http://10.1.1.5:8080"
```

âœ… **Bon :**
```python
api_url = "http://api:8080"
```

### 2. Utiliser des noms de Services courts dans le mÃªme namespace

âœ… **Bon :**
```python
# Depuis namespace "production"
db_host = "database"  # RÃ©sout vers database.production.svc.cluster.local
```

### 3. Utiliser des FQDN cross-namespace

âœ… **Bon :**
```python
# Depuis namespace "app" vers namespace "monitoring"
metrics_url = "http://prometheus.monitoring.svc.cluster.local:9090"
```

### 4. DÃ©finir des Readiness Probes

```yaml
readinessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 10
```

**Toujours dÃ©finir une Readiness Probe** pour Ã©viter que des Pods non prÃªts reÃ§oivent du trafic.

### 5. Utiliser des Named Ports

```yaml
# Dans le Pod
ports:
- name: http
  containerPort: 8080

# Dans le Service
ports:
- port: 80
  targetPort: http
```

Plus maintenable et lisible.

### 6. Configurer des limites de ressources

```yaml
resources:
  requests:
    memory: "128Mi"
    cpu: "100m"
  limits:
    memory: "256Mi"
    cpu: "200m"
```

EmpÃªche un Pod dÃ©faillant de monopoliser les ressources du Node.

### 7. Utiliser des labels significatifs

```yaml
labels:
  app: backend
  version: v2
  tier: api
  environment: production
```

Facilite la sÃ©lection et l'organisation.

### 8. Tester la rÃ©silience

Simulez des pannes pour vÃ©rifier que votre application tolÃ¨re les redÃ©marrages :

```bash
# Tuer un Pod
microk8s kubectl delete pod <pod-name>

# VÃ©rifier que le Service continue de fonctionner
# (les autres Pods prennent le relais)
```

## Points clÃ©s Ã  retenir

### âœ… Deux modes de communication

1. **Pod-to-Pod direct** : Rarement utilisÃ©, IPs Ã©phÃ©mÃ¨res
2. **Via Services** : RecommandÃ©, abstraction stable

### âœ… Services = Load Balancers internes

- DNS stable
- ClusterIP stable
- Distribution automatique du trafic

### âœ… Kube-proxy implÃ©mente les Services

- Via iptables (ou IPVS)
- DNAT (Destination NAT)
- Load balancing automatique

### âœ… Endpoints = Liste des Pods backend

- Mis Ã  jour automatiquement
- UtilisÃ© par kube-proxy

### âœ… Types de Services

- **ClusterIP** : Interne uniquement (par dÃ©faut)
- **NodePort** : Accessible via Node IP
- **LoadBalancer** : Avec IP externe (nÃ©cessite MetalLB ou cloud)
- **ExternalName** : Alias vers nom externe

### âœ… Readiness Probes

Cruciales pour garantir que seuls les Pods sains reÃ§oivent du trafic

### âœ… DNS pour la dÃ©couverte

- Nom court : mÃªme namespace
- FQDN : `service.namespace.svc.cluster.local`

## Pourquoi tout cela est important ?

La communication est le cÅ“ur de toute architecture distribuÃ©e. Comprendre comment les Pods et Services communiquent vous permet de :

1. **Concevoir des architectures robustes** : Services dÃ©couplÃ©s et rÃ©silients
2. **DÃ©boguer efficacement** : Comprendre oÃ¹ chercher quand Ã§a ne fonctionne pas
3. **Optimiser les performances** : RÃ©duire la latence, amÃ©liorer le dÃ©bit
4. **SÃ©curiser** : Comprendre les flux pour appliquer les bonnes Network Policies

## Prochaines Ã©tapes

Maintenant que vous maÃ®trisez la communication interne, le prochain chapitre explore :

- **7.5 Test de connectivitÃ© interne** : Outils et commandes pratiques pour diagnostiquer et valider le rÃ©seau

Avec ces connaissances sur la communication Pod-to-Pod et Service-to-Pod, vous avez les fondations pour construire des applications distribuÃ©es robustes dans Kubernetes !

---

**Note :** Ce chapitre se concentre sur la communication interne au cluster. L'exposition externe (Ingress, LoadBalancer externe) sera abordÃ©e dans les chapitres suivants.

â­ï¸ [Test de connectivitÃ© interne](/07-reseau-kubernetes-interne/05-test-de-connectivite-interne.md)
