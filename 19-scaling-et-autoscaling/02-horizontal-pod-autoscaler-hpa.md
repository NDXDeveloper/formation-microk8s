üîù Retour au [Sommaire](/SOMMAIRE.md)

# 19.2 Horizontal Pod Autoscaler (HPA)

## Introduction au HPA

Le **Horizontal Pod Autoscaler (HPA)** est un contr√¥leur Kubernetes qui ajuste automatiquement le nombre de r√©pliques de vos pods en fonction de la charge observ√©e. Contrairement au scaling manuel o√π vous devez intervenir, le HPA surveille vos applications et scale automatiquement.

### Analogie simple

Imaginez un restaurant :
- **Scaling manuel** : Le g√©rant d√©cide √† l'avance combien de serveurs seront pr√©sents (5 le midi, 2 le soir)
- **HPA (autoscaling)** : Le g√©rant observe en temps r√©el le nombre de clients et appelle des serveurs suppl√©mentaires quand c'est n√©cessaire, puis les renvoie quand √ßa se calme

Le HPA fait exactement la m√™me chose avec vos pods !

### Pourquoi utiliser le HPA ?

**Avantages principaux :**

1. **R√©activit√© 24/7** : Le HPA surveille votre application en permanence, m√™me la nuit
2. **Adaptation automatique** : Plus besoin d'intervention humaine pour ajuster les ressources
3. **Optimisation des co√ªts** : Scale down automatiquement quand la charge diminue
4. **Am√©lioration des performances** : Scale up rapidement lors des pics de trafic
5. **Tranquillit√© d'esprit** : Vous pouvez dormir tranquille, le HPA g√®re les impr√©vus

## Concept "Horizontal" vs "Vertical"

Avant d'aller plus loin, comprenons bien la diff√©rence :

### Horizontal Scaling (HPA)
- **Ajoute ou retire des pods** (plus de copies de l'application)
- Exemple : 1 serveur web ‚Üí 5 serveurs web
- C'est le sujet de cette section

### Vertical Scaling (VPA)
- **Augmente ou diminue les ressources** d'un pod existant (plus de CPU/RAM)
- Exemple : Un pod avec 1 CPU ‚Üí le m√™me pod avec 4 CPU
- Nous verrons cela dans la section 19.3

**M√©taphore :**
- **Horizontal** : Embaucher plus de serveurs au restaurant
- **Vertical** : Rendre chaque serveur plus rapide et efficace

## Comment fonctionne le HPA ?

Le HPA suit un cycle de d√©cision simple :

```
1. Observer les m√©triques (CPU, m√©moire, requ√™tes/sec, etc.)
   ‚Üì
2. Comparer avec les seuils d√©finis
   ‚Üì
3. Calculer le nombre de r√©pliques id√©al
   ‚Üì
4. Scaler le Deployment (si n√©cessaire)
   ‚Üì
5. Attendre un peu (cooldown period)
   ‚Üì
6. Retour √† l'√©tape 1
```

### Cycle de v√©rification

Par d√©faut, le HPA :
- V√©rifie les m√©triques **toutes les 15 secondes**
- Prend une d√©cision de scaling **toutes les 3 minutes** pour scale up
- Attend **5 minutes** avant de scale down (pour √©viter les fluctuations)

Ces d√©lais peuvent √™tre ajust√©s selon vos besoins.

## Pr√©requis : Metrics Server

Le HPA a besoin de m√©triques pour fonctionner. Sur MicroK8s, il faut activer le **Metrics Server** :

```bash
microk8s enable metrics-server
```

Le Metrics Server collecte les m√©triques de consommation CPU et m√©moire de tous les pods du cluster.

### V√©rifier que Metrics Server fonctionne

Apr√®s quelques minutes, testez :

```bash
kubectl top nodes
```

Sortie attendue :
```
NAME          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
microk8s-vm   450m         22%    2048Mi          51%
```

Testez aussi pour les pods :

```bash
kubectl top pods
```

Si vous obtenez des r√©sultats, c'est que Metrics Server fonctionne correctement !

## Configuration de base du HPA

### M√©thode 1 : Commande imp√©rative (rapide)

La fa√ßon la plus simple de cr√©er un HPA :

```bash
kubectl autoscale deployment nginx-web --cpu-percent=50 --min=1 --max=10
```

Cette commande signifie :
- **nginx-web** : Le deployment √† scaler
- **--cpu-percent=50** : Maintenir l'utilisation CPU autour de 50%
- **--min=1** : Minimum 1 r√©plique (jamais en dessous)
- **--max=10** : Maximum 10 r√©pliques (jamais au-dessus)

### M√©thode 2 : Fichier YAML (recommand√©)

Pour une configuration plus p√©renne et versionnable, cr√©ez un fichier `hpa-nginx.yaml` :

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-web-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-web
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
```

Appliquez la configuration :

```bash
kubectl apply -f hpa-nginx.yaml
```

### D√©composition du fichier YAML

Analysons chaque partie :

**scaleTargetRef** : Quel Deployment scaler ?
```yaml
scaleTargetRef:
  apiVersion: apps/v1
  kind: Deployment
  name: nginx-web    # Le nom de votre deployment
```

**minReplicas** : Nombre minimum de pods
```yaml
minReplicas: 1    # Ne jamais descendre en dessous de 1
```

**maxReplicas** : Nombre maximum de pods
```yaml
maxReplicas: 10    # Ne jamais d√©passer 10 pods
```

**metrics** : Sur quoi baser le scaling ?
```yaml
metrics:
- type: Resource
  resource:
    name: cpu    # Surveiller le CPU
    target:
      type: Utilization
      averageUtilization: 50    # Cible : 50% d'utilisation
```

## V√©rifier l'√©tat du HPA

### Lister les HPA actifs

```bash
kubectl get hpa
```

Sortie :
```
NAME             REFERENCE              TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nginx-web-hpa    Deployment/nginx-web   15%/50%   1         10        1          5m
```

Lecture de cette sortie :
- **TARGETS** : `15%/50%` = utilisation actuelle de 15% / cible de 50%
- **REPLICAS** : Actuellement 1 r√©plique (car charge faible)
- **MINPODS/MAXPODS** : Limites configur√©es

### D√©tails complets du HPA

Pour plus d'informations :

```bash
kubectl describe hpa nginx-web-hpa
```

Sortie d√©taill√©e :
```
Name:                     nginx-web-hpa
Namespace:                default
Reference:                Deployment/nginx-web
Metrics:                  ( current / target )
  resource cpu on pods:   15% / 50%
Min replicas:             1
Max replicas:             10
Deployment pods:          1 current / 1 desired
Conditions:
  Type            Status  Reason            Message
  ----            ------  ------            -------
  AbleToScale     True    ReadyForNewScale  recommended size matches current size
  ScalingActive   True    ValidMetricFound  the HPA was able to successfully calculate a replica count
Events:
  Type    Reason             Age   From                       Message
  ----    ------             ----  ----                       -------
  Normal  SuccessfulRescale  2m    horizontal-pod-autoscaler  New size: 3; reason: cpu resource utilization above target
```

## Comprendre le calcul du HPA

Le HPA utilise une formule simple pour d√©terminer le nombre de r√©pliques :

```
Nombre de r√©pliques d√©sir√© = ‚åàR√©pliques actuelles √ó (M√©trique actuelle / M√©trique cible)‚åâ
```

**Exemple concret :**

√âtat actuel :
- 2 r√©pliques en cours d'ex√©cution
- Utilisation CPU moyenne : 80%
- Cible CPU : 50%

Calcul :
```
R√©pliques d√©sir√©es = 2 √ó (80 / 50) = 2 √ó 1.6 = 3.2 ‚Üí 4 r√©pliques
```

Le HPA va scaler √† **4 r√©pliques** (arrondi au sup√©rieur).

### Pourquoi cette formule ?

Si l'utilisation actuelle (80%) est sup√©rieure √† la cible (50%), cela signifie que les pods sont surcharg√©s. En ajoutant plus de r√©pliques, la charge se r√©partit et l'utilisation moyenne diminue.

## M√©triques disponibles

Le HPA peut se baser sur diff√©rents types de m√©triques :

### 1. M√©triques de ressources (CPU et M√©moire)

**CPU - La plus courante :**

```yaml
metrics:
- type: Resource
  resource:
    name: cpu
    target:
      type: Utilization
      averageUtilization: 50    # 50% d'utilisation CPU
```

**M√©moire :**

```yaml
metrics:
- type: Resource
  resource:
    name: memory
    target:
      type: Utilization
      averageUtilization: 70    # 70% d'utilisation m√©moire
```

**Important :** Pour que ces m√©triques fonctionnent, vos pods doivent avoir des `requests` d√©finis :

```yaml
resources:
  requests:
    cpu: "100m"
    memory: "128Mi"
  limits:
    cpu: "500m"
    memory: "256Mi"
```

### 2. M√©triques multiples

Vous pouvez combiner plusieurs m√©triques. Le HPA prendra le plus grand nombre de r√©pliques calcul√© :

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: multi-metric-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mon-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

**Comportement :**
- Si CPU recommande 5 r√©pliques et m√©moire recommande 3 r√©pliques
- Le HPA choisira **5 r√©pliques** (le maximum)

### 3. M√©triques personnalis√©es (avanc√©)

Le HPA peut aussi utiliser des m√©triques custom comme :
- Nombre de requ√™tes HTTP par seconde
- Longueur de la file d'attente de messages
- Latence moyenne des requ√™tes
- M√©triques m√©tier sp√©cifiques

Nous verrons cela dans la section 19.6 (Custom metrics).

## HPA en action : Simulation

Voyons comment le HPA r√©agit √† une augmentation de charge.

### √âtape 1 : Cr√©er un deployment simple

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
spec:
  replicas: 1
  selector:
    matchLabels:
      app: php-apache
  template:
    metadata:
      labels:
        app: php-apache
    spec:
      containers:
      - name: php-apache
        image: k8s.gcr.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 200m
          limits:
            cpu: 500m
```

Cette image est con√ßue pour consommer du CPU lors des requ√™tes.

### √âtape 2 : Cr√©er le HPA

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
```

### √âtape 3 : √âtat initial

```bash
kubectl get hpa php-apache-hpa --watch
```

Sortie initiale (charge faible) :
```
NAME              REFERENCE                TARGETS   MINPODS   MAXPODS   REPLICAS
php-apache-hpa    Deployment/php-apache    0%/50%    1         10        1
```

### √âtape 4 : G√©n√©rer de la charge (optionnel pour test)

Dans un nouveau terminal :

```bash
kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://php-apache; done"
```

Cela envoie des requ√™tes en continu vers l'application.

### √âtape 5 : Observer le scaling automatique

Avec `kubectl get hpa --watch`, vous verrez :

**Apr√®s 30 secondes :**
```
NAME              REFERENCE                TARGETS    MINPODS   MAXPODS   REPLICAS
php-apache-hpa    Deployment/php-apache    250%/50%   1         10        1
```

Le CPU monte √† 250% (beaucoup plus que la cible de 50%).

**Apr√®s 1 minute :**
```
NAME              REFERENCE                TARGETS   MINPODS   MAXPODS   REPLICAS
php-apache-hpa    Deployment/php-apache    250%/50%  1         10        5
```

Le HPA a scal√© √† **5 r√©pliques**.

**Apr√®s 2 minutes :**
```
NAME              REFERENCE                TARGETS   MINPODS   MAXPODS   REPLICAS
php-apache-hpa    Deployment/php-apache    45%/50%   1         10        5
```

Avec 5 r√©pliques, l'utilisation CPU est maintenant autour de 45%, proche de la cible de 50%.

**Apr√®s avoir arr√™t√© la charge (5-10 minutes) :**
```
NAME              REFERENCE                TARGETS   MINPODS   MAXPODS   REPLICAS
php-apache-hpa    Deployment/php-apache    0%/50%    1         10        1
```

Le HPA a automatiquement r√©duit √† 1 r√©plique (le minimum).

## Configuration avanc√©e du HPA

### Comportement de scaling (Behavior)

Depuis Kubernetes 1.18, vous pouvez contr√¥ler finement le comportement de scaling :

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: advanced-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mon-app
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 4
        periodSeconds: 15
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 15
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Min
```

**Explication des comportements :**

**scaleUp (mont√©e en charge) :**
- **stabilizationWindowSeconds: 0** : R√©agir imm√©diatement
- **100% toutes les 15 secondes** : Peut doubler le nombre de pods rapidement
- **4 pods toutes les 15 secondes** : Alternative pour scaling agressif
- **selectPolicy: Max** : Choisir le scaling le plus agressif

**scaleDown (r√©duction) :**
- **stabilizationWindowSeconds: 300** : Attendre 5 minutes avant de r√©duire
- **50% toutes les 15 secondes** : R√©duire progressivement
- **2 pods par minute** : R√©duction mod√©r√©e
- **selectPolicy: Min** : Choisir le scaling le plus conservateur

### Pourquoi ces diff√©rences ?

**Scale up agressif :** Mieux vaut avoir trop de capacit√© temporairement que de perdre des requ√™tes
**Scale down prudent :** √âviter les oscillations (flapping) et donner du temps pour observer la tendance

## Bonnes pratiques du HPA

### 1. Toujours d√©finir requests et limits

**Mauvais exemple (ne fonctionne pas) :**
```yaml
containers:
- name: app
  image: mon-app:latest
  # Pas de resources d√©finis !
```

**Bon exemple :**
```yaml
containers:
- name: app
  image: mon-app:latest
  resources:
    requests:
      cpu: "250m"
      memory: "128Mi"
    limits:
      cpu: "1000m"
      memory: "512Mi"
```

Sans `requests`, le HPA ne peut pas calculer les pourcentages !

### 2. Choisir des seuils r√©alistes

**Trop bas (inefficace) :**
```yaml
averageUtilization: 20    # Scale trop t√¥t, gaspillage
```

**Trop haut (dangereux) :**
```yaml
averageUtilization: 95    # Pas de marge, risque de saturation
```

**Recommand√© :**
```yaml
averageUtilization: 50-70    # Bon √©quilibre
```

### 3. D√©finir des limites min/max appropri√©es

```yaml
minReplicas: 2    # Au moins 2 pour la redondance
maxReplicas: 20   # Limite raisonnable pour √©viter d'exploser les co√ªts
```

**Erreur courante :**
```yaml
minReplicas: 1    # Dangereux : un seul point de d√©faillance
maxReplicas: 1000 # Irr√©aliste : pourrait saturer le cluster
```

### 4. Tester avec des charges r√©alistes

Avant la production, testez avec des outils de charge :
- Apache Bench (ab)
- Hey
- Locust
- K6

Exemple avec hey :
```bash
hey -z 5m -c 50 http://mon-app.example.com
```

Observez comment le HPA r√©agit et ajustez les param√®tres.

### 5. Surveiller le HPA

Utilisez les commandes suivantes r√©guli√®rement :

```bash
# √âtat g√©n√©ral
kubectl get hpa

# D√©tails et √©v√©nements
kubectl describe hpa <nom>

# M√©triques en temps r√©el
kubectl top pods

# Historique des scales
kubectl get events --field-selector involvedObject.name=<nom-hpa>
```

### 6. Commencer simple, complexifier progressivement

**√âtape 1 :** HPA bas√© sur CPU uniquement
```yaml
metrics:
- type: Resource
  resource:
    name: cpu
    target:
      type: Utilization
      averageUtilization: 50
```

**√âtape 2 :** Ajouter la m√©moire
```yaml
metrics:
- type: Resource
  resource:
    name: cpu
    target:
      type: Utilization
      averageUtilization: 50
- type: Resource
  resource:
    name: memory
    target:
      type: Utilization
      averageUtilization: 70
```

**√âtape 3 :** Ajouter des m√©triques custom (voir section 19.6)

### 7. Documenter vos choix

Utilisez des annotations pour expliquer vos d√©cisions :

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mon-hpa
  annotations:
    description: "HPA pour l'API principale. Scaling bas√© sur CPU √† 50%. Max 10 pods pour respecter le budget."
    contact: "equipe-devops@example.com"
    last-tuned: "2025-10-15"
    tuning-notes: "R√©duit maxReplicas de 20 √† 10 apr√®s analyse des co√ªts"
spec:
  # ...
```

## Cas d'usage courants

### 1. API web avec trafic variable

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-web-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-web
  minReplicas: 3      # Minimum pour haute dispo
  maxReplicas: 50     # Peut g√©rer de gros pics
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
```

### 2. Worker de traitement de files

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: queue-worker-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: queue-worker
  minReplicas: 1      # Peut √™tre √† 0 avec KEDA
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

### 3. Application batch nocturne

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: batch-processor-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: batch-processor
  minReplicas: 0      # Attention : n√©cessite une configuration sp√©ciale
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80
```

**Note :** Le scaling √† 0 n√©cessite des solutions suppl√©mentaires comme KEDA.

## Limitations et pi√®ges √† √©viter

### 1. Le HPA ne peut pas scaler en dessous de minReplicas

Si vous d√©finissez `minReplicas: 2`, le HPA ne descendra jamais √† 0 ou 1, m√™me si la charge est nulle.

### 2. Conflit avec le scaling manuel

**Ne faites pas ceci :**
```bash
# HPA est activ√©...
kubectl scale deployment mon-app --replicas=5  # Conflit !
```

Le HPA reprendra le contr√¥le et ajustera selon ses r√®gles. Si vous voulez scaler manuellement, d√©sactivez d'abord le HPA :

```bash
kubectl delete hpa mon-app-hpa
```

### 3. M√©triques non disponibles

Si Metrics Server ne fonctionne pas ou si les pods n'ont pas de `requests`, vous verrez :

```
TARGETS: <unknown>/50%
```

Solution : V√©rifiez Metrics Server et les resource requests.

### 4. Flapping (oscillations)

Le HPA peut osciller entre deux valeurs (ex: 3 ‚Üî 4 r√©pliques) si les seuils sont trop serr√©s.

**Solution :** Augmentez `stabilizationWindowSeconds` pour scale down :

```yaml
behavior:
  scaleDown:
    stabilizationWindowSeconds: 300  # 5 minutes
```

### 5. Startup lent des pods

Si vos pods mettent 2 minutes √† d√©marrer, le HPA peut cr√©er trop de r√©pliques avant que les premi√®res soient pr√™tes.

**Solution :** Configurez des `readinessProbe` appropri√©es.

### 6. Scaling trop lent face √† un pic brutal

Si votre application re√ßoit un trafic massif instantan√©, le HPA peut ne pas scaler assez vite.

**Solutions :**
- Augmentez `minReplicas` pour avoir plus de capacit√© de base
- Utilisez des politiques de scaling agressives
- Combinez avec le Cluster Autoscaler (multi-node)

## Supprimer un HPA

Pour d√©sactiver l'autoscaling :

```bash
kubectl delete hpa mon-app-hpa
```

Le Deployment continue de fonctionner avec son nombre actuel de r√©pliques, mais ne scale plus automatiquement.

## R√©sum√©

Le **Horizontal Pod Autoscaler (HPA)** est un outil puissant pour automatiser le scaling de vos applications Kubernetes.

**Points cl√©s :**

1. Le HPA ajuste automatiquement le nombre de r√©pliques selon les m√©triques observ√©es
2. N√©cessite Metrics Server pour fonctionner
3. Bas√© principalement sur CPU et m√©moire (mais extensible avec m√©triques custom)
4. Utilise une formule simple : `r√©pliques = r√©pliques_actuelles √ó (m√©trique_actuelle / m√©trique_cible)`
5. Plus r√©actif pour scale up que pour scale down (par design)
6. N√©cessite que les pods aient des `requests` de ressources d√©finies
7. D√©finissez toujours des limites min/max r√©alistes

**Comparaison avec le scaling manuel :**

| Aspect | Scaling Manuel | HPA |
|--------|---------------|-----|
| Intervention humaine | Requise | Automatique |
| R√©activit√© | Lente (d√©pend de vous) | Rapide (15s-3min) |
| Disponibilit√© 24/7 | Non | Oui |
| Optimisation co√ªts | Manuelle | Automatique |
| Complexit√© | Simple | Moyenne |
| Contr√¥le | Total | D√©l√©gu√© √† K8s |

**Quand utiliser le HPA ?**

‚úÖ Applications web avec trafic variable
‚úÖ APIs avec pics de charge
‚úÖ Workers de files d'attente
‚úÖ Services avec patterns pr√©visibles mais variables
‚úÖ Environnements de production n√©cessitant haute disponibilit√©

‚ùå Applications avec charge stable et pr√©visible (le scaling manuel suffit)
‚ùå Stateful applications complexes (bases de donn√©es) - n√©cessite une attention particuli√®re
‚ùå Applications avec temps de d√©marrage tr√®s long (>5 minutes)

Dans les prochaines sections, nous d√©couvrirons le **Vertical Pod Autoscaler (VPA)** qui ajuste les ressources CPU/RAM des pods, et comment utiliser des **m√©triques personnalis√©es** pour un scaling encore plus intelligent !

---

**Prochaine section :** 19.3 Vertical Pod Autoscaler (VPA) - Ajuster automatiquement les ressources de vos pods !

‚è≠Ô∏è [Vertical Pod Autoscaler (VPA)](/19-scaling-et-autoscaling/03-vertical-pod-autoscaler-vpa.md)
