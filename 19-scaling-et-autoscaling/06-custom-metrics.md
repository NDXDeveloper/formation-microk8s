ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 19.6 Custom Metrics

## Introduction aux Custom Metrics

Les **custom metrics (mÃ©triques personnalisÃ©es)** permettent au HPA de scaler vos applications basÃ© sur des mÃ©triques **mÃ©tier** ou **applicatives** plutÃ´t que simplement sur le CPU et la mÃ©moire. C'est un niveau avancÃ© d'autoscaling qui permet une rÃ©activitÃ© beaucoup plus pertinente.

### Le problÃ¨me avec les mÃ©triques CPU/mÃ©moire seules

Jusqu'Ã  prÃ©sent, nous avons utilisÃ© le HPA avec des mÃ©triques CPU et mÃ©moire :

```yaml
metrics:
- type: Resource
  resource:
    name: cpu
    target:
      type: Utilization
      averageUtilization: 50
```

**Limitation de cette approche :**

Imaginons une API web :
- **ScÃ©nario 1** : 1000 requÃªtes/sec, pods Ã  30% CPU â†’ HPA ne fait rien (sous le seuil de 50%)
- **ScÃ©nario 2** : 100 requÃªtes/sec mais complexes, pods Ã  70% CPU â†’ HPA scale up

Dans le scÃ©nario 1, votre application peut Ãªtre surchargÃ©e (latence Ã©levÃ©e) mais le HPA ne rÃ©agit pas car le CPU est bas. **C'est un problÃ¨me !**

**Ce qui serait mieux :**
Scaler basÃ© sur le **nombre de requÃªtes par seconde** ou la **latence moyenne**, pas le CPU.

### La solution : Custom Metrics

Avec les custom metrics, vous pouvez scaler basÃ© sur :

âœ… **RequÃªtes HTTP par seconde**
âœ… **Latence moyenne des rÃ©ponses**
âœ… **Longueur d'une file d'attente de messages**
âœ… **Nombre de connexions actives**
âœ… **Taux d'erreurs**
âœ… **MÃ©triques mÃ©tier** (commandes en attente, utilisateurs connectÃ©s, etc.)

### Analogie simple

**Autoscaling basÃ© sur CPU (ancien) :**
- Restaurant qui embauche des cuisiniers basÃ© sur la tempÃ©rature dans la cuisine
- Si la cuisine chauffe â†’ plus de cuisiniers
- **ProblÃ¨me :** La chaleur ne reflÃ¨te pas directement le nombre de clients

**Autoscaling basÃ© sur custom metrics (moderne) :**
- Restaurant qui embauche des cuisiniers basÃ© sur le nombre de commandes en attente
- 50 commandes en attente â†’ plus de cuisiniers
- **Avantage :** Indicateur direct du besoin rÃ©el

## Architecture des Custom Metrics

Pour utiliser les custom metrics avec le HPA, vous avez besoin de trois composants :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  HPA (Horizontal Pod Autoscaler)        â”‚
â”‚  "Je veux scaler basÃ© sur http_requests_per_second"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“ Interroge
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Custom Metrics API (Prometheus Adapter)        â”‚
â”‚  "Traduction : Prometheus, donne-moi cette mÃ©trique"    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“ Interroge
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Prometheus                           â”‚
â”‚  "J'ai collectÃ© cette mÃ©trique depuis vos pods"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†‘ Collecte
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Votre Application                      â”‚
â”‚  Expose /metrics avec http_requests_total, etc.         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Les trois composants expliquÃ©s

**1. Votre Application**
- Doit **exposer des mÃ©triques** au format Prometheus
- GÃ©nÃ©ralement via un endpoint `/metrics`
- Utilise des bibliothÃ¨ques client Prometheus (disponibles pour tous les langages)

**2. Prometheus**
- **Collecte** les mÃ©triques de vos applications (scraping)
- **Stocke** l'historique des mÃ©triques
- **Permet de les requÃªter** via PromQL
- Nous avons vu Prometheus dans la section 12

**3. Prometheus Adapter (Custom Metrics API)**
- **Pont** entre Prometheus et Kubernetes
- Traduit les requÃªtes du HPA en requÃªtes PromQL
- Expose la Custom Metrics API (`custom.metrics.k8s.io`)
- Le HPA interroge cette API au lieu de Metrics Server

## PrÃ©requis

Avant d'utiliser les custom metrics, vous devez avoir :

### 1. Prometheus installÃ©

```bash
microk8s enable prometheus
```

VÃ©rification :
```bash
kubectl get pods -n monitoring | grep prometheus
```

### 2. Votre application expose des mÃ©triques

Votre application doit exposer un endpoint `/metrics` au format Prometheus.

**Exemple de mÃ©triques exposÃ©es :**
```
# HELP http_requests_total Total HTTP requests
# TYPE http_requests_total counter
http_requests_total{method="GET",status="200"} 1547

# HELP http_request_duration_seconds HTTP request latency
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{le="0.1"} 824
http_request_duration_seconds_bucket{le="0.5"} 1420
http_request_duration_seconds_sum 345.2
http_request_duration_seconds_count 1547
```

Ne vous inquiÃ©tez pas si cela semble complexe ! Les bibliothÃ¨ques client Prometheus gÃ©nÃ¨rent cela automatiquement.

### 3. Prometheus Adapter installÃ©

C'est le composant qui fait le pont entre Prometheus et l'API Kubernetes.

## Installation du Prometheus Adapter

### MÃ©thode 1 : Via Helm (RecommandÃ©)

**Ã‰tape 1 : S'assurer que Helm est disponible**

```bash
microk8s enable helm3
alias helm='microk8s helm3'
```

**Ã‰tape 2 : Ajouter le repository Helm**

```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
```

**Ã‰tape 3 : CrÃ©er un fichier de configuration**

CrÃ©ez `prometheus-adapter-values.yaml` :

```yaml
prometheus:
  url: http://prometheus-k8s.monitoring.svc
  port: 9090

rules:
  default: true
  custom:
  - seriesQuery: 'http_requests_total{namespace!="",pod!=""}'
    resources:
      overrides:
        namespace: {resource: "namespace"}
        pod: {resource: "pod"}
    name:
      matches: "^(.*)_total"
      as: "${1}_per_second"
    metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[2m])) by (<<.GroupBy>>)'

  - seriesQuery: 'http_request_duration_seconds_sum{namespace!="",pod!=""}'
    resources:
      overrides:
        namespace: {resource: "namespace"}
        pod: {resource: "pod"}
    name:
      as: "http_request_latency_seconds"
    metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[2m])) by (<<.GroupBy>>) / sum(rate(http_request_duration_seconds_count{<<.LabelMatchers>>}[2m])) by (<<.GroupBy>>)'

resources:
  requests:
    cpu: 100m
    memory: 128Mi
  limits:
    cpu: 100m
    memory: 128Mi
```

Cette configuration dÃ©finit des **rÃ¨gles** pour transformer les mÃ©triques Prometheus en mÃ©triques Kubernetes.

**Ã‰tape 4 : Installer le Prometheus Adapter**

```bash
helm install prometheus-adapter prometheus-community/prometheus-adapter \
  --namespace monitoring \
  --values prometheus-adapter-values.yaml
```

**Ã‰tape 5 : VÃ©rifier l'installation**

```bash
kubectl get pods -n monitoring | grep prometheus-adapter
```

Sortie attendue :
```
prometheus-adapter-xxxxxxxxx-xxxxx   1/1     Running   0          2m
```

**Ã‰tape 6 : VÃ©rifier que l'API Custom Metrics est disponible**

```bash
kubectl get apiservices | grep custom.metrics
```

Sortie :
```
v1beta1.custom.metrics.k8s.io   monitoring/prometheus-adapter   True   5m
```

Le statut doit Ãªtre `True`.

## Exposer des mÃ©triques depuis votre application

Pour que tout cela fonctionne, votre application doit exposer des mÃ©triques. Voici comment faire selon le langage.

### Exemple en Python (Flask)

**Installer la bibliothÃ¨que Prometheus :**

```bash
pip install prometheus-client
```

**Code de l'application :**

```python
from flask import Flask, request
from prometheus_client import Counter, Histogram, generate_latest, REGISTRY
import time

app = Flask(__name__)

# DÃ©finir les mÃ©triques
request_count = Counter(
    'http_requests_total',
    'Total HTTP Requests',
    ['method', 'endpoint', 'status']
)

request_latency = Histogram(
    'http_request_duration_seconds',
    'HTTP Request Latency',
    ['method', 'endpoint']
)

@app.before_request
def before_request():
    request.start_time = time.time()

@app.after_request
def after_request(response):
    request_latency.labels(
        method=request.method,
        endpoint=request.path
    ).observe(time.time() - request.start_time)

    request_count.labels(
        method=request.method,
        endpoint=request.path,
        status=response.status_code
    ).inc()

    return response

@app.route('/')
def hello():
    return "Hello, World!"

@app.route('/api/data')
def get_data():
    time.sleep(0.1)  # Simuler un traitement
    return {"data": "example"}

@app.route('/metrics')
def metrics():
    return generate_latest(REGISTRY)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
```

**Endpoint /metrics :**

Quand vous appelez `http://votre-app:8080/metrics`, vous obtenez :

```
# HELP http_requests_total Total HTTP Requests
# TYPE http_requests_total counter
http_requests_total{method="GET",endpoint="/",status="200"} 1547
http_requests_total{method="GET",endpoint="/api/data",status="200"} 823

# HELP http_request_duration_seconds HTTP Request Latency
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{method="GET",endpoint="/api/data",le="0.005"} 0
http_request_duration_seconds_bucket{method="GET",endpoint="/api/data",le="0.01"} 0
http_request_duration_seconds_bucket{method="GET",endpoint="/api/data",le="0.1"} 620
http_request_duration_seconds_sum{method="GET",endpoint="/api/data"} 82.3
http_request_duration_seconds_count{method="GET",endpoint="/api/data"} 823
```

### Exemple en Go

```go
package main

import (
    "net/http"
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promhttp"
    "github.com/prometheus/client_golang/prometheus/promauto"
)

var (
    httpRequestsTotal = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "http_requests_total",
            Help: "Total HTTP requests",
        },
        []string{"method", "endpoint", "status"},
    )

    httpRequestDuration = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name: "http_request_duration_seconds",
            Help: "HTTP request latency",
        },
        []string{"method", "endpoint"},
    )
)

func helloHandler(w http.ResponseWriter, r *http.Request) {
    timer := prometheus.NewTimer(httpRequestDuration.WithLabelValues(r.Method, r.URL.Path))
    defer timer.ObserveDuration()

    w.Write([]byte("Hello, World!"))
    httpRequestsTotal.WithLabelValues(r.Method, r.URL.Path, "200").Inc()
}

func main() {
    http.HandleFunc("/", helloHandler)
    http.Handle("/metrics", promhttp.Handler())
    http.ListenAndServe(":8080", nil)
}
```

### Exemple en Node.js

```javascript
const express = require('express');
const promClient = require('prom-client');

const app = express();
const register = new promClient.Registry();

// DÃ©finir les mÃ©triques
const httpRequestsTotal = new promClient.Counter({
    name: 'http_requests_total',
    help: 'Total HTTP requests',
    labelNames: ['method', 'endpoint', 'status'],
    registers: [register]
});

const httpRequestDuration = new promClient.Histogram({
    name: 'http_request_duration_seconds',
    help: 'HTTP request latency',
    labelNames: ['method', 'endpoint'],
    registers: [register]
});

// Middleware pour capturer les mÃ©triques
app.use((req, res, next) => {
    const start = Date.now();

    res.on('finish', () => {
        const duration = (Date.now() - start) / 1000;
        httpRequestDuration.labels(req.method, req.path).observe(duration);
        httpRequestsTotal.labels(req.method, req.path, res.statusCode).inc();
    });

    next();
});

app.get('/', (req, res) => {
    res.send('Hello, World!');
});

app.get('/metrics', async (req, res) => {
    res.set('Content-Type', register.contentType);
    res.end(await register.metrics());
});

app.listen(8080, () => {
    console.log('Server running on port 8080');
});
```

## Configurer Prometheus pour scraper vos mÃ©triques

Une fois que votre application expose des mÃ©triques, vous devez dire Ã  Prometheus de les collecter.

### MÃ©thode 1 : Annotations sur le pod

La faÃ§on la plus simple avec le Prometheus installÃ© par MicroK8s :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mon-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mon-api
  template:
    metadata:
      labels:
        app: mon-api
      annotations:
        prometheus.io/scrape: "true"    # Activer le scraping
        prometheus.io/port: "8080"       # Port oÃ¹ /metrics est exposÃ©
        prometheus.io/path: "/metrics"   # Chemin de l'endpoint
    spec:
      containers:
      - name: api
        image: mon-api:latest
        ports:
        - containerPort: 8080
```

Avec ces annotations, Prometheus dÃ©couvre automatiquement vos pods et collecte les mÃ©triques.

### MÃ©thode 2 : ServiceMonitor (si vous utilisez Prometheus Operator)

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: mon-api-monitor
  labels:
    app: mon-api
spec:
  selector:
    matchLabels:
      app: mon-api
  endpoints:
  - port: http
    path: /metrics
    interval: 30s
```

### VÃ©rifier que Prometheus collecte les mÃ©triques

**1. AccÃ©der Ã  l'interface Prometheus :**

```bash
kubectl port-forward -n monitoring svc/prometheus-k8s 9090:9090
```

Puis ouvrez : http://localhost:9090

**2. Tester une requÃªte :**

Dans l'interface Prometheus, testez la requÃªte :

```
http_requests_total
```

Si vous voyez des rÃ©sultats, c'est que Prometheus collecte bien vos mÃ©triques !

## Configurer le HPA avec Custom Metrics

Maintenant que tout est en place, crÃ©ons un HPA qui utilise des custom metrics.

### Exemple 1 : Scaler basÃ© sur les requÃªtes HTTP par seconde

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mon-api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mon-api
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"    # 1000 requÃªtes/sec par pod
```

**Explication :**

- **type: Pods** : MÃ©trique par pod (pas par ressource comme CPU)
- **metric.name** : Nom de la mÃ©trique custom (dÃ©fini dans les rÃ¨gles du Prometheus Adapter)
- **averageValue: "1000"** : Chaque pod doit gÃ©rer en moyenne 1000 requÃªtes/sec
- Si un pod reÃ§oit 1500 req/sec â†’ scale up
- Si un pod reÃ§oit 500 req/sec â†’ scale down

**Comment Ã§a marche :**

```
Ã‰tat actuel :
- 2 pods
- Total : 4000 requÃªtes/sec
- Par pod : 2000 requÃªtes/sec

Calcul du HPA :
- Cible : 1000 requÃªtes/sec par pod
- Actuel : 2000 requÃªtes/sec par pod
- Pods nÃ©cessaires : (2000 / 1000) Ã— 2 = 4 pods

RÃ©sultat : HPA scale Ã  4 pods
```

### Exemple 2 : Scaler basÃ© sur la latence

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mon-api-hpa-latency
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mon-api
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Pods
    pods:
      metric:
        name: http_request_latency_seconds
      target:
        type: AverageValue
        averageValue: "0.5"    # Latence max : 500ms
```

Si la latence moyenne dÃ©passe 500ms, le HPA ajoute des pods pour amÃ©liorer les performances.

### Exemple 3 : Combiner CPU et custom metrics

Vous pouvez utiliser plusieurs mÃ©triques. Le HPA prendra le plus grand nombre de rÃ©pliques calculÃ© :

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mon-api-hpa-combined
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mon-api
  minReplicas: 2
  maxReplicas: 20
  metrics:
  # MÃ©trique CPU classique
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  # Custom metric : requÃªtes/sec
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"

  # Custom metric : latence
  - type: Pods
    pods:
      metric:
        name: http_request_latency_seconds
      target:
        type: AverageValue
        averageValue: "0.5"
```

**Comportement :**

Le HPA calcule le nombre de rÃ©pliques nÃ©cessaires pour chaque mÃ©trique :
- CPU recommande : 5 rÃ©pliques
- RequÃªtes/sec recommande : 8 rÃ©pliques
- Latence recommande : 6 rÃ©pliques

**RÃ©sultat :** Le HPA scale Ã  **8 rÃ©pliques** (le maximum).

## VÃ©rifier le fonctionnement

### Voir les custom metrics disponibles

```bash
kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1 | jq .
```

Vous devriez voir vos mÃ©triques custom listÃ©es.

### Consulter une mÃ©trique spÃ©cifique

```bash
kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests_per_second" | jq .
```

Sortie exemple :
```json
{
  "kind": "MetricValueList",
  "apiVersion": "custom.metrics.k8s.io/v1beta1",
  "metadata": {},
  "items": [
    {
      "describedObject": {
        "kind": "Pod",
        "namespace": "default",
        "name": "mon-api-xxxxx-abcd",
        "apiVersion": "/v1"
      },
      "metricName": "http_requests_per_second",
      "timestamp": "2025-10-25T10:30:00Z",
      "value": "1250"
    }
  ]
}
```

### Voir l'Ã©tat du HPA

```bash
kubectl get hpa mon-api-hpa
```

Sortie :
```
NAME           REFERENCE          TARGETS      MINPODS   MAXPODS   REPLICAS   AGE
mon-api-hpa    Deployment/mon-api 1250/1000    2         20        3          5m
```

**TARGETS** montre `1250/1000` :
- **1250** : Valeur actuelle (requÃªtes/sec par pod)
- **1000** : Cible configurÃ©e

Le HPA a scalÃ© Ã  3 rÃ©pliques pour ramener la charge par pod vers 1000.

### DÃ©tails du HPA

```bash
kubectl describe hpa mon-api-hpa
```

Sortie :
```
Name:                     mon-api-hpa
Namespace:                default
Reference:                Deployment/mon-api
Metrics:                  ( current / target )
  "http_requests_per_second" on pods:  1250 / 1k
Min replicas:             2
Max replicas:             20
Deployment pods:          3 current / 3 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    ReadyForNewScale    recommended size matches current size
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:
  Type    Reason             Age   From                       Message
  ----    ------             ----  ----                       -------
  Normal  SuccessfulRescale  2m    horizontal-pod-autoscaler  New size: 3; reason: pods metric http_requests_per_second above target
```

## Cas d'usage pratiques

### 1. API web avec charge variable

**ProblÃ¨me :** Scaler basÃ© uniquement sur le CPU ne reflÃ¨te pas la vraie charge.

**Solution :** Scaler basÃ© sur les requÃªtes/sec et la latence.

```yaml
metrics:
- type: Pods
  pods:
    metric:
      name: http_requests_per_second
    target:
      type: AverageValue
      averageValue: "1000"
```

### 2. Worker de file d'attente (RabbitMQ, Kafka)

**ProblÃ¨me :** Besoin de scaler basÃ© sur le nombre de messages en attente, pas le CPU.

**MÃ©trique Ã  exposer :**
```
queue_messages_pending{queue="orders"} 150
```

**HPA :**
```yaml
metrics:
- type: Pods
  pods:
    metric:
      name: queue_messages_pending
    target:
      type: AverageValue
      averageValue: "10"    # Max 10 messages par worker
```

Si la file a 150 messages et 5 workers :
- Par worker : 30 messages
- Cible : 10 messages par worker
- HPA scale Ã  : 150 / 10 = 15 workers

### 3. Connexions WebSocket

**ProblÃ¨me :** Nombre de connexions simultanÃ©es limitÃ©es par pod.

**MÃ©trique Ã  exposer :**
```
websocket_connections_active 450
```

**HPA :**
```yaml
metrics:
- type: Pods
  pods:
    metric:
      name: websocket_connections_active
    target:
      type: AverageValue
      averageValue: "100"    # Max 100 connexions par pod
```

### 4. Traitement batch basÃ© sur le backlog

**ProblÃ¨me :** Jobs en attente qui s'accumulent.

**MÃ©trique Ã  exposer :**
```
batch_jobs_pending 5000
```

**HPA :**
```yaml
metrics:
- type: Pods
  pods:
    metric:
      name: batch_jobs_pending
    target:
      type: AverageValue
      averageValue: "50"    # Chaque worker traite 50 jobs
```

### 5. Gaming : Joueurs connectÃ©s

**ProblÃ¨me :** Serveurs de jeu qui doivent scaler selon le nombre de joueurs.

**MÃ©trique Ã  exposer :**
```
game_players_connected{server="lobby"} 250
```

**HPA :**
```yaml
metrics:
- type: Pods
  pods:
    metric:
      name: game_players_connected
    target:
      type: AverageValue
      averageValue: "50"    # Max 50 joueurs par instance
```

## RÃ¨gles du Prometheus Adapter expliquÃ©es

Les rÃ¨gles dans la configuration du Prometheus Adapter transforment les mÃ©triques Prometheus en mÃ©triques Kubernetes.

### Anatomie d'une rÃ¨gle

```yaml
- seriesQuery: 'http_requests_total{namespace!="",pod!=""}'
  resources:
    overrides:
      namespace: {resource: "namespace"}
      pod: {resource: "pod"}
  name:
    matches: "^(.*)_total"
    as: "${1}_per_second"
  metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[2m])) by (<<.GroupBy>>)'
```

**seriesQuery :**
- SÃ©lectionne quelles mÃ©triques Prometheus utiliser
- Ici : `http_requests_total` avec les labels `namespace` et `pod`

**resources.overrides :**
- Mappe les labels Prometheus aux ressources Kubernetes
- `namespace` â†’ ressource Kubernetes `namespace`
- `pod` â†’ ressource Kubernetes `pod`

**name :**
- Transforme le nom de la mÃ©trique
- `matches: "^(.*)_total"` : Capture tout avant `_total`
- `as: "${1}_per_second"` : Renomme en `_per_second`
- Exemple : `http_requests_total` â†’ `http_requests_per_second`

**metricsQuery :**
- RequÃªte PromQL pour calculer la valeur finale
- `rate(...[2m])` : Calcule le taux par seconde sur 2 minutes
- `sum(...) by (...)` : AgrÃ¨ge par pod/namespace

### Exemple de transformation

**MÃ©trique Prometheus brute :**
```
http_requests_total{namespace="default",pod="mon-api-xxxxx",method="GET",status="200"} 15470
```

**AprÃ¨s transformation (rate sur 2 minutes) :**
```
http_requests_per_second{namespace="default",pod="mon-api-xxxxx"} 128.5
```

Cette mÃ©trique transformÃ©e est maintenant disponible pour le HPA !

### RÃ¨gle pour la latence

```yaml
- seriesQuery: 'http_request_duration_seconds_sum{namespace!="",pod!=""}'
  resources:
    overrides:
      namespace: {resource: "namespace"}
      pod: {resource: "pod"}
  name:
    as: "http_request_latency_seconds"
  metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[2m])) by (<<.GroupBy>>) / sum(rate(http_request_duration_seconds_count{<<.LabelMatchers>>}[2m])) by (<<.GroupBy>>)'
```

**Calcul de la latence moyenne :**
```
Latence = Somme des durÃ©es / Nombre de requÃªtes
        = rate(http_request_duration_seconds_sum) / rate(http_request_duration_seconds_count)
```

C'est un calcul PromQL standard pour obtenir la latence moyenne.

## Types de mÃ©triques custom

Il existe trois types de mÃ©triques que vous pouvez utiliser avec le HPA :

### 1. Pods metrics

MÃ©triques **par pod** (ce que nous avons vu principalement) :

```yaml
- type: Pods
  pods:
    metric:
      name: http_requests_per_second
    target:
      type: AverageValue
      averageValue: "1000"
```

**Usage :** Quand la mÃ©trique est directement liÃ©e Ã  chaque pod.

### 2. Object metrics

MÃ©triques liÃ©es Ã  un **objet Kubernetes spÃ©cifique** (Ingress, Service, etc.) :

```yaml
- type: Object
  object:
    metric:
      name: requests_per_second
    describedObject:
      apiVersion: networking.k8s.io/v1
      kind: Ingress
      name: mon-ingress
    target:
      type: Value
      value: "10000"    # 10k requÃªtes/sec au total (pas par pod)
```

**Usage :** Quand la mÃ©trique concerne l'entrÃ©e/sortie globale, pas individuellement par pod.

### 3. External metrics

MÃ©triques provenant de sources **externes** Ã  Kubernetes (AWS CloudWatch, Datadog, etc.) :

```yaml
- type: External
  external:
    metric:
      name: sqs_queue_length
      selector:
        matchLabels:
          queue: "orders"
    target:
      type: AverageValue
      averageValue: "30"
```

**Usage :** Scaler basÃ© sur des mÃ©triques cloud (longueur file SQS, mÃ©triques RDS, etc.).

## Debugging et dÃ©pannage

### ProblÃ¨me 1 : HPA affiche "unknown" pour custom metric

**SymptÃ´me :**

```bash
kubectl get hpa
NAME      REFERENCE        TARGETS         MINPODS   MAXPODS   REPLICAS
my-hpa    Deployment/app   <unknown>/1000  2         10        2
```

**Causes possibles :**

**1. Prometheus Adapter pas installÃ© ou pas fonctionnel**

VÃ©rifier :
```bash
kubectl get pods -n monitoring | grep prometheus-adapter
```

**2. MÃ©trique pas exposÃ©e par l'application**

VÃ©rifier que `/metrics` fonctionne :
```bash
kubectl port-forward pod/mon-api-xxxxx 8080:8080
curl http://localhost:8080/metrics | grep http_requests
```

**3. Prometheus ne collecte pas la mÃ©trique**

Dans l'interface Prometheus (port-forward 9090), tester :
```
http_requests_total
```

Si aucun rÃ©sultat â†’ Prometheus ne collecte pas. VÃ©rifier les annotations du pod.

**4. RÃ¨gle du Prometheus Adapter incorrecte**

Consulter les logs :
```bash
kubectl logs -n monitoring deployment/prometheus-adapter
```

Cherchez des erreurs de parsing des rÃ¨gles.

**5. API Custom Metrics pas enregistrÃ©e**

VÃ©rifier :
```bash
kubectl get apiservices | grep custom.metrics
```

Doit afficher `True`.

### ProblÃ¨me 2 : MÃ©trique disponible mais HPA ne scale pas

**SymptÃ´me :**

La mÃ©trique a une valeur, mais le HPA reste Ã  2 rÃ©pliques alors qu'il devrait scaler.

**Causes possibles :**

**1. Valeur en dessous du seuil**

VÃ©rifier les valeurs rÃ©elles :
```bash
kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests_per_second" | jq .
```

Si la valeur est infÃ©rieure Ã  la cible, pas de scale up normal.

**2. Cooldown period**

Le HPA attend un certain temps avant de scaler :
- **Scale up** : 3 minutes par dÃ©faut
- **Scale down** : 5 minutes par dÃ©faut

**3. maxReplicas atteint**

VÃ©rifier :
```bash
kubectl get hpa my-hpa
```

Si `REPLICAS` = `MAXPODS`, vous avez atteint la limite.

### ProblÃ¨me 3 : Scaling instable (flapping)

**SymptÃ´me :**

Le HPA scale up puis down puis up constamment (oscillations).

**Causes :**

- Seuil trop proche de la valeur actuelle
- MÃ©triques trop volatiles

**Solutions :**

**1. Augmenter la fenÃªtre de calcul dans les rÃ¨gles :**

```yaml
metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[5m])) by (<<.GroupBy>>)'
```

Changez `[2m]` en `[5m]` pour lisser les variations.

**2. Configurer le comportement du HPA :**

```yaml
behavior:
  scaleDown:
    stabilizationWindowSeconds: 300    # 5 minutes
    policies:
    - type: Percent
      value: 50
      periodSeconds: 60
```

**3. Augmenter la marge du seuil :**

Au lieu de `averageValue: "1000"`, utilisez `averageValue: "800"` pour avoir plus de marge.

## MÃ©triques custom avancÃ©es

### MÃ©triques business

Vous pouvez exposer n'importe quelle mÃ©trique mÃ©tier :

```python
orders_pending = Gauge('orders_pending', 'Pending orders waiting for processing')
users_active = Gauge('users_active', 'Currently active users')
revenue_per_minute = Counter('revenue_per_minute', 'Revenue per minute')

# Dans votre code
orders_pending.set(len(pending_orders))
users_active.set(count_active_users())
revenue_per_minute.inc(order.amount)
```

Puis scaler basÃ© sur ces mÃ©triques !

### MÃ©triques composÃ©es

Vous pouvez crÃ©er des mÃ©triques calculÃ©es dans Prometheus Adapter :

**Exemple : Ratio erreurs/succÃ¨s**

```yaml
- seriesQuery: 'http_requests_total'
  resources:
    overrides:
      namespace: {resource: "namespace"}
      pod: {resource: "pod"}
  name:
    as: "http_error_rate"
  metricsQuery: 'sum(rate(http_requests_total{status=~"5.."}[2m])) by (<<.GroupBy>>) / sum(rate(http_requests_total[2m])) by (<<.GroupBy>>)'
```

Cela calcule le pourcentage de requÃªtes en erreur (5xx).

## Limitations des custom metrics

### 1. ComplexitÃ© accrue

Configuration plus complexe que les mÃ©triques CPU/mÃ©moire :
- Besoin de Prometheus
- Besoin de Prometheus Adapter
- Configuration des rÃ¨gles
- Instrumentation de l'application

### 2. DÃ©lai de rÃ©action

Les custom metrics ont un dÃ©lai :
- Application expose la mÃ©trique (temps rÃ©el)
- Prometheus collecte toutes les 30-60s
- Prometheus Adapter interroge Prometheus
- HPA interroge l'Adapter toutes les 15s
- **DÃ©lai total : 1-2 minutes**

Pour des rÃ©actions instantanÃ©es, ce n'est pas idÃ©al.

### 3. Debugging plus difficile

Plus de composants = plus de points de dÃ©faillance :
- Application ne gÃ©nÃ¨re pas la mÃ©trique ?
- Prometheus ne la collecte pas ?
- RÃ¨gle de l'Adapter incorrecte ?
- Nom de mÃ©trique incorrect dans le HPA ?

### 4. Overhead

Plus de ressources consommÃ©es :
- Prometheus stocke toutes les mÃ©triques
- Prometheus Adapter effectue des calculs
- Plus de trafic rÃ©seau

## Bonnes pratiques

### 1. Commencer simple

DÃ©marrez avec des mÃ©triques CPU/mÃ©moire, puis ajoutez progressivement des custom metrics :

```yaml
# Phase 1 : CPU uniquement
metrics:
- type: Resource
  resource:
    name: cpu
    target:
      type: Utilization
      averageUtilization: 70

# Phase 2 : Ajouter requÃªtes/sec
metrics:
- type: Resource
  resource:
    name: cpu
    target:
      type: Utilization
      averageUtilization: 70
- type: Pods
  pods:
    metric:
      name: http_requests_per_second
    target:
      type: AverageValue
      averageValue: "1000"
```

### 2. Choisir les bonnes mÃ©triques

Les meilleures mÃ©triques pour l'autoscaling sont :
- âœ… **Proportionnelles Ã  la charge** (requÃªtes/sec, messages en file)
- âœ… **Faciles Ã  interprÃ©ter** (valeurs claires)
- âœ… **Stables** (pas trop volatiles)

Ã‰vitez :
- âŒ MÃ©triques binaires (0 ou 1)
- âŒ MÃ©triques trop volatiles (fluctuations constantes)
- âŒ MÃ©triques sans lien direct avec la charge

### 3. Tester en environnement de dev

Avant la production :

```bash
# GÃ©nÃ©rer de la charge
hey -z 5m -c 100 http://mon-api.example.com

# Observer le HPA en temps rÃ©el
watch -n 5 kubectl get hpa
```

Ajustez les seuils selon les rÃ©sultats observÃ©s.

### 4. Surveiller le HPA

Configurez des alertes pour :
- HPA qui n'arrive pas Ã  obtenir les mÃ©triques
- HPA qui atteint constamment maxReplicas
- Oscillations (flapping)

### 5. Documenter vos mÃ©triques

```yaml
metadata:
  annotations:
    metrics-documentation: |
      http_requests_per_second: Nombre de requÃªtes HTTP par seconde par pod.
      Cible: 1000 req/sec par pod (latence < 100ms observÃ©e Ã  cette charge).
      DerniÃ¨re calibration: 2025-10-25
```

### 6. Utiliser des noms de mÃ©triques cohÃ©rents

Convention Prometheus :
- `_total` pour les counters (cumulatifs)
- `_seconds` pour les durÃ©es
- `_bytes` pour les tailles
- Pas d'unitÃ© pour les gauges

### 7. Attention au coÃ»t

Les custom metrics peuvent gÃ©nÃ©rer beaucoup de donnÃ©es dans Prometheus. Surveillez :
- La taille de stockage Prometheus
- Le nombre de time series
- L'utilisation CPU/RAM de Prometheus

## RÃ©sumÃ©

Les **custom metrics** permettent un autoscaling intelligent basÃ© sur des mÃ©triques mÃ©tier plutÃ´t que simplement CPU/mÃ©moire.

**Points clÃ©s :**

1. NÃ©cessite trois composants : Application instrumentÃ©e, Prometheus, Prometheus Adapter
2. Permet de scaler sur n'importe quelle mÃ©trique (requÃªtes/sec, latence, files d'attente, etc.)
3. Plus pertinent que CPU/mÃ©moire pour beaucoup d'applications
4. Configuration plus complexe mais beaucoup plus puissant
5. DÃ©lai de rÃ©action de 1-2 minutes (pas instantanÃ©)
6. Trois types : Pods metrics, Object metrics, External metrics

**Quand utiliser les custom metrics ?**

âœ… API web oÃ¹ la charge n'est pas reflÃ©tÃ©e par le CPU
âœ… Workers de files d'attente (RabbitMQ, Kafka, SQS)
âœ… Applications WebSocket (connexions simultanÃ©es)
âœ… MÃ©triques business (commandes en attente, utilisateurs actifs)
âœ… Latence comme critÃ¨re de qualitÃ© de service

âŒ Applications simples oÃ¹ CPU/mÃ©moire suffisent
âŒ Si vous n'avez pas les ressources pour maintenir Prometheus
âŒ Besoin de rÃ©activitÃ© instantanÃ©e (< 30 secondes)

**Progression recommandÃ©e :**

1. **Scaling manuel** â†’ Comprendre les bases
2. **HPA avec CPU** â†’ Autoscaling basique
3. **Metrics Server** â†’ Fondation technique
4. **Custom metrics** â†’ Autoscaling intelligent

Les custom metrics reprÃ©sentent le **niveau expert** de l'autoscaling Kubernetes. Elles offrent une prÃ©cision et une pertinence inÃ©galÃ©es, mais nÃ©cessitent plus d'investissement en configuration et maintenance.

---

**Prochaine section :** 19.7 Tests de charge - Valider que votre autoscaling fonctionne correctement !

â­ï¸ [Tests de charge](/19-scaling-et-autoscaling/07-tests-de-charge.md)
