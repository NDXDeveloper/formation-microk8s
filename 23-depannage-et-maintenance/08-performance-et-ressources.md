üîù Retour au [Sommaire](/SOMMAIRE.md)

# 23.8 Performance et Ressources

## Introduction

La gestion des ressources (CPU et m√©moire) est cruciale dans Kubernetes. Une mauvaise configuration peut entra√Æner des pods qui ne d√©marrent pas, des applications lentes, des crashs, ou un cluster entier qui devient instable. Comprendre comment Kubernetes g√®re les ressources et savoir diagnostiquer les probl√®mes de performance est essentiel pour maintenir un cluster sain.

> **Pour les d√©butants** : Imaginez votre cluster Kubernetes comme un h√¥tel. Chaque pod est comme un client qui r√©serve une chambre (ressources). Si un client demande une suite pr√©sidentielle (beaucoup de ressources) mais qu'il n'y en a plus de disponible, il ne peut pas s'enregistrer (le pod ne d√©marre pas). Si certains clients utilisent trop de ressources communes (salle de sport, restaurant), cela d√©grade l'exp√©rience pour tous (ralentissement du cluster). Cette section vous apprend √† g√©rer efficacement cet "h√¥tel".

## Comprendre les Ressources dans Kubernetes

### Les Deux Types de Ressources

Kubernetes g√®re principalement deux types de ressources :

#### 1. CPU (Processeur)

**Unit√©s** :
- Mesur√©e en "cores" ou "millicores" (m)
- 1 core = 1000 millicores (1000m)
- 100m = 0.1 core = 10% d'un CPU

**Exemples** :
```yaml
resources:
  requests:
    cpu: 100m      # 0.1 core
  limits:
    cpu: 500m      # 0.5 core
```

**Caract√©ristique importante** : La CPU est une ressource **compressible**
- Si un pod demande plus de CPU qu'allou√©, il sera simplement ralenti (throttled)
- Le pod ne crashe pas, il tourne juste plus lentement

#### 2. M√©moire (RAM)

**Unit√©s** :
- Mesur√©e en bytes, mais on utilise g√©n√©ralement :
  - `Ki` = Kibibytes (1024 bytes)
  - `Mi` = Mebibytes (1024 Ki)
  - `Gi` = Gibibytes (1024 Mi)

**Exemples** :
```yaml
resources:
  requests:
    memory: 128Mi   # 128 Mebibytes
  limits:
    memory: 512Mi   # 512 Mebibytes
```

**Caract√©ristique importante** : La m√©moire est une ressource **incompressible**
- Si un pod d√©passe sa limite de m√©moire, il est **tu√©** (OOMKilled - Out Of Memory Killed)
- Le pod crashe et red√©marre

### Requests vs Limits

Deux concepts fondamentaux :

#### Requests (Demandes)

**D√©finition** : La quantit√© **minimale** de ressources garantie au pod.

**Utilisation** :
- Kubernetes utilise les requests pour **d√©cider sur quel n≈ìud placer le pod**
- Le scheduler cherche un n≈ìud avec suffisamment de ressources disponibles

**Exemple** :
```yaml
resources:
  requests:
    cpu: 250m
    memory: 256Mi
```

Kubernetes garantit que ce pod aura au moins 250m CPU et 256Mi de m√©moire disponibles.

#### Limits (Limites)

**D√©finition** : La quantit√© **maximale** de ressources qu'un pod peut utiliser.

**Utilisation** :
- Emp√™che un pod de monopoliser toutes les ressources
- Protection contre les fuites m√©moire ou boucles infinies

**Exemple** :
```yaml
resources:
  limits:
    cpu: 1000m
    memory: 1Gi
```

Le pod ne pourra jamais utiliser plus d'1 CPU core ou 1 Gi de m√©moire.

### Diff√©rence entre Requests et Limits

**Analogie** :
- **Request** = La taille de chambre d'h√¥tel que vous avez r√©serv√©e (garantie)
- **Limit** = La taille maximale que votre chambre peut avoir (vous ne pouvez pas agrandir au-del√†)

**Configuration recommand√©e** :
```yaml
resources:
  requests:
    cpu: 100m        # Ce dont j'ai besoin minimum
    memory: 128Mi
  limits:
    cpu: 500m        # Ce que je peux utiliser maximum
    memory: 512Mi
```

**Cas sp√©ciaux** :

1. **Pas de requests ni limits** :
```yaml
# Aucune garantie, aucune limite
# Mauvaise pratique en production !
```

2. **Requests seulement** :
```yaml
resources:
  requests:
    cpu: 250m
    memory: 256Mi
# Pas de limits = peut utiliser autant que disponible
```

3. **Limits seulement** :
```yaml
resources:
  limits:
    cpu: 500m
    memory: 512Mi
# Kubernetes met automatiquement requests = limits
```

4. **Requests = Limits** :
```yaml
resources:
  requests:
    cpu: 500m
    memory: 512Mi
  limits:
    cpu: 500m
    memory: 512Mi
# QoS = Guaranteed (meilleure garantie)
```

## Probl√®me 1 : Pod Bloqu√© en Pending (Ressources Insuffisantes)

### Sympt√¥mes

Le pod reste en √©tat `Pending` ind√©finiment.

```bash
microk8s kubectl get pods
# NAME      READY   STATUS    RESTARTS   AGE
# mon-pod   0/1     Pending   0          5m
```

**Message typique** :
```
0/1 nodes are available: 1 Insufficient cpu.
0/1 nodes are available: 1 Insufficient memory.
```

### Diagnostic

#### √âtape 1 : Examiner le Pod

```bash
microk8s kubectl describe pod mon-pod
```

**Chercher dans Events** :
```
Events:
  Type     Reason            Message
  ----     ------            -------
  Warning  FailedScheduling  0/1 nodes are available: 1 Insufficient cpu.
```

#### √âtape 2 : Voir les Ressources Demand√©es

```bash
# Ressources demand√©es par le pod
microk8s kubectl get pod mon-pod -o jsonpath='{.spec.containers[*].resources}'
```

#### √âtape 3 : V√©rifier les Ressources Disponibles

```bash
# Vue d'ensemble du n≈ìud
microk8s kubectl describe node

# Chercher les sections :
# - Capacity: Ressources totales du n≈ìud
# - Allocatable: Ressources disponibles pour les pods
# - Allocated resources: Ce qui est d√©j√† utilis√©
```

**Exemple de sortie** :
```
Capacity:
  cpu:                4
  memory:             8Gi
Allocatable:
  cpu:                4
  memory:             7.5Gi
Allocated resources:
  Resource           Requests      Limits
  --------           --------      ------
  cpu                3500m (87%)   7000m (175%)
  memory             6Gi (80%)     12Gi (160%)
```

**Interpr√©tation** :
- **Capacity** : Ressources physiques totales
- **Allocatable** : Un peu moins (r√©serv√© pour le syst√®me)
- **Requests** : Ce qui est garanti aux pods existants
- **Limits** : Ce que les pods peuvent utiliser au maximum

#### √âtape 4 : Calculer les Ressources Disponibles

```bash
# Script pour voir les ressources disponibles
microk8s kubectl describe node | grep -A 5 "Allocated resources"
```

**Disponible pour nouveaux pods** = Allocatable - Requests (pas Limits !)

### Solutions Courantes

#### Solution 1 : R√©duire les Requests du Pod

Si le pod demande trop de ressources :

```yaml
# Avant
resources:
  requests:
    cpu: 2000m      # 2 cores
    memory: 4Gi

# Apr√®s
resources:
  requests:
    cpu: 500m       # 0.5 core
    memory: 1Gi
```

**Comment d√©terminer les bonnes valeurs** :
1. Commencer avec des valeurs conservatrices
2. Surveiller l'utilisation r√©elle
3. Ajuster en fonction des besoins

#### Solution 2 : Supprimer des Pods pour Lib√©rer des Ressources

```bash
# Identifier les pods gourmands
microk8s kubectl top pods --sort-by=memory
microk8s kubectl top pods --sort-by=cpu

# Supprimer ou r√©duire l'√©chelle de certains pods
microk8s kubectl scale deployment app-non-critique --replicas=0
```

#### Solution 3 : Ajouter de la Capacit√© au N≈ìud

**Option 1 : Augmenter les ressources du n≈ìud existant**
- Ajouter de la RAM
- Ajouter des CPU cores
- Red√©marrer MicroK8s

**Option 2 : Ajouter un n≈ìud au cluster**
```bash
# Sur le n≈ìud existant
microk8s add-node

# Sur le nouveau n≈ìud (suivre les instructions affich√©es)
microk8s join <token>
```

#### Solution 4 : Optimiser les Requests Existants

Beaucoup de pods demandent trop de ressources "au cas o√π" :

```bash
# Voir l'utilisation r√©elle vs demand√©e
microk8s kubectl top pods

# Comparer avec les requests
microk8s kubectl get pods -o custom-columns=NAME:.metadata.name,CPU_REQUEST:.spec.containers[0].resources.requests.cpu,MEM_REQUEST:.spec.containers[0].resources.requests.memory
```

Si un pod demande 1Gi mais n'utilise que 200Mi, vous pouvez r√©duire ses requests.

## Probl√®me 2 : Pod Tu√© (OOMKilled)

### Sympt√¥mes

Le pod est constamment red√©marr√© avec le statut `OOMKilled` (Out Of Memory).

```bash
microk8s kubectl get pods
# NAME      READY   STATUS      RESTARTS   AGE
# mon-pod   0/1     OOMKilled   5          10m

# Ou apr√®s red√©marrage
# NAME      READY   STATUS      RESTARTS   AGE
# mon-pod   1/1     Running     5          10m
```

**RESTARTS √©lev√©** est un indicateur cl√©.

### Diagnostic

#### √âtape 1 : Confirmer le OOMKill

```bash
# Voir les d√©tails du pod
microk8s kubectl describe pod mon-pod

# Chercher dans Events
# Events:
#   Warning  OOMKilled  Container was OOMKilled
```

**Dans les logs du conteneur pr√©c√©dent** :
```bash
microk8s kubectl logs mon-pod --previous
```

Souvent, vous verrez les derniers messages avant le kill.

#### √âtape 2 : Voir les Limites Actuelles

```bash
# Limites du pod
microk8s kubectl get pod mon-pod -o jsonpath='{.spec.containers[*].resources.limits.memory}'
```

#### √âtape 3 : Voir l'Utilisation R√©elle

```bash
# Utilisation actuelle (si le pod tourne)
microk8s kubectl top pod mon-pod

# Sortie :
# NAME      CPU(cores)   MEMORY(bytes)
# mon-pod   100m         450Mi
```

**Note** : Le pod peut √™tre tu√© avant que vous ne puissiez voir son pic d'utilisation.

#### √âtape 4 : Comprendre Pourquoi

**Causes courantes** :
1. **Limite trop basse** : L'application a r√©ellement besoin de plus de m√©moire
2. **Fuite m√©moire** : Bug dans l'application qui accumule de la m√©moire
3. **Pic temporaire** : Charge inhabituelle qui d√©passe les limites
4. **Pas de limite d√©finie** : Le pod utilise toute la m√©moire disponible

### Solutions Courantes

#### Solution 1 : Augmenter la Limite M√©moire

Si l'application a besoin de plus de m√©moire :

```yaml
# Avant
resources:
  limits:
    memory: 512Mi

# Apr√®s
resources:
  limits:
    memory: 1Gi      # Doubler ou augmenter selon besoin
```

**Attention** : Assurez-vous que le n≈ìud a assez de m√©moire totale.

#### Solution 2 : Identifier et Corriger une Fuite M√©moire

**Signes d'une fuite** :
- Utilisation m√©moire qui augmente constamment
- Ne redescend jamais
- OOMKilled apr√®s un certain temps de fonctionnement

**Diagnostic** :
```bash
# Surveiller l'utilisation dans le temps
watch -n 5 microk8s kubectl top pod mon-pod
```

**Solution** : Corriger le code de l'application (hors scope Kubernetes).

#### Solution 3 : Red√©marrer P√©riodiquement

Si vous ne pouvez pas corriger la fuite imm√©diatement, red√©marrez p√©riodiquement :

```yaml
# Ajouter une livenessProbe qui force le red√©marrage
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 300
  periodSeconds: 60
  timeoutSeconds: 5
  failureThreshold: 3
```

**Ou utiliser un CronJob pour red√©marrer** :
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: restart-leaky-app
spec:
  schedule: "0 2 * * *"  # Tous les jours √† 2h
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: pod-restarter
          containers:
          - name: kubectl
            image: bitnami/kubectl:latest
            command:
            - /bin/sh
            - -c
            - kubectl rollout restart deployment/mon-app
          restartPolicy: Never
```

#### Solution 4 : Optimiser l'Application

**Conseils g√©n√©raux** :
- Utiliser des pools de connexions
- Limiter le cache en m√©moire
- Utiliser Redis/Memcached pour cache externe
- Nettoyer les objets non utilis√©s
- Utiliser des profilers pour identifier les gourmands en m√©moire

## Probl√®me 3 : Performance Lente (CPU Throttling)

### Sympt√¥mes

- Application anormalement lente
- Timeouts fr√©quents
- Requ√™tes qui prennent beaucoup de temps

### Diagnostic

#### √âtape 1 : V√©rifier l'Utilisation CPU

```bash
# Utilisation actuelle
microk8s kubectl top pod mon-pod

# Sortie :
# NAME      CPU(cores)   MEMORY(bytes)
# mon-pod   490m         256Mi
```

#### √âtape 2 : Comparer avec les Limits

```bash
# Voir les limits CPU
microk8s kubectl get pod mon-pod -o jsonpath='{.spec.containers[*].resources.limits.cpu}'
# Sortie : 500m
```

**Si l'utilisation est proche ou √©gale √† la limit** : Le pod est probablement throttled (brid√©).

#### √âtape 3 : V√©rifier le Throttling

Sur le n≈ìud, vous pouvez voir les statistiques de throttling :

```bash
# Trouver le container ID
CONTAINER_ID=$(microk8s kubectl get pod mon-pod -o jsonpath='{.status.containerStatuses[0].containerID}' | cut -d'/' -f3)

# Voir les stats de throttling
cat /sys/fs/cgroup/cpu/kubepods.slice/*/cri-containerd-${CONTAINER_ID}.scope/cpu.stat

# Chercher :
# nr_throttled: nombre de fois throttl√©
# throttled_time: temps total de throttling
```

#### √âtape 4 : Profiler l'Application

**Indices de throttling** :
- Haute utilisation CPU constante
- Performance qui varie beaucoup
- Pics de latence r√©guliers

### Solutions Courantes

#### Solution 1 : Augmenter la Limite CPU

```yaml
# Avant
resources:
  limits:
    cpu: 500m

# Apr√®s
resources:
  limits:
    cpu: 1000m    # 1 core
```

#### Solution 2 : Supprimer la Limite CPU

Pour certaines applications, supprimer la limite peut √™tre appropri√© :

```yaml
resources:
  requests:
    cpu: 250m     # Gardez les requests
  limits:
    # Pas de limite CPU
    memory: 512Mi # Gardez la limite m√©moire !
```

**Attention** : Le pod pourra utiliser plus de CPU si disponible, mais partagera √©quitablement avec les autres.

#### Solution 3 : Optimiser l'Application

- Utiliser des caches
- Optimiser les requ√™tes base de donn√©es
- Utiliser des connexions async/non-bloquantes
- Parall√©liser les t√¢ches
- Profiler et optimiser le code

#### Solution 4 : Augmenter le Nombre de R√©plicas

Au lieu d'un pod puissant, plusieurs pods moins puissants :

```yaml
# Avant
replicas: 1
resources:
  requests:
    cpu: 1000m

# Apr√®s
replicas: 4
resources:
  requests:
    cpu: 250m     # Charge r√©partie sur 4 pods
```

## Probl√®me 4 : Pods √âvict√©s (Evicted)

### Sympt√¥mes

Des pods sont expuls√©s du n≈ìud et apparaissent avec le statut `Evicted`.

```bash
microk8s kubectl get pods
# NAME          READY   STATUS    RESTARTS   AGE
# mon-pod-abc   0/1     Evicted   0          10m
```

### Diagnostic

#### √âtape 1 : Voir la Raison de l'√âviction

```bash
microk8s kubectl describe pod mon-pod-abc

# Chercher :
# Reason:  Evicted
# Message: The node was low on resource: memory
# ou
# Message: The node had condition: [DiskPressure]
```

**Raisons courantes** :
- **Memory Pressure** : M√©moire du n≈ìud faible
- **Disk Pressure** : Espace disque faible
- **PID Pressure** : Trop de processus

#### √âtape 2 : V√©rifier l'√âtat du N≈ìud

```bash
# √âtat du n≈ìud
microk8s kubectl describe node | grep -A 5 "Conditions:"

# Chercher :
# MemoryPressure   False
# DiskPressure     False
# PIDPressure      False
```

**Si une condition est True** : Le n≈ìud est sous pression.

#### √âtape 3 : V√©rifier les Ressources du N≈ìud

```bash
# Utilisation globale
microk8s kubectl top node

# Sur le n≈ìud directement
free -h    # M√©moire
df -h      # Disque
```

### Solutions Courantes

#### Solution 1 : Nettoyer les Pods √âvict√©s

Ils ne servent plus √† rien :

```bash
# Supprimer tous les pods √©vict√©s
microk8s kubectl get pods -A | grep Evicted | awk '{print $2 " -n " $1}' | xargs -L1 microk8s kubectl delete pod
```

#### Solution 2 : Lib√©rer de la M√©moire

```bash
# Identifier les pods gourmands
microk8s kubectl top pods --all-namespaces --sort-by=memory

# R√©duire l'√©chelle ou supprimer
microk8s kubectl scale deployment app-gourmande --replicas=1
```

#### Solution 3 : Lib√©rer de l'Espace Disque

```bash
# Nettoyer les images
microk8s ctr images rm $(microk8s ctr images ls -q)

# Nettoyer les conteneurs arr√™t√©s
microk8s crictl rmi --prune

# Nettoyer les logs
sudo find /var/snap/microk8s/common/var/log -type f -name "*.log" -mtime +7 -delete
```

#### Solution 4 : D√©finir des Requests/Limits Appropri√©es

Pour √©viter les √©victions futures :

```yaml
resources:
  requests:
    memory: 256Mi     # R√©aliste
  limits:
    memory: 512Mi     # Pas trop haut
```

#### Solution 5 : Augmenter les Ressources du N≈ìud

- Ajouter de la RAM
- Ajouter de l'espace disque
- Ajouter un nouveau n≈ìud

## Quality of Service (QoS) Classes

Kubernetes attribue automatiquement une QoS class √† chaque pod selon ses requests/limits.

### Les Trois Classes

#### 1. Guaranteed (Garanti)

**D√©finition** : Meilleure garantie, derniers √† √™tre √©vict√©s.

**Conditions** :
- Chaque conteneur a requests ET limits
- requests = limits pour CPU et m√©moire

**Exemple** :
```yaml
resources:
  requests:
    cpu: 500m
    memory: 512Mi
  limits:
    cpu: 500m      # = requests
    memory: 512Mi  # = requests
```

**Utilisation** : Applications critiques (bases de donn√©es, services essentiels)

#### 2. Burstable (Rafale)

**D√©finition** : Garantie partielle.

**Conditions** :
- Au moins un conteneur a requests OU limits
- requests ‚â† limits

**Exemple** :
```yaml
resources:
  requests:
    cpu: 250m
    memory: 256Mi
  limits:
    cpu: 1000m     # > requests
    memory: 1Gi    # > requests
```

**Utilisation** : La plupart des applications

#### 3. BestEffort (Meilleur Effort)

**D√©finition** : Aucune garantie, premiers √† √™tre √©vict√©s.

**Conditions** :
- Aucun requests ni limits d√©finis

**Exemple** :
```yaml
# Pas de resources du tout
containers:
- name: mon-app
  image: nginx
```

**Utilisation** : Jobs non critiques, t√¢ches batch

### Ordre d'√âviction

En cas de pression sur les ressources :

```
1. BestEffort   (√©vict√©s en premier)
2. Burstable    (√©vict√©s ensuite)
3. Guaranteed   (√©vict√©s en dernier recours)
```

### V√©rifier la QoS d'un Pod

```bash
microk8s kubectl get pod mon-pod -o jsonpath='{.status.qosClass}'
# Sortie : Burstable, Guaranteed, ou BestEffort
```

## Outils de Monitoring et Diagnostic

### 1. kubectl top

**Installation du Metrics Server** (si pas d√©j√† fait) :
```bash
microk8s enable metrics-server

# Attendre qu'il soit pr√™t
microk8s kubectl wait --for=condition=ready pod -n kube-system -l k8s-app=metrics-server --timeout=60s
```

**Utilisation** :

```bash
# Utilisation des n≈ìuds
microk8s kubectl top nodes

# Utilisation des pods
microk8s kubectl top pods

# Tous les namespaces
microk8s kubectl top pods --all-namespaces

# Trier par utilisation
microk8s kubectl top pods --sort-by=cpu
microk8s kubectl top pods --sort-by=memory

# Conteneurs individuels
microk8s kubectl top pods --containers
```

### 2. kubectl describe

```bash
# Vue compl√®te d'un n≈ìud
microk8s kubectl describe node

# Vue compl√®te d'un pod
microk8s kubectl describe pod mon-pod
```

**Sections importantes** :
- **Conditions** : √âtat du n≈ìud
- **Capacity/Allocatable** : Ressources disponibles
- **Allocated resources** : Ressources utilis√©es
- **Events** : √âvictions, OOM, etc.

### 3. Prometheus + Grafana

Pour un monitoring avanc√© :

```bash
# Activer Prometheus (si pas d√©j√† fait)
microk8s enable prometheus

# Acc√©der √† Grafana
microk8s kubectl port-forward -n prometheus service/prometheus-grafana 3000:80

# Ouvrir : http://localhost:3000
# Login par d√©faut : admin / prom-operator
```

**M√©triques disponibles** :
- Utilisation CPU/m√©moire dans le temps
- Historique des performances
- Alertes sur seuils
- Dashboards pr√©-configur√©s

### 4. Commandes Syst√®me (sur le N≈ìud)

```bash
# Vue g√©n√©rale
htop

# Utilisation CPU/m√©moire
top

# Statistiques I/O
iostat -x 5

# Processus qui consomment le plus
ps aux --sort=-%mem | head -10
ps aux --sort=-%cpu | head -10
```

## Optimisation des Ressources

### Strat√©gie 1 : Right-Sizing

**Processus** :

1. **Commencer conservateur** :
```yaml
resources:
  requests:
    cpu: 100m
    memory: 128Mi
  limits:
    cpu: 500m
    memory: 512Mi
```

2. **Surveiller pendant quelques jours** :
```bash
watch -n 60 microk8s kubectl top pod mon-pod
```

3. **Ajuster selon l'utilisation r√©elle** :

```yaml
# Si utilisation moyenne : CPU 250m, M√©moire 300Mi
resources:
  requests:
    cpu: 250m        # Bas√© sur moyenne
    memory: 300Mi
  limits:
    cpu: 500m        # 2x la moyenne pour les pics
    memory: 600Mi    # 2x la moyenne
```

### Strat√©gie 2 : Horizontal Pod Autoscaling (HPA)

**Principe** : Ajuster automatiquement le nombre de r√©plicas selon la charge.

**Configuration** :

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mon-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mon-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Scaler si CPU > 70%
```

**Cr√©er via commande** :
```bash
microk8s kubectl autoscale deployment mon-app --cpu-percent=70 --min=2 --max=10
```

**V√©rifier** :
```bash
microk8s kubectl get hpa
```

### Strat√©gie 3 : Vertical Pod Autoscaling (VPA)

**Principe** : Ajuster automatiquement les requests/limits selon l'utilisation.

**Installation** :
```bash
# VPA n'est pas inclus dans MicroK8s par d√©faut
# Installation manuelle n√©cessaire
```

**Configuration** :
```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: mon-app-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mon-app
  updatePolicy:
    updateMode: "Auto"  # ou "Initial", "Off"
```

### Strat√©gie 4 : Resource Quotas

**Principe** : Limiter les ressources totales d'un namespace.

**Configuration** :
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: development
spec:
  hard:
    requests.cpu: "4"       # Total requests CPU
    requests.memory: 8Gi    # Total requests m√©moire
    limits.cpu: "8"         # Total limits CPU
    limits.memory: 16Gi     # Total limits m√©moire
    pods: "20"              # Nombre max de pods
```

**Appliquer** :
```bash
microk8s kubectl apply -f quota.yaml

# V√©rifier
microk8s kubectl describe quota -n development
```

### Strat√©gie 5 : LimitRange

**Principe** : D√©finir des valeurs par d√©faut et des contraintes par pod.

**Configuration** :
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-range
  namespace: development
spec:
  limits:
  - max:
      cpu: "2"          # Un pod ne peut pas demander plus
      memory: 2Gi
    min:
      cpu: 100m         # Un pod doit demander au moins
      memory: 128Mi
    default:
      cpu: 500m         # Valeur par d√©faut si non sp√©cifi√©
      memory: 512Mi
    defaultRequest:
      cpu: 250m
      memory: 256Mi
    type: Container
```

**Avantage** : Les d√©veloppeurs n'ont plus besoin de sp√©cifier les ressources (valeurs par d√©faut appliqu√©es).

## Workflows de D√©pannage

### Workflow 1 : Pod Pending (Ressources Insuffisantes)

```bash
# 1. V√©rifier le probl√®me
microk8s kubectl describe pod mon-pod | grep -A 5 Events

# 2. Voir les ressources demand√©es
microk8s kubectl get pod mon-pod -o jsonpath='{.spec.containers[*].resources}'

# 3. Voir les ressources disponibles
microk8s kubectl describe node | grep -A 5 "Allocated resources"

# 4. Solutions possibles :

# Option A : R√©duire les requests
microk8s kubectl edit deployment mon-app
# Modifier resources.requests

# Option B : Lib√©rer des ressources
microk8s kubectl scale deployment app-non-critique --replicas=0

# Option C : Ajouter un n≈ìud
microk8s add-node
```

### Workflow 2 : OOMKilled

```bash
# 1. Confirmer le probl√®me
microk8s kubectl describe pod mon-pod | grep OOM

# 2. Voir la limite actuelle
microk8s kubectl get pod mon-pod -o jsonpath='{.spec.containers[*].resources.limits.memory}'

# 3. Voir l'utilisation (si pod en cours)
microk8s kubectl top pod mon-pod

# 4. Augmenter la limite
microk8s kubectl edit deployment mon-app
# Modifier resources.limits.memory

# 5. V√©rifier
microk8s kubectl rollout status deployment mon-app
microk8s kubectl get pods
```

### Workflow 3 : Performance Lente

```bash
# 1. V√©rifier l'utilisation CPU
microk8s kubectl top pod mon-pod

# 2. Comparer avec les limits
microk8s kubectl get pod mon-pod -o jsonpath='{.spec.containers[*].resources.limits.cpu}'

# 3. Si throttl√©, augmenter
microk8s kubectl edit deployment mon-app
# Augmenter resources.limits.cpu

# 4. Ou ajouter des r√©plicas
microk8s kubectl scale deployment mon-app --replicas=3

# 5. Surveiller
watch -n 5 microk8s kubectl top pods -l app=mon-app
```

### Workflow 4 : Optimisation Globale

```bash
# 1. Audit des ressources actuelles
microk8s kubectl top pods --all-namespaces

# 2. Identifier les pods sur/sous-dimensionn√©s
# Sur-dimensionn√©s : limits >> utilisation
# Sous-dimensionn√©s : utilisation proche des limits

# 3. Ajuster les configurations
# Pour chaque deployment probl√©matique :
microk8s kubectl edit deployment <nom>

# 4. Mettre en place HPA pour les apps variables
microk8s kubectl autoscale deployment mon-app --cpu-percent=70 --min=2 --max=10

# 5. D√©finir des quotas pour les namespaces
kubectl apply -f resource-quota.yaml
```

## Bonnes Pratiques

### 1. Toujours D√©finir Requests et Limits

‚ùå **Mauvais** :
```yaml
containers:
- name: mon-app
  image: nginx
  # Pas de resources !
```

‚úÖ **Bon** :
```yaml
containers:
- name: mon-app
  image: nginx
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi
```

### 2. Requests Bas√©s sur l'Utilisation R√©elle

- D√©marrer conservateur
- Surveiller l'utilisation r√©elle
- Ajuster apr√®s observation

### 3. Limits Appropri√©es

**Pour CPU** :
- Limits pas trop strictes (permet les burst)
- Ou pas de limite du tout si charge variable

**Pour M√©moire** :
- TOUJOURS d√©finir une limite
- 2-3x la moyenne d'utilisation
- Prot√®ge contre les fuites m√©moire

### 4. Utiliser QoS Guaranteed pour les Services Critiques

```yaml
# Base de donn√©es, cache, etc.
resources:
  requests:
    cpu: 1000m
    memory: 2Gi
  limits:
    cpu: 1000m      # = requests
    memory: 2Gi     # = requests
```

### 5. Impl√©menter HPA pour les Charges Variables

```bash
# Applications web, API, workers
microk8s kubectl autoscale deployment web-app --cpu-percent=70 --min=2 --max=10
```

### 6. Utiliser Resource Quotas par √âquipe/Projet

```yaml
# Namespace par √©quipe avec quotas
apiVersion: v1
kind: ResourceQuota
metadata:
  name: team-a-quota
  namespace: team-a
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi
```

### 7. Surveiller R√©guli√®rement

```bash
# Dashboard quotidien
microk8s kubectl top nodes
microk8s kubectl top pods --all-namespaces --sort-by=memory | head -20

# Alertes Prometheus/Grafana
# - CPU > 80%
# - M√©moire > 85%
# - Pods √©vict√©s
# - N≈ìuds sous pression
```

### 8. Documenter les D√©cisions

```yaml
# Commentaires dans les YAML
resources:
  requests:
    cpu: 250m      # Bas√© sur moyenne 200m + 25% marge (test√© sur 2 semaines)
    memory: 512Mi  # Utilisation max vue : 400Mi
  limits:
    cpu: 1000m     # Permet burst pour pics de trafic
    memory: 1Gi    # 2x requests pour s√©curit√©
```

## Checklist Performance et Ressources

### Pour Chaque Deployment

- [ ] Requests CPU d√©finis ?
- [ ] Requests m√©moire d√©finis ?
- [ ] Limits m√©moire d√©finis ? (OBLIGATOIRE)
- [ ] Limits CPU d√©finis ? (recommand√©)
- [ ] Requests bas√©s sur utilisation r√©elle ?
- [ ] QoS class appropri√©e ?
- [ ] HPA configur√© si charge variable ?
- [ ] Readiness/Liveness probes configur√©es ?

### Pour le Cluster

- [ ] Metrics Server activ√© ?
- [ ] Utilisation n≈ìud < 80% ?
- [ ] Quotas d√©finis par namespace ?
- [ ] LimitRange configur√© ?
- [ ] Monitoring en place (Prometheus/Grafana) ?
- [ ] Alertes configur√©es ?
- [ ] Plan de scaling document√© ?

### Diagnostics R√©guliers

- [ ] `kubectl top nodes` - N≈ìuds OK ?
- [ ] `kubectl top pods --all-namespaces` - Pods normaux ?
- [ ] `kubectl get pods -A | grep -E 'Evicted|OOMKilled'` - Probl√®mes ?
- [ ] `kubectl describe nodes | grep Pressure` - Pressions ?

## R√©sum√©

### Commandes Essentielles

```bash
# Monitoring
kubectl top nodes
kubectl top pods --all-namespaces
kubectl describe node

# Diagnostic
kubectl describe pod <pod>
kubectl get pod <pod> -o jsonpath='{.spec.containers[*].resources}'
kubectl get pod <pod> -o jsonpath='{.status.qosClass}'

# Actions
kubectl edit deployment <deploy>
kubectl scale deployment <deploy> --replicas=<n>
kubectl autoscale deployment <deploy> --cpu-percent=70 --min=2 --max=10

# Nettoyage
kubectl delete pod -A --field-selector status.phase=Failed
kubectl get pods -A | grep Evicted | awk '{print $2 " -n " $1}' | xargs -L1 kubectl delete pod
```

### Concepts Cl√©s

| Concept | D√©finition | Impact |
|---------|------------|--------|
| **Requests** | Ressources garanties | Utilis√© par le scheduler |
| **Limits** | Ressources maximales | CPU throttled, M√©moire OOMKilled |
| **QoS Guaranteed** | requests = limits | Derniers √©vict√©s |
| **QoS Burstable** | requests ‚â† limits | √âvict√©s moyennement |
| **QoS BestEffort** | Pas de resources | Premiers √©vict√©s |
| **Throttling** | Limitation CPU | Performance d√©grad√©e |
| **OOMKilled** | D√©passement m√©moire | Pod tu√© |
| **Evicted** | Expulsion | Ressources n≈ìud faibles |

### Valeurs Typiques

| Type d'Application | CPU Request | CPU Limit | Memory Request | Memory Limit |
|-------------------|-------------|-----------|----------------|--------------|
| Service web l√©ger | 100m | 500m | 128Mi | 512Mi |
| API backend | 250m | 1000m | 256Mi | 1Gi |
| Worker/Job | 500m | 2000m | 512Mi | 2Gi |
| Base de donn√©es | 1000m | 1000m | 1Gi | 1Gi |
| Cache (Redis) | 100m | 500m | 512Mi | 512Mi |

**Note** : Ces valeurs sont indicatives, ajustez selon vos besoins r√©els !

### Points Cl√©s √† Retenir

1. **TOUJOURS d√©finir resources** en production
2. **Requests** = ce dont vous avez besoin minimum
3. **Limits** = protection contre sur-utilisation
4. **M√©moire** : limite OBLIGATOIRE (risque OOMKilled)
5. **CPU** : limite optionnelle (risque throttling)
6. **Surveiller** pour ajuster les valeurs
7. **HPA** pour les charges variables
8. **QoS Guaranteed** pour les services critiques

**Prochaine √©tape** : Section 23.9 - Mise √† jour de MicroK8s

---


‚è≠Ô∏è [Mise √† jour de MicroK8s](/23-depannage-et-maintenance/09-mise-a-jour-de-microk8s.md)
