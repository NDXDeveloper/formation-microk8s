ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 18.6 Optimisation des ressources

## Introduction

Nous avons maintenant couvert tous les **mÃ©canismes** de gestion des ressources (18.1 Ã  18.5). Mais connaÃ®tre les outils ne suffit pas - il faut savoir les **utiliser efficacement**. C'est le but de cette section : apprendre Ã  **optimiser** les ressources de votre cluster.

L'optimisation des ressources consiste Ã  trouver le **juste Ã©quilibre** entre :
- ğŸŸ¢ Performance : Assez de ressources pour que les applications fonctionnent bien
- ğŸŸ¢ StabilitÃ© : Protection contre les pannes et surcharges
- ğŸŸ¢ CoÃ»t : Ne pas gaspiller de ressources inutilisÃ©es
- ğŸŸ¢ DensitÃ© : Maximiser le nombre de Pods par nÅ“ud

## Pourquoi optimiser ?

### Le problÃ¨me du gaspillage

Sans optimisation, voici ce qui se passe souvent :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Ressources configurÃ©es vs rÃ©elles      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Application A                                 â”‚
â”‚  â”œâ”€ Requests configurÃ©s : 2 CPU, 4Gi RAM       â”‚
â”‚  â””â”€ Usage rÃ©el moyen   : 0.3 CPU, 800Mi RAM    â”‚
â”‚     Gaspillage : 85% CPU, 80% RAM âŒ           â”‚
â”‚                                                â”‚
â”‚  Application B                                 â”‚
â”‚  â”œâ”€ Requests configurÃ©s : 1 CPU, 2Gi RAM       â”‚
â”‚  â””â”€ Usage rÃ©el moyen   : 0.1 CPU, 300Mi RAM    â”‚
â”‚     Gaspillage : 90% CPU, 85% RAM âŒ           â”‚
â”‚                                                â”‚
â”‚  RÃ©sultat :                                    â”‚
â”‚  â€¢ Cluster sous-utilisÃ©                        â”‚
â”‚  â€¢ CoÃ»ts Ã©levÃ©s                                â”‚
â”‚  â€¢ Faible densitÃ© de Pods                      â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Les bÃ©nÃ©fices de l'optimisation

âœ… **RÃ©duction des coÃ»ts** :
- Moins de serveurs nÃ©cessaires
- Meilleure utilisation de l'infrastructure existante
- ROI amÃ©liorÃ©

âœ… **Meilleure densitÃ©** :
- Plus de Pods par nÅ“ud
- Cluster plus efficient
- Moins de ressources gaspillÃ©es

âœ… **StabilitÃ© accrue** :
- Requests rÃ©alistes = meilleur scheduling
- Limits appropriÃ©es = moins d'OOM kills
- PrÃ©visibilitÃ© amÃ©liorÃ©e

âœ… **Performance optimale** :
- Ressources suffisantes quand nÃ©cessaire
- Pas de sur-allocation qui ralentit
- Ã‰quilibre performance/coÃ»t

## Analogie : La gestion d'un parking

Imaginez un parking d'immeuble :

**Sans optimisation** (requests trop Ã©levÃ©s) :
```
RÃ©servations :
â”œâ”€ Appartement 1 : 3 places (possÃ¨de 1 voiture)
â”œâ”€ Appartement 2 : 4 places (possÃ¨de 1 voiture)
â””â”€ Appartement 3 : 2 places (possÃ¨de 2 voitures)

RÃ©sultat :
â€¢ 9 places rÃ©servÃ©es
â€¢ 4 voitures rÃ©elles
â€¢ 5 places gaspillÃ©es (55%) âŒ
â€¢ Nouveaux locataires refusÃ©s alors qu'il y a de la place
```

**Avec optimisation** (right-sizing) :
```
RÃ©servations :
â”œâ”€ Appartement 1 : 1 place + 1 flex (possÃ¨de 1 voiture)
â”œâ”€ Appartement 2 : 1 place + 1 flex (possÃ¨de 1 voiture)
â””â”€ Appartement 3 : 2 places (possÃ¨de 2 voitures)

RÃ©sultat :
â€¢ 6 places rÃ©servÃ©es (4 fixes + 2 flex)
â€¢ 4 voitures rÃ©elles
â€¢ 3 places libres pour nouveaux locataires âœ…
â€¢ FlexibilitÃ© pour visites occasionnelles
```

Dans Kubernetes :
- **Places rÃ©servÃ©es** = Requests
- **Places flexibles** = DiffÃ©rence entre Requests et Limits
- **Voitures rÃ©elles** = Usage rÃ©el des ressources

## MÃ©thodologie d'optimisation

### Le cycle d'optimisation continue

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Cycle d'optimisation (itÃ©ratif)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  1ï¸âƒ£  MESURER                                    â”‚
â”‚      â””â”€ Collecter les mÃ©triques d'usage rÃ©el     â”‚
â”‚                                                  â”‚
â”‚  2ï¸âƒ£  ANALYSER                                   â”‚
â”‚      â””â”€ Comparer usage vs requests/limits        â”‚
â”‚                                                  â”‚
â”‚  3ï¸âƒ£  IDENTIFIER                                 â”‚
â”‚      â””â”€ Trouver les sur/sous-allocations         â”‚
â”‚                                                  â”‚
â”‚  4ï¸âƒ£  AJUSTER                                    â”‚
â”‚      â””â”€ Modifier les requests/limits             â”‚
â”‚                                                  â”‚
â”‚  5ï¸âƒ£  DÃ‰PLOYER                                   â”‚
â”‚      â””â”€ Appliquer les changements                â”‚
â”‚                                                  â”‚
â”‚  6ï¸âƒ£  VALIDER                                    â”‚
â”‚      â””â”€ VÃ©rifier que Ã§a fonctionne bien          â”‚
â”‚                                                  â”‚
â”‚  7ï¸âƒ£  RÃ‰PÃ‰TER â†º                                  â”‚
â”‚      â””â”€ Recommencer aprÃ¨s 2-4 semaines           â”‚
â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 1 : Mesurer l'usage rÃ©el

**Outils nÃ©cessaires** :
- Prometheus (mÃ©triques)
- Grafana (visualisation)
- kubectl top (quick check)

**MÃ©triques Ã  collecter** :

```yaml
# CPU
- container_cpu_usage_seconds_total
- Percentile 95 sur 7-14 jours
- Pics d'utilisation

# MÃ©moire
- container_memory_working_set_bytes
- Maximum observÃ© sur 7-14 jours
- Tendances de croissance
```

**DurÃ©e de mesure recommandÃ©e** :
- Minimum : **7 jours** (couvre un cycle hebdomadaire)
- RecommandÃ© : **14-30 jours** (couvre variations mensuelles)
- IdÃ©al : **60-90 jours** (patterns saisonniers)

### Phase 2 : Analyser les donnÃ©es

**Calculer les ratios** :

```
Ratio d'utilisation CPU = Usage P95 / Requests CPU
Ratio d'utilisation RAM = Usage Max / Requests Memory

Exemples :
â€¢ Usage P95: 300m, Requests: 500m â†’ Ratio: 60% âœ… (OK)
â€¢ Usage P95: 400m, Requests: 500m â†’ Ratio: 80% âœ… (Bon)
â€¢ Usage P95: 100m, Requests: 1000m â†’ Ratio: 10% âŒ (Sur-allouÃ©)
â€¢ Usage P95: 900m, Requests: 500m â†’ Ratio: 180% âš ï¸ (Sous-allouÃ©)
```

**CatÃ©gorisation** :

| Ratio d'utilisation | Statut | Action |
|---------------------|--------|--------|
| < 20% | ğŸ”´ Fortement sur-allouÃ© | RÃ©duire requests immÃ©diatement |
| 20-50% | ğŸŸ¡ Sur-allouÃ© | RÃ©duire requests prudemment |
| 50-70% | ğŸŸ¢ Bien dimensionnÃ© | Surveiller |
| 70-85% | ğŸŸ¢ Optimal | Parfait, maintenir |
| 85-95% | ğŸŸ¡ Sous-allouÃ© | Augmenter requests |
| > 95% | ğŸ”´ Critique | Augmenter requests urgence |

### Phase 3 : Identifier les opportunitÃ©s

**Questions Ã  se poser** :

1. **Quels Pods sont sur-allouÃ©s ?**
   - Usage rÃ©el << Requests
   - Candidates Ã  la rÃ©duction

2. **Quels Pods sont sous-allouÃ©s ?**
   - Usage rÃ©el >> Requests
   - Risque d'Ã©viction ou throttling

3. **Quels Pods sont instables ?**
   - OOMKilled frÃ©quents
   - CPU throttling Ã©levÃ©

4. **Quelle est la marge de sÃ©curitÃ© ?**
   - DiffÃ©rence entre usage P95 et P99
   - VariabilitÃ© de l'usage

### Phase 4 : Ajuster les ressources

**Formules de calcul** :

**Pour le CPU (requests)** :
```
Nouveau Request CPU = Usage P95 Ã— 1.2
                    = Usage P95 + 20% marge
```

**Pour le CPU (limits)** :
```
Option 1 (conservative) : Limit CPU = Request Ã— 2
Option 2 (permissive)   : Pas de limit CPU (burst libre)
```

**Pour la mÃ©moire (requests)** :
```
Nouveau Request Memory = Usage Max Ã— 1.15
                       = Usage Max + 15% marge
```

**Pour la mÃ©moire (limits)** :
```
Limit Memory = Request Ã— 1.5
             = Request + 50% marge
```

**Exemple de calcul** :

```yaml
# DonnÃ©es observÃ©es sur 14 jours
CPU Usage :
  - P50: 200m
  - P95: 350m
  - P99: 450m
  - Max: 500m

Memory Usage :
  - P50: 400Mi
  - P95: 600Mi
  - P99: 700Mi
  - Max: 750Mi

# Configuration actuelle (avant optimisation)
resources:
  requests:
    cpu: "1"        # 1000m
    memory: "2Gi"   # 2048Mi
  limits:
    cpu: "2"
    memory: "4Gi"

# Analyse
CPU : Usage P95 350m vs Request 1000m â†’ Ratio 35% (sur-allouÃ© âŒ)
RAM : Usage Max 750Mi vs Request 2Gi â†’ Ratio 36% (sur-allouÃ© âŒ)

# Nouvelle configuration optimisÃ©e
resources:
  requests:
    cpu: "420m"     # 350m Ã— 1.2 = 420m
    memory: "862Mi" # 750Mi Ã— 1.15 = 862Mi
  limits:
    cpu: "840m"     # 420m Ã— 2
    memory: "1293Mi"# 862Mi Ã— 1.5

# RÃ©sultat
CPU Ã©conomisÃ© : 580m (58% rÃ©duction)
RAM Ã©conomisÃ©e : 1186Mi (58% rÃ©duction)
```

## Right-Sizing : Les patterns d'optimisation

### Pattern 1 : Application web frontend

**CaractÃ©ristiques** :
- Usage CPU variable (pics pendant la journÃ©e)
- MÃ©moire stable
- Peut gÃ©rer du throttling CPU

**Configuration optimale** :

```yaml
# Avant optimisation (trop gÃ©nÃ©reux)
resources:
  requests:
    memory: "512Mi"
    cpu: "500m"
  limits:
    memory: "1Gi"
    cpu: "1"

# AprÃ¨s observation : P95 = 150m CPU, Max = 320Mi RAM

# AprÃ¨s optimisation
resources:
  requests:
    memory: "368Mi"  # 320Mi Ã— 1.15
    cpu: "180m"      # 150m Ã— 1.2
  limits:
    memory: "552Mi"  # 368Mi Ã— 1.5
    # Pas de limit CPU - peut burst si besoin
```

**Ã‰conomie** : 64% CPU, 28% RAM

### Pattern 2 : API backend

**CaractÃ©ristiques** :
- Usage CPU prÃ©visible
- MÃ©moire peut croÃ®tre (caches)
- Performance critique

**Configuration optimale** :

```yaml
# AprÃ¨s observation : P95 = 400m CPU, Max = 800Mi RAM

resources:
  requests:
    memory: "920Mi"  # 800Mi Ã— 1.15
    cpu: "480m"      # 400m Ã— 1.2
  limits:
    memory: "1380Mi" # 920Mi Ã— 1.5
    cpu: "960m"      # 480m Ã— 2 (permet bursts)
```

### Pattern 3 : Base de donnÃ©es

**CaractÃ©ristiques** :
- Usage mÃ©moire Ã©levÃ© et stable
- CPU peut pic lors des requÃªtes
- Critique - nÃ©cessite Guaranteed QoS

**Configuration optimale** :

```yaml
# AprÃ¨s observation : P95 = 1.5 CPU, Max = 3.8Gi RAM

resources:
  limits:  # Forme courte - Guaranteed
    memory: "4.37Gi"  # 3.8Gi Ã— 1.15
    cpu: "1800m"      # 1.5 Ã— 1.2
  # requests = limits automatiquement
```

### Pattern 4 : Worker batch

**CaractÃ©ristiques** :
- Usage trÃ¨s variable
- Bursts importants pendant le traitement
- Peut Ãªtre interrompu

**Configuration optimale** :

```yaml
# AprÃ¨s observation : P95 = 600m CPU, Max = 1.2Gi RAM
# Mais bursts jusqu'Ã  3 CPU possibles

resources:
  requests:
    memory: "1.38Gi"  # 1.2Gi Ã— 1.15
    cpu: "720m"       # 600m Ã— 1.2
  limits:
    memory: "2.76Gi"  # 2Ã— pour les pics
    cpu: "3"          # Permet bursts complets
```

### Pattern 5 : Service stateless lÃ©ger

**CaractÃ©ristiques** :
- Usage minimal et constant
- Nombreux replicas
- Optimisation = forte densitÃ©

**Configuration optimale** :

```yaml
# AprÃ¨s observation : P95 = 50m CPU, Max = 80Mi RAM

resources:
  requests:
    memory: "92Mi"   # 80Mi Ã— 1.15
    cpu: "60m"       # 50m Ã— 1.2
  limits:
    memory: "138Mi"  # 92Mi Ã— 1.5
    cpu: "120m"      # 60m Ã— 2
```

**BÃ©nÃ©fice** : 100+ Pods par nÅ“ud possible

## StratÃ©gies d'optimisation

### StratÃ©gie 1 : Optimisation progressive

âœ… **Approche sÃ»re** pour la production

```
Phase 1 : Non-production (Dev/Staging)
â”œâ”€ Appliquer l'optimisation
â”œâ”€ Tester pendant 1 semaine
â””â”€ Valider stabilitÃ©

Phase 2 : Production - Canary
â”œâ”€ 10% des Pods optimisÃ©s
â”œâ”€ Observer pendant 48h
â””â”€ Valider mÃ©triques

Phase 3 : Production - Progressive
â”œâ”€ 25% des Pods
â”œâ”€ 50% des Pods
â”œâ”€ 75% des Pods
â””â”€ 100% des Pods (si tout va bien)
```

### StratÃ©gie 2 : Par criticitÃ©

**Ordre d'optimisation** :

```
1ï¸âƒ£  Services non-critiques
    â””â”€ Impact faible si problÃ¨me

2ï¸âƒ£  Services internes
    â””â”€ Utilisateurs internes tolÃ©rants

3ï¸âƒ£  Services clients (non-transactionnels)
    â””â”€ Tableau de bord, analytics

4ï¸âƒ£  Services clients critiques
    â””â”€ APIs principales

5ï¸âƒ£  Services transactionnels
    â””â”€ Paiements, commandes
    â””â”€ Attention maximale
```

### StratÃ©gie 3 : Quick wins d'abord

**Prioriser par impact** :

```yaml
# Score d'optimisation = Ã‰conomie Ã— FacilitÃ©

Pod A :
  - Requests actuels : 4 CPU, 8Gi RAM
  - Usage rÃ©el : 0.5 CPU, 1Gi RAM
  - Ã‰conomie potentielle : 3.5 CPU, 7Gi RAM
  - FacilitÃ© : Facile (service simple)
  - Score : â­â­â­â­â­ (PRIORITAIRE)

Pod B :
  - Requests actuels : 0.5 CPU, 512Mi RAM
  - Usage rÃ©el : 0.4 CPU, 400Mi RAM
  - Ã‰conomie potentielle : 0.1 CPU, 112Mi RAM
  - FacilitÃ© : Facile
  - Score : â­ (Faible prioritÃ©)

Pod C :
  - Requests actuels : 2 CPU, 4Gi RAM
  - Usage rÃ©el : 0.3 CPU, 800Mi RAM
  - Ã‰conomie potentielle : 1.7 CPU, 3.2Gi RAM
  - FacilitÃ© : Difficile (base de donnÃ©es stateful)
  - Score : â­â­ (Attention requise)
```

**Commencer par les Pods avec score â­â­â­â­â­**

## Over-provisioning vs Under-provisioning

### Le dilemme

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Over vs Under Provisioning             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Over-provisioning (requests trop Ã©levÃ©s)      â”‚
â”‚  âŒ Gaspillage de ressources                   â”‚
â”‚  âŒ CoÃ»ts Ã©levÃ©s                               â”‚
â”‚  âŒ Faible densitÃ©                             â”‚
â”‚  âœ… Haute stabilitÃ©                            â”‚
â”‚  âœ… Bonnes performances                        â”‚
â”‚                                                â”‚
â”‚  Under-provisioning (requests trop bas)        â”‚
â”‚  âœ… Utilisation maximale                       â”‚
â”‚  âœ… CoÃ»ts rÃ©duits                              â”‚
â”‚  âœ… Haute densitÃ©                              â”‚
â”‚  âŒ Risque d'instabilitÃ©                       â”‚
â”‚  âŒ Ã‰victions possibles                        â”‚
â”‚  âŒ Performance dÃ©gradÃ©e                       â”‚
â”‚                                                â”‚
â”‚  Sweet Spot (Ã©quilibre optimal)                â”‚
â”‚  âœ… Bonne utilisation                          â”‚
â”‚  âœ… CoÃ»ts contrÃ´lÃ©s                            â”‚
â”‚  âœ… Bonne densitÃ©                              â”‚
â”‚  âœ… StabilitÃ© maintenue                        â”‚
â”‚  âœ… Performances acceptables                   â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Trouver le sweet spot

**RÃ¨gle gÃ©nÃ©rale** :

```
Requests = Usage P95 + marge de sÃ©curitÃ©

Marge de sÃ©curitÃ© recommandÃ©e :
â€¢ CPU : +20% (moins critique, compressible)
â€¢ RAM : +15% (plus critique, non-compressible)
```

**Adapter selon le contexte** :

```yaml
# Service critique (prioritÃ© stabilitÃ©)
Marge CPU : +30-40%
Marge RAM : +25-30%

# Service standard (Ã©quilibre)
Marge CPU : +20%
Marge RAM : +15%

# Service non-critique (prioritÃ© densitÃ©)
Marge CPU : +10-15%
Marge RAM : +10%
```

### Signes d'under-provisioning

âš ï¸ **Indicateurs Ã  surveiller** :

```yaml
CPU Under-provisioning :
  - Throttling Ã©levÃ© (> 25% du temps)
  - Latence accrue
  - Timeouts frÃ©quents
  - Usage constant proche de 100%

MÃ©moire Under-provisioning :
  - OOMKilled rÃ©pÃ©tÃ©s
  - Pods redÃ©marrÃ©s frÃ©quemment
  - Ã‰victions par memory pressure
  - Usage constamment > 90%
```

**Action** : Augmenter les requests immÃ©diatement

### Signes d'over-provisioning

âœ… **Indicateurs** :

```yaml
CPU Over-provisioning :
  - Usage moyen < 30% des requests
  - Usage P95 < 50% des requests
  - Pas de pics significatifs

MÃ©moire Over-provisioning :
  - Usage max < 50% des requests
  - MÃ©moire stable et basse
  - Pas de croissance tendancielle
```

**Action** : RÃ©duire les requests progressivement

## Outils d'optimisation

### 1. kubectl top - Quick check

**Usage basique** :

```bash
# Usage actuel des Pods
kubectl top pods -n production

# Tri par mÃ©moire
kubectl top pods -n production --sort-by=memory

# Tri par CPU
kubectl top pods -n production --sort-by=cpu

# Tous les namespaces
kubectl top pods --all-namespaces
```

**Limitations** :
- Snapshot instantanÃ© (pas d'historique)
- DonnÃ©es rÃ©centes uniquement
- Pas de percentiles

### 2. Prometheus queries

**RequÃªtes utiles** :

```promql
# CPU P95 sur 7 jours
quantile(0.95,
  rate(container_cpu_usage_seconds_total[7d])
) by (pod, container)

# MÃ©moire maximum sur 7 jours
max_over_time(
  container_memory_working_set_bytes[7d]
) by (pod, container)

# Ratio d'utilisation CPU
rate(container_cpu_usage_seconds_total[5m])
/ on(pod, container) group_left
kube_pod_container_resource_requests{resource="cpu"}

# Throttling CPU
rate(container_cpu_cfs_throttled_seconds_total[5m])
```

### 3. Goldilocks - Recommandations automatiques

**Goldilocks** est un outil qui gÃ©nÃ¨re des recommandations de requests/limits automatiquement.

**Installation** :

```bash
# Avec Helm
helm repo add fairwinds-stable https://charts.fairwinds.com/stable
helm install goldilocks fairwinds-stable/goldilocks \
  --namespace goldilocks --create-namespace
```

**Utilisation** :

```bash
# Activer pour un namespace
kubectl label namespace production goldilocks.fairwinds.com/enabled=true

# AccÃ©der au dashboard
kubectl -n goldilocks port-forward svc/goldilocks-dashboard 8080:80
# Ouvrir http://localhost:8080
```

**RÃ©sultat** : Recommandations basÃ©es sur VPA (Vertical Pod Autoscaler)

### 4. Kubecost - Analyse financiÃ¨re

**Kubecost** fournit des insights sur les coÃ»ts et l'utilisation.

**Installation** :

```bash
# Avec Helm
helm repo add kubecost https://kubecost.github.io/cost-analyzer/
helm install kubecost kubecost/cost-analyzer \
  --namespace kubecost --create-namespace
```

**MÃ©triques fournies** :
- CoÃ»t par namespace
- CoÃ»t par Pod
- Recommandations d'optimisation
- Idle resources (ressources gaspillÃ©es)

### 5. kube-resource-report

**GÃ©nÃ¨re des rapports HTML** sur l'utilisation des ressources.

```bash
# Installation
pip install kube-resource-report

# GÃ©nÃ©ration du rapport
kube-resource-report --output-dir ./reports
```

**Contenu** :
- Ratio utilisation/requests
- Pods sur-allouÃ©s
- Pods sous-allouÃ©s
- Recommandations

### 6. Scripts personnalisÃ©s

**Script d'analyse simple** :

```bash
#!/bin/bash
# analyze-resources.sh

echo "=== Analyse des ressources ==="

for namespace in $(kubectl get ns -o jsonpath='{.items[*].metadata.name}'); do
  echo ""
  echo "Namespace: $namespace"

  # Requests totaux
  requests_cpu=$(kubectl get pods -n $namespace -o json | \
    jq '[.items[].spec.containers[].resources.requests.cpu // "0"] |
        map(gsub("m";"") | tonumber) | add')

  requests_mem=$(kubectl get pods -n $namespace -o json | \
    jq '[.items[].spec.containers[].resources.requests.memory // "0"] |
        map(gsub("Mi";"") | gsub("Gi";"000") | tonumber) | add')

  echo "  Total CPU requests: ${requests_cpu}m"
  echo "  Total Memory requests: ${requests_mem}Mi"
done
```

## Dashboard Grafana pour l'optimisation

### Panels recommandÃ©s

**Panel 1 : Ratio d'utilisation par Pod** :

```promql
# CPU
sum(rate(container_cpu_usage_seconds_total[5m])) by (pod)
/ on(pod) group_left
sum(kube_pod_container_resource_requests{resource="cpu"}) by (pod)
Ã— 100
```

**Panel 2 : Top 10 Pods sur-allouÃ©s** :

```promql
# DiffÃ©rence entre requests et usage rÃ©el
(
  sum(kube_pod_container_resource_requests{resource="cpu"}) by (pod)
  -
  sum(rate(container_cpu_usage_seconds_total[5m])) by (pod)
)
> 0.5  # Plus de 500m gaspillÃ©s
```

**Panel 3 : Ã‰conomies potentielles** :

```promql
# CPU gaspillÃ© total
sum(
  kube_pod_container_resource_requests{resource="cpu"}
  -
  rate(container_cpu_usage_seconds_total[5m])
)
```

**Panel 4 : Pods Ã  risque (sous-allouÃ©s)** :

```promql
# Usage > 90% des requests
sum(rate(container_cpu_usage_seconds_total[5m])) by (pod)
/ on(pod) group_left
sum(kube_pod_container_resource_requests{resource="cpu"}) by (pod)
> 0.9
```

### Dashboard complet exemple

```json
{
  "dashboard": {
    "title": "Resource Optimization",
    "panels": [
      {
        "title": "Cluster Efficiency",
        "targets": [
          {
            "expr": "sum(rate(container_cpu_usage_seconds_total[5m])) / sum(kube_node_status_capacity{resource='cpu'}) * 100"
          }
        ]
      },
      {
        "title": "Over-provisioned Pods (CPU)",
        "targets": [
          {
            "expr": "topk(10, sum(kube_pod_container_resource_requests{resource='cpu'} - rate(container_cpu_usage_seconds_total[5m])) by (pod))"
          }
        ]
      },
      {
        "title": "Under-provisioned Pods (Memory)",
        "targets": [
          {
            "expr": "sum(container_memory_working_set_bytes / kube_pod_container_resource_requests{resource='memory'}) by (pod) > 0.9"
          }
        ]
      }
    ]
  }
}
```

## Optimisation par type de charge

### Workload prÃ©visible

**Exemples** : API stable, service web constant

**StratÃ©gie** :
```yaml
# Requests serrÃ©s (usage P95 + 15%)
# Limits modÃ©rÃ©s (2Ã— requests)
resources:
  requests:
    cpu: "500m"      # BasÃ© sur P95
    memory: "512Mi"
  limits:
    cpu: "1"         # 2Ã— pour pics occasionnels
    memory: "768Mi"  # 1.5Ã— pour sÃ©curitÃ©
```

### Workload variable

**Exemples** : E-commerce (pics journÃ©e), streaming

**StratÃ©gie** :
```yaml
# Requests basÃ©s sur charge moyenne haute
# Limits gÃ©nÃ©reux pour absorber les pics
resources:
  requests:
    cpu: "300m"      # Charge moyenne-haute
    memory: "512Mi"
  limits:
    cpu: "2"         # Absorbe pics (6Ã— requests)
    memory: "1Gi"    # 2Ã— pour pics
```

### Workload par batch

**Exemples** : Jobs ETL, traitement de donnÃ©es

**StratÃ©gie** :
```yaml
# Requests modÃ©rÃ©s
# Limits Ã©levÃ©s (bursts importants OK)
resources:
  requests:
    cpu: "500m"      # Garantie minimale
    memory: "1Gi"
  limits:
    cpu: "4"         # Peut utiliser beaucoup pendant traitement
    memory: "4Gi"    # DonnÃ©es en mÃ©moire
```

### Workload critique

**Exemples** : Bases de donnÃ©es, caches

**StratÃ©gie** :
```yaml
# Guaranteed QoS
# Requests = Limits (aucun burst)
resources:
  limits:
    cpu: "2"         # Stable et prÃ©visible
    memory: "4Gi"    # Pas de surprise
  # requests = limits automatiquement
```

## Bonnes pratiques d'optimisation

### 1. Ne jamais optimiser aveuglÃ©ment

âŒ **Mauvais** :
```yaml
# RÃ©duire de 50% partout sans analyse
resources:
  requests:
    cpu: "250m"    # Avant: 500m
    memory: "256Mi" # Avant: 512Mi
```

âœ… **Bon** :
```yaml
# BasÃ© sur 14 jours d'observations
# P95 CPU = 180m, Max RAM = 220Mi
resources:
  requests:
    cpu: "216m"    # 180m Ã— 1.2
    memory: "253Mi" # 220Mi Ã— 1.15
```

### 2. Optimiser namespace par namespace

âœ… **Approche structurÃ©e** :

```
Semaine 1 : Namespace "dev" (non-critique)
Semaine 2 : Namespace "staging"
Semaine 3 : Namespace "production-internal"
Semaine 4 : Namespace "production-customer"
```

### 3. Conserver un historique

âœ… **Documenter les changements** :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-backend
  annotations:
    optimization-history: |
      2025-10-01: Initial deployment - 1 CPU, 2Gi RAM
      2025-10-15: Reduced to 500m CPU, 1Gi RAM (usage analysis)
      2025-11-01: Increased to 600m CPU, 1Gi RAM (latency issues)
spec:
  template:
    spec:
      containers:
      - name: api
        resources:
          requests:
            cpu: "600m"
            memory: "1Gi"
```

### 4. Alertes sur les changements

âœ… **Monitoring post-optimisation** :

```yaml
# Alerte Prometheus
- alert: HighCPUAfterOptimization
  expr: |
    rate(container_cpu_usage_seconds_total[5m])
    / on(pod) group_left
    kube_pod_container_resource_requests{resource="cpu"}
    > 0.95
  for: 15m
  annotations:
    summary: "Pod {{ $labels.pod }} using >95% CPU after optimization"
```

### 5. Rollback plan

âœ… **Toujours prÃ©voir un rollback** :

```bash
# Avant optimisation - sauvegarder la config actuelle
kubectl get deployment api-backend -o yaml > backup-api-backend-pre-optim.yaml

# DÃ©ployer l'optimisation
kubectl apply -f api-backend-optimized.yaml

# Si problÃ¨me - rollback immÃ©diat
kubectl apply -f backup-api-backend-pre-optim.yaml
```

### 6. Tests de charge post-optimisation

âœ… **Valider avec des tests** :

```bash
# Test de charge avec k6
k6 run --vus 100 --duration 10m load-test.js

# Observer pendant le test
kubectl top pods -n production --sort-by=cpu
watch -n 2 'kubectl get hpa -n production'

# VÃ©rifier les mÃ©triques
# - Latence : doit rester acceptable
# - Error rate : pas d'augmentation
# - CPU/RAM : dans les limites
```

### 7. Optimisation diffÃ©renciÃ©e par environnement

âœ… **Adapter selon l'environnement** :

```yaml
# Development - agressif
resources:
  requests:
    cpu: "100m"    # Minimal
    memory: "256Mi"

# Staging - modÃ©rÃ©
resources:
  requests:
    cpu: "300m"    # BasÃ© sur usage
    memory: "512Mi"

# Production - conservateur
resources:
  requests:
    cpu: "600m"    # Usage P95 + 30%
    memory: "1Gi"  # Usage Max + 25%
```

## Automatisation de l'optimisation

### Vertical Pod Autoscaler (VPA)

**VPA** ajuste automatiquement les requests/limits.

**Installation** :

```bash
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler
./hack/vpa-up.sh
```

**Utilisation** :

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-backend-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-backend
  updatePolicy:
    updateMode: "Auto"  # Auto, Recreate, Initial, ou Off
  resourcePolicy:
    containerPolicies:
    - containerName: api
      minAllowed:
        cpu: "100m"
        memory: "128Mi"
      maxAllowed:
        cpu: "2"
        memory: "4Gi"
```

**Modes** :
- `Off` : Recommandations seulement (pas d'action)
- `Initial` : Applique Ã  la crÃ©ation du Pod
- `Recreate` : RecrÃ©e les Pods pour appliquer
- `Auto` : Applique en direct (Ã©viction + recrÃ©ation)

âš ï¸ **Attention** : VPA et HPA sont incompatibles sur les mÃªmes mÃ©triques.

### Script de recommandations pÃ©riodique

```bash
#!/bin/bash
# weekly-optimization-check.sh

echo "=== Weekly Optimization Report ==="
date

NAMESPACES="production staging"

for ns in $NAMESPACES; do
  echo ""
  echo "Namespace: $ns"

  # Pods with low utilization (<30%)
  echo "  Over-provisioned Pods (CPU <30%):"
  kubectl top pods -n $ns --no-headers | awk '{
    if ($2 != "0m") {
      # Parse CPU usage (remove 'm')
      cpu = substr($2, 1, length($2)-1)
      # Get requests
      cmd = "kubectl get pod "$1" -n '"$ns"' -o jsonpath='\''{.spec.containers[0].resources.requests.cpu}'\''";
      cmd | getline requests
      close(cmd)
      # Parse requests
      if (requests ~ /m$/) {
        req = substr(requests, 1, length(requests)-1)
      } else {
        req = requests * 1000
      }
      # Calculate ratio
      if (req > 0) {
        ratio = (cpu / req) * 100
        if (ratio < 30) {
          printf "    - %s: %.0f%% utilization\n", $1, ratio
        }
      }
    }
  }'

  # Pods with high utilization (>85%)
  echo "  Under-provisioned Pods (CPU >85%):"
  kubectl top pods -n $ns --no-headers | awk '{
    if ($2 != "0m") {
      cpu = substr($2, 1, length($2)-1)
      cmd = "kubectl get pod "$1" -n '"$ns"' -o jsonpath='\''{.spec.containers[0].resources.requests.cpu}'\''";
      cmd | getline requests
      close(cmd)
      if (requests ~ /m$/) {
        req = substr(requests, 1, length(requests)-1)
      } else {
        req = requests * 1000
      }
      if (req > 0) {
        ratio = (cpu / req) * 100
        if (ratio > 85) {
          printf "    - %s: %.0f%% utilization âš ï¸\n", $1, ratio
        }
      }
    }
  }'
done

echo ""
echo "=== Run this script weekly for continuous optimization ==="
```

**Automatiser avec CronJob** :

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: weekly-optimization-report
  namespace: monitoring
spec:
  schedule: "0 9 * * 1"  # Tous les lundis Ã  9h
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: resource-analyzer
          containers:
          - name: analyzer
            image: bitnami/kubectl:latest
            command:
            - /bin/bash
            - -c
            - |
              # Script d'analyse ici
              echo "Optimization report generated"
          restartPolicy: OnFailure
```

## Cas pratiques d'optimisation

### Cas 1 : Startup en croissance

**Contexte** :
- Cluster de 10 nÅ“uds
- Budget serrÃ©
- Croissance rapide

**Objectif** : Maximiser la densitÃ© sans sacrifier la stabilitÃ©

**Actions** :
1. Analyse complÃ¨te sur 30 jours
2. Optimisation agressive (marge 10-15%)
3. Monitoring intensif post-optimisation
4. Gain : 30% de capacitÃ© supplÃ©mentaire

**RÃ©sultat** :
```
Avant :
- 200 Pods sur 10 nÅ“uds
- Utilisation moyenne : 45%
- CoÃ»t mensuel : $5000

AprÃ¨s :
- 280 Pods sur 10 nÅ“uds
- Utilisation moyenne : 65%
- CoÃ»t mensuel : $5000
- Ã‰conomie effective : $1600 (capacitÃ© supplÃ©mentaire)
```

### Cas 2 : Entreprise Ã©tablie

**Contexte** :
- Cluster de 100 nÅ“uds
- StabilitÃ© primordiale
- Budget confortable

**Objectif** : Optimiser sans risque

**Actions** :
1. Analyse sur 90 jours (patterns saisonniers)
2. Optimisation conservative (marge 25-30%)
3. Rollout ultra-progressif (1% par jour)
4. Tests de charge Ã  chaque Ã©tape

**RÃ©sultat** :
```
Avant :
- 2000 Pods sur 100 nÅ“uds
- Utilisation moyenne : 30%
- CoÃ»t mensuel : $80000

AprÃ¨s :
- 2000 Pods sur 75 nÅ“uds
- Utilisation moyenne : 55%
- CoÃ»t mensuel : $60000
- Ã‰conomie : $20000/mois ($240000/an)
```

### Cas 3 : Application variable (e-commerce)

**Contexte** :
- Pics Black Friday, NoÃ«l
- Charge normale Ã— 10 pendant les pics
- HPA configurÃ©

**Objectif** : Optimiser pour charge normale, burst pour pics

**Actions** :
1. Analyse hors-pÃ©riodes de pic
2. Requests basÃ©s sur charge normale
3. Limits gÃ©nÃ©reux pour les pics
4. HPA pour scaling horizontal

**Configuration finale** :
```yaml
resources:
  requests:
    cpu: "300m"      # Charge normale
    memory: "512Mi"
  limits:
    cpu: "2"         # Absorbe pics Ã— 6
    memory: "2Gi"    # Ã— 4
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: frontend-hpa
spec:
  minReplicas: 10    # Normal
  maxReplicas: 100   # Pics
  targetCPUUtilizationPercentage: 70
```

**RÃ©sultat** :
- Charges normales : OptimisÃ© (Ã©conomie 40%)
- Pics : Scale horizontal + burst CPU/RAM
- CoÃ»t moyen rÃ©duit de 35%

## Points clÃ©s Ã  retenir

ğŸ”‘ **Les essentiels** :

1. **Mesurer avant d'optimiser** : 7-30 jours de donnÃ©es minimum
2. **Analyser les percentiles** : P95 pour CPU, Max pour RAM
3. **Ajouter des marges** : +20% CPU, +15% RAM (minimum)
4. **Optimiser progressivement** : Non-critique â†’ Critique
5. **Monitoring intensif** : Post-optimisation pendant 2 semaines
6. **Automatiser** : Scripts, VPA, dashboards
7. **Documenter** : Historique des changements et justifications
8. **Adapter** : Par environnement et type de workload

## Checklist d'optimisation

âœ… **Avant de commencer** :
- [ ] Prometheus + Grafana installÃ©s et fonctionnels
- [ ] MÃ©triques collectÃ©es depuis > 7 jours
- [ ] Dashboard d'analyse crÃ©Ã©
- [ ] Backup des configurations actuelles

âœ… **Analyse** :
- [ ] Usage P95 CPU calculÃ© par Pod
- [ ] Usage Max RAM calculÃ© par Pod
- [ ] Ratios d'utilisation documentÃ©s
- [ ] Pods sur-allouÃ©s identifiÃ©s (>20% gaspillage)
- [ ] Pods sous-allouÃ©s identifiÃ©s (>85% usage)

âœ… **Planification** :
- [ ] Ordre d'optimisation dÃ©fini
- [ ] Nouvelles valeurs calculÃ©es (formules appliquÃ©es)
- [ ] Fichiers YAML prÃ©parÃ©s
- [ ] Rollback plan documentÃ©

âœ… **DÃ©ploiement** :
- [ ] Tests en non-production d'abord
- [ ] Rollout progressif (10% â†’ 25% â†’ 50% â†’ 100%)
- [ ] Monitoring actif pendant le rollout
- [ ] Alertes configurÃ©es

âœ… **Validation** :
- [ ] Aucune OOMKilled aprÃ¨s 48h
- [ ] Pas de dÃ©gradation de performance
- [ ] Utilisation dans les ratios cibles (50-85%)
- [ ] Tests de charge passÃ©s

âœ… **Documentation** :
- [ ] Changements documentÃ©s (annotations)
- [ ] Ã‰conomies calculÃ©es et reportÃ©es
- [ ] LeÃ§ons apprises notÃ©es
- [ ] Prochaine revue planifiÃ©e (3 mois)

## Conclusion

L'optimisation des ressources est un **processus continu**, pas une action ponctuelle. C'est l'Ã©quilibre entre performance, stabilitÃ© et coÃ»t.

**Principes directeurs** :
- ğŸ“Š **Mesurer** : DonnÃ©es rÃ©elles > Suppositions
- ğŸ¯ **Cibler** : Quick wins d'abord, critique en dernier
- ğŸŒ **Progresser** : Lent et sÃ»r > Rapide et risquÃ©
- ğŸ” **ItÃ©rer** : Optimiser â†’ Mesurer â†’ Ajuster â†’ RÃ©pÃ©ter

**BÃ©nÃ©fices** :
- ğŸ’° RÃ©duction des coÃ»ts (20-50% typique)
- ğŸ“ˆ Meilleure densitÃ© de Pods
- ğŸ¯ StabilitÃ© maintenue ou amÃ©liorÃ©e
- ğŸš€ Performances optimales

Avec les outils et mÃ©thodologies de ce chapitre, vous Ãªtes maintenant Ã©quipÃ© pour optimiser votre cluster Kubernetes de maniÃ¨re professionnelle et efficace.

**Prochaine Ã©tape** : Dans la section suivante (18.7), nous verrons les **Best Practices** qui consolident tout ce que nous avons appris dans ce chapitre.

---


â­ï¸ [Best practices](/18-gestion-des-ressources/07-best-practices.md)
