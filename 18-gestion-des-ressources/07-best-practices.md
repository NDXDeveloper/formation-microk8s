üîù Retour au [Sommaire](/SOMMAIRE.md)

# 18.7 Best Practices

## Introduction

Nous voici √† la derni√®re section du chapitre 18 sur la gestion des ressources. Nous avons couvert √©norm√©ment de terrain :
- **18.1** - Requests et Limits (fondamentaux)
- **18.2** - Resource Quotas (budget namespace)
- **18.3** - LimitRanges (contraintes individuelles)
- **18.4** - QoS Classes (priorit√© automatique)
- **18.5** - PriorityClasses (priorit√© explicite)
- **18.6** - Optimisation des ressources (efficacit√©)

Cette section **consolide** tout ce que nous avons appris en un ensemble de **best practices** actionnables, organis√©es et faciles √† suivre. C'est votre guide de r√©f√©rence pour une gestion professionnelle des ressources Kubernetes.

## R√©capitulatif : Les 6 piliers de la gestion des ressources

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Les 6 piliers de la gestion des ressources      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                         ‚îÇ
‚îÇ  1Ô∏è‚É£  D√âFINIR                                           ‚îÇ
‚îÇ      Requests et Limits sur chaque conteneur            ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  2Ô∏è‚É£  LIMITER                                           ‚îÇ
‚îÇ      Resource Quotas par namespace                      ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  3Ô∏è‚É£  CONTRAINDRE                                       ‚îÇ
‚îÇ      LimitRanges pour valeurs min/max/default           ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  4Ô∏è‚É£  PROT√âGER                                          ‚îÇ
‚îÇ      QoS Classes (automatique) + PriorityClasses        ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  5Ô∏è‚É£  OPTIMISER                                         ‚îÇ
‚îÇ      Right-sizing bas√© sur m√©triques r√©elles            ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  6Ô∏è‚É£  MONITORER                                         ‚îÇ
‚îÇ      Surveillance continue et ajustements               ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Best Practices par th√®me

### üéØ Configuration des Requests et Limits

#### BP 1.1 : Toujours d√©finir des Requests

‚úÖ **FAIRE** :
```yaml
# Toujours d√©finir requests, m√™me approximatifs
resources:
  requests:
    memory: "256Mi"
    cpu: "200m"
  limits:
    memory: "512Mi"
    cpu: "400m"
```

‚ùå **NE PAS FAIRE** :
```yaml
# Jamais de Pod sans resources en production
resources: {}
# ou pire : pas de section resources du tout
```

**Raison** : Sans requests, le scheduler ne peut pas prendre de d√©cisions √©clair√©es.

#### BP 1.2 : Toujours d√©finir des Limits m√©moire

‚úÖ **FAIRE** :
```yaml
resources:
  requests:
    memory: "512Mi"
  limits:
    memory: "1Gi"    # TOUJOURS pr√©sent
```

‚ùå **NE PAS FAIRE** :
```yaml
resources:
  requests:
    memory: "512Mi"
  # Pas de limit m√©moire = risque de fuite m√©moire
```

**Raison** : Sans limit m√©moire, une fuite peut consommer toute la RAM du n≈ìud.

#### BP 1.3 : Limits CPU optionnels mais r√©fl√©chis

‚úÖ **OPTION A - Avec limits** (pr√©visible) :
```yaml
resources:
  requests:
    cpu: "500m"
  limits:
    cpu: "1"    # Pr√©visible mais peut throttle
```

‚úÖ **OPTION B - Sans limits** (flexible) :
```yaml
resources:
  requests:
    cpu: "500m"
  # Pas de limit CPU = peut burst si disponible
```

**Choix** :
- **Avec limits** : Applications critiques n√©cessitant performance pr√©visible
- **Sans limits** : Applications pouvant b√©n√©ficier de bursts occasionnels

#### BP 1.4 : Ratio Requests/Limits raisonnable

‚úÖ **BON** :
```yaml
resources:
  requests:
    memory: "512Mi"
    cpu: "500m"
  limits:
    memory: "1Gi"     # Ratio 2:1 ‚úÖ
    cpu: "1"          # Ratio 2:1 ‚úÖ
```

‚ùå **MAUVAIS** :
```yaml
resources:
  requests:
    memory: "128Mi"
    cpu: "100m"
  limits:
    memory: "4Gi"     # Ratio 32:1 ‚ùå
    cpu: "4"          # Ratio 40:1 ‚ùå
```

**R√®gle** : Ratio maximum recommand√© = 3:1 (jusqu'√† 5:1 pour certains cas)

#### BP 1.5 : Requests = Limits pour applications critiques

‚úÖ **FAIRE** (Guaranteed QoS) :
```yaml
# Base de donn√©es, cache, infrastructure critique
resources:
  limits:
    memory: "2Gi"
    cpu: "1"
  # requests = limits automatiquement
```

**Raison** : QoS Guaranteed = derniers √† √™tre √©vinc√©s

### üèõÔ∏è Gouvernance avec Quotas et LimitRanges

#### BP 2.1 : Un Resource Quota par namespace

‚úÖ **FAIRE** :
```yaml
# Chaque namespace a son quota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: development
spec:
  hard:
    requests.cpu: "10"
    requests.memory: "20Gi"
    pods: "50"
```

**Raison** : Protection contre la sur-consommation et √©quit√© entre √©quipes

#### BP 2.2 : Un LimitRange par namespace

‚úÖ **FAIRE** :
```yaml
# Chaque namespace a ses contraintes
apiVersion: v1
kind: LimitRange
metadata:
  name: dev-limits
  namespace: development
spec:
  limits:
  - type: Container
    max:
      cpu: "2"
      memory: "4Gi"
    defaultRequest:
      cpu: "100m"
      memory: "128Mi"
```

**Raison** : Emp√™che qu'un Pod monopolise le quota + fournit des defaults

#### BP 2.3 : Defaults en dev, pas en prod

‚úÖ **Development** :
```yaml
# Defaults = facilite le travail
spec:
  limits:
  - type: Container
    default:
      cpu: "500m"
      memory: "512Mi"
```

‚úÖ **Production** :
```yaml
# Pas de defaults = configuration consciente
spec:
  limits:
  - type: Container
    max:
      cpu: "4"
      memory: "8Gi"
    # Pas de section default
```

**Raison** : En prod, les ressources doivent √™tre explicitement d√©finies

#### BP 2.4 : Documenter les quotas

‚úÖ **FAIRE** :
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: team-a-quota
  namespace: team-a
  annotations:
    contact: "team-a-lead@company.com"
    last-review: "2025-10-01"
    justification: "Bas√© sur usage moyen + 30% croissance"
spec:
  hard:
    requests.cpu: "20"
    requests.memory: "40Gi"
```

#### BP 2.5 : R√©vision trimestrielle

‚úÖ **Processus** :
```
Tous les 3 mois :
1. Analyser l'utilisation r√©elle vs quotas
2. Identifier les namespaces proche de la limite
3. Ajuster si justifi√©
4. Documenter les changements
```

### üéØ Priorit√©s et Protection

#### BP 3.1 : Hi√©rarchie claire de PriorityClasses

‚úÖ **FAIRE** (5-7 niveaux max) :
```yaml
critical:  10000  # Infrastructure, paiements
high:      5000   # APIs client
medium:    2000   # Services internes
default:   1000   # Par d√©faut (globalDefault)
low:       500    # Batch non urgent
very-low:  100    # Maintenance
```

‚ùå **NE PAS FAIRE** (trop de niveaux) :
```yaml
# 15+ niveaux = confusion inutile
ultra-mega-critical: 25000
super-critical: 22000
very-critical: 20000
...
```

#### BP 3.2 : GlobalDefault d√©fini

‚úÖ **FAIRE** :
```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: default-priority
value: 1000
globalDefault: true  # Un seul dans le cluster
```

**Raison** : √âvite d'avoir des Pods √† priorit√© 0 par d√©faut

#### BP 3.3 : Pr√©emption avec pr√©caution

‚úÖ **Pattern recommand√©** :
```yaml
# Haute priorit√© : pr√©emption activ√©e
value: 10000
preemptionPolicy: PreemptLowerPriority

# Priorit√© moyenne : pas de pr√©emption
value: 2000
preemptionPolicy: Never
```

**R√®gle** : Plus la priorit√© est haute, plus on autorise la pr√©emption

#### BP 3.4 : Combiner QoS Guaranteed + Haute PriorityClass

‚úÖ **Protection maximale** :
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: critical-database
spec:
  priorityClassName: critical-priority  # 10000
  containers:
  - name: postgres
    resources:
      limits:
        memory: "4Gi"
        cpu: "2"
      # QoS: Guaranteed
```

**R√©sultat** : Dernier Pod √† √™tre √©vinc√©, toutes conditions confondues

### üìä Monitoring et Observabilit√©

#### BP 4.1 : Dashboards essentiels

‚úÖ **Dashboards Grafana minimum** :
1. **Utilisation globale** : CPU/RAM cluster
2. **Utilisation par namespace** : Breakdown d√©taill√©
3. **Ratio usage/requests** : Identifier sur-allocation
4. **Top 10 consommateurs** : Pods gourmands
5. **QoS et Priority distribution** : Vue d'ensemble
6. **√âvictions et OOMKills** : Indicateurs de probl√®mes

#### BP 4.2 : Alertes critiques

‚úÖ **Alertes Prometheus recommand√©es** :

```yaml
# 1. Utilisation √©lev√©e vs requests
- alert: HighResourceUsageVsRequests
  expr: |
    container_memory_working_set_bytes
    / on(pod) kube_pod_container_resource_requests{resource="memory"}
    > 0.9
  for: 15m

# 2. OOMKills r√©p√©t√©s
- alert: FrequentOOMKills
  expr: |
    increase(kube_pod_container_status_restarts_total{reason="OOMKilled"}[1h]) > 3

# 3. Quota proche de la limite
- alert: QuotaNearLimit
  expr: |
    kube_resourcequota{type="used"}
    / on(resource, namespace) kube_resourcequota{type="hard"}
    > 0.85

# 4. Pods sans resources en production
- alert: PodsWithoutResourcesInProd
  expr: |
    count(kube_pod_container_resource_requests{namespace="production"} == 0) > 0
```

#### BP 4.3 : R√©vision hebdomadaire

‚úÖ **Routine recommand√©e** :

```
Tous les lundis matin :
1. V√©rifier les alertes de la semaine pass√©e
2. Analyser les OOMKills et √©victions
3. Identifier les Pods probl√©matiques
4. Planifier les actions correctives
5. V√©rifier la tendance d'utilisation
```

#### BP 4.4 : M√©triques cl√©s √† suivre

‚úÖ **KPIs essentiels** :
```yaml
Cluster :
  - Taux d'utilisation CPU global (cible: 60-75%)
  - Taux d'utilisation RAM global (cible: 70-80%)
  - Nombre de n≈ìuds
  - Co√ªt par Pod

Namespace :
  - Utilisation vs quota (ne pas d√©passer 85%)
  - Nombre de Pods
  - Distribution QoS Classes

Pod :
  - Ratio usage/requests (cible: 60-80%)
  - Nombre de restarts (OOMKills)
  - CPU throttling (< 25%)
```

### üöÄ Optimisation Continue

#### BP 5.1 : Cycle d'optimisation r√©gulier

‚úÖ **Planning recommand√©** :

```
Semaine 1-2 : Collecte de donn√©es (14 jours minimum)
Semaine 3   : Analyse et identification
Semaine 4   : Calculs et pr√©paration
Semaine 5   : Tests en staging
Semaine 6   : Rollout production (10% ‚Üí 50% ‚Üí 100%)
Semaine 7-8 : Validation et monitoring
```

R√©p√©ter tous les **3 mois**

#### BP 5.2 : Analyse bas√©e sur percentiles

‚úÖ **FAIRE** :
```yaml
# Utiliser P95 pour CPU (pas la moyenne)
CPU Requests = P95 usage √ó 1.2

# Utiliser Max pour m√©moire (pas la moyenne)
Memory Requests = Max usage √ó 1.15
```

‚ùå **NE PAS FAIRE** :
```yaml
# Moyenne = sous-estime les pics
CPU Requests = Avg usage √ó 1.2  # ‚ùå
Memory Requests = Avg usage √ó 1.15  # ‚ùå
```

#### BP 5.3 : Optimiser par vagues

‚úÖ **Ordre recommand√©** :
```
1. Non-production (dev, staging)
2. Services non-critiques
3. Services internes
4. Services clients (non-transactionnels)
5. Services critiques (avec extr√™me pr√©caution)
```

#### BP 5.4 : Toujours garder un rollback

‚úÖ **FAIRE** :
```bash
# Backup avant changement
kubectl get deployment api-backend -o yaml > backup-pre-optim.yaml

# Appliquer optimisation
kubectl apply -f api-backend-optimized.yaml

# Si probl√®me
kubectl apply -f backup-pre-optim.yaml
```

#### BP 5.5 : Tests de charge post-optimisation

‚úÖ **Valider avec tests** :
```bash
# Test de charge
k6 run --vus 100 --duration 10m load-test.js

# Pendant le test
kubectl top pods -w
kubectl get hpa -w

# V√©rifier
# - Latence acceptable
# - Pas d'errors
# - Ressources dans les limites
# - Pas d'OOMKills
```

### üèóÔ∏è Organisation et Gouvernance

#### BP 6.1 : Ownership clair

‚úÖ **D√©finir les responsabilit√©s** :
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: team-frontend
  annotations:
    owner: "frontend-team@company.com"
    budget: "$2000/month"
    review-frequency: "monthly"
```

#### BP 6.2 : Process d'approbation

‚úÖ **Pour ressources importantes** :
```yaml
# Haute priorit√© n√©cessite approbation
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: ultra-high
  annotations:
    approval-required: "yes"
    approved-by: "CTO"
    approval-date: "2025-10-15"
value: 10000
```

#### BP 6.3 : Documentation centralis√©e

‚úÖ **Wiki/Confluence/Notion** :
```markdown
# Guide des ressources Kubernetes

## Requests et Limits recommand√©s
- Frontend: 100m CPU, 128Mi RAM
- API: 500m CPU, 512Mi RAM
- Base de donn√©es: 2 CPU, 4Gi RAM

## PriorityClasses disponibles
- critical (10000): Infrastructure core
- high (5000): Services clients
- default (1000): Services standard

## Process de demande d'augmentation de quota
1. Remplir le formulaire [lien]
2. Justifier avec m√©triques
3. Approbation platform team
4. Mise √† jour sous 48h
```

#### BP 6.4 : Labels et annotations standards

‚úÖ **Convention de nommage** :
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-backend
  labels:
    app: api-backend
    team: backend-team
    env: production
    cost-center: engineering
  annotations:
    resource-profile: "medium"
    last-optimized: "2025-10-01"
    optimization-target: "cost"
```

### üîí S√©curit√© et Conformit√©

#### BP 7.1 : Pas de BestEffort en production

‚úÖ **Forcer avec LimitRange** :
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: no-besteffort
  namespace: production
spec:
  limits:
  - type: Container
    defaultRequest:
      memory: "128Mi"
      cpu: "100m"
```

**R√©sultat** : Tout Pod sans resources aura au minimum ces requests

#### BP 7.2 : Network Policies + Resource Quotas

‚úÖ **S√©curit√© en profondeur** :
```yaml
# Limiter les ressources
kind: ResourceQuota
# +
# Limiter le r√©seau
kind: NetworkPolicy
```

**Principe** : Defense in depth

#### BP 7.3 : PodDisruptionBudgets pour services critiques

‚úÖ **FAIRE** :
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-pdb
spec:
  minAvailable: 2  # Toujours 2 Pods minimum
  selector:
    matchLabels:
      app: api-backend
```

**+ Haute PriorityClass + Guaranteed QoS** = Triple protection

#### BP 7.4 : Audit r√©gulier

‚úÖ **Checklist trimestrielle** :
```
‚ñ° Pods sans resources en production ? (doit √™tre 0)
‚ñ° Quotas respect√©s dans tous les namespaces ?
‚ñ° LimitRanges d√©finis partout ?
‚ñ° PriorityClasses utilis√©es correctement ?
‚ñ° Pas de valeurs aberrantes (>10 CPU, >64Gi RAM) ?
‚ñ° Documentation √† jour ?
```

## Anti-patterns √† √©viter

### ‚ùå Anti-pattern 1 : "√áa marche en dev, ship en prod"

```yaml
# ‚ùå MAUVAIS
# Configuration dev pouss√©e en prod sans ajustement
resources:
  requests:
    cpu: "50m"     # Trop faible pour prod
    memory: "64Mi"
```

‚úÖ **CORRECT** :
```yaml
# Adapter les resources par environnement
# dev/config.yaml
resources:
  requests:
    cpu: "50m"
    memory: "64Mi"

# prod/config.yaml
resources:
  requests:
    cpu: "500m"
    memory: "512Mi"
```

### ‚ùå Anti-pattern 2 : "Pas s√ªr, je mets 10 CPU"

```yaml
# ‚ùå MAUVAIS
# Sur-allocation par pr√©caution
resources:
  requests:
    cpu: "10"      # Gaspillage √©norme
    memory: "32Gi"
```

‚úÖ **CORRECT** :
```yaml
# Bas√© sur des m√©triques r√©elles
# Usage observ√© P95 = 600m, Max = 1.2Gi
resources:
  requests:
    cpu: "720m"    # 600m √ó 1.2
    memory: "1.38Gi"  # 1.2Gi √ó 1.15
```

### ‚ùå Anti-pattern 3 : "Je copie-colle la config du voisin"

```yaml
# ‚ùå MAUVAIS
# Copier sans comprendre
resources:
  requests:
    cpu: "2"       # Pour une base de donn√©es
    memory: "8Gi"
# Alors que c'est un frontend l√©ger !
```

‚úÖ **CORRECT** :
```yaml
# Adapter au workload r√©el
resources:
  requests:
    cpu: "200m"    # Frontend l√©ger
    memory: "256Mi"
```

### ‚ùå Anti-pattern 4 : "Set and forget"

```yaml
# ‚ùå MAUVAIS
# Configuration jamais r√©vis√©e depuis 2 ans
resources:
  requests:
    cpu: "4"       # Surdimensionn√©
    memory: "16Gi"
# Usage r√©el = 10%
```

‚úÖ **CORRECT** :
```
Routine d'optimisation tous les 3 mois
‚Üí Analyse
‚Üí Ajustement
‚Üí Validation
```

### ‚ùå Anti-pattern 5 : "Requests = Limits partout"

```yaml
# ‚ùå MAUVAIS (sauf si vraiment critique)
# Guaranteed pour tout = gaspillage
resources:
  limits:
    cpu: "1"
    memory: "2Gi"
# Pour un service non-critique
```

‚úÖ **CORRECT** :
```yaml
# Guaranteed uniquement pour critique
# Burstable pour le reste
resources:
  requests:
    cpu: "500m"
    memory: "1Gi"
  limits:
    cpu: "2"       # Peut burst
    memory: "2Gi"
```

### ‚ùå Anti-pattern 6 : "Pas de monitoring, on verra bien"

```yaml
# ‚ùå MAUVAIS
# D√©ployer sans monitoring
kubectl apply -f deployment.yaml
# Puis oublier
```

‚úÖ **CORRECT** :
```yaml
# Monitoring + alertes + dashboards
# Surveillance continue
# Ajustements r√©guliers
```

### ‚ùå Anti-pattern 7 : "Pas besoin de quotas, on se fait confiance"

```yaml
# ‚ùå MAUVAIS
# Pas de Resource Quotas
# R√©sultat : Un namespace monopolise tout
```

‚úÖ **CORRECT** :
```yaml
# Quotas syst√©matiques
# Protection collective
# √âquit√© garantie
```

## Configurations recommand√©es par type d'application

### üì± Application Frontend (React, Vue, Angular)

```yaml
# Caract√©ristiques : L√©ger, stateless, nombreux replicas
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    spec:
      priorityClassName: medium-priority
      containers:
      - name: nginx
        image: nginx:alpine
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            # Pas de limit CPU (peut burst)
        # QoS: Burstable
```

### üîå API Backend (REST/GraphQL)

```yaml
# Caract√©ristiques : Performance importante, usage variable
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-backend
spec:
  replicas: 3
  template:
    spec:
      priorityClassName: high-priority
      containers:
      - name: api
        image: my-api:v1.0
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1"
        # QoS: Burstable
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
spec:
  minReplicas: 3
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70
```

### üóÑÔ∏è Base de donn√©es (PostgreSQL, MySQL, MongoDB)

```yaml
# Caract√©ristiques : Critique, usage stable, stateful
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgresql
spec:
  replicas: 1
  template:
    spec:
      priorityClassName: critical-priority
      containers:
      - name: postgres
        image: postgres:15
        resources:
          limits:
            memory: "4Gi"
            cpu: "2"
          # requests = limits automatiquement
          # QoS: Guaranteed
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: postgres-pdb
spec:
  maxUnavailable: 0  # Aucune disruption accept√©e
  selector:
    matchLabels:
      app: postgresql
```

### ‚ö° Cache (Redis, Memcached)

```yaml
# Caract√©ristiques : Critique, m√©moire importante, lecture intensive
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
spec:
  replicas: 1
  template:
    spec:
      priorityClassName: critical-priority
      containers:
      - name: redis
        image: redis:7-alpine
        resources:
          limits:
            memory: "2Gi"
            cpu: "1"
          # QoS: Guaranteed
        # Pas de HPA (stateful cache)
```

### üîÑ Worker de traitement

```yaml
# Caract√©ristiques : Bursts importants, peut √™tre interrompu
apiVersion: apps/v1
kind: Deployment
metadata:
  name: worker
spec:
  replicas: 10
  template:
    spec:
      priorityClassName: low-priority
      containers:
      - name: worker
        image: my-worker:v1.0
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "2"         # Bursts importants OK
        # QoS: Burstable
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: worker-hpa
spec:
  minReplicas: 5
  maxReplicas: 50
  targetCPUUtilizationPercentage: 75
```

### ‚è∞ CronJob de maintenance

```yaml
# Caract√©ristiques : P√©riodique, non-critique, peut prendre du temps
apiVersion: batch/v1
kind: CronJob
metadata:
  name: nightly-cleanup
spec:
  schedule: "0 2 * * *"  # 2h du matin
  jobTemplate:
    spec:
      template:
        spec:
          priorityClassName: very-low-priority
          containers:
          - name: cleanup
            image: cleanup-tool:v1
            resources:
              requests:
                memory: "512Mi"
                cpu: "500m"
              limits:
                memory: "2Gi"
                cpu: "2"
          restartPolicy: OnFailure
```

## Configuration par environnement

### üîß Environnement Development

```yaml
# Philosophie : Ressources minimales, haute densit√©, √©conomie

# Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: development
---
# Resource Quota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: development
spec:
  hard:
    requests.cpu: "10"
    requests.memory: "20Gi"
    limits.cpu: "20"
    limits.memory: "40Gi"
    pods: "50"
---
# LimitRange
apiVersion: v1
kind: LimitRange
metadata:
  name: dev-limits
  namespace: development
spec:
  limits:
  - type: Container
    max:
      cpu: "1"
      memory: "2Gi"
    default:
      cpu: "200m"       # Defaults g√©n√©reux
      memory: "256Mi"
    defaultRequest:
      cpu: "100m"
      memory: "128Mi"
---
# PriorityClass
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: dev-priority
value: 500
globalDefault: true  # Pour ce namespace
```

### üß™ Environnement Staging

```yaml
# Philosophie : Similaire √† prod mais ressources r√©duites

# Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: staging
---
# Resource Quota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: staging-quota
  namespace: staging
spec:
  hard:
    requests.cpu: "30"
    requests.memory: "60Gi"
    limits.cpu: "60"
    limits.memory: "120Gi"
    pods: "100"
---
# LimitRange
apiVersion: v1
kind: LimitRange
metadata:
  name: staging-limits
  namespace: staging
spec:
  limits:
  - type: Container
    max:
      cpu: "2"
      memory: "4Gi"
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "250m"
      memory: "256Mi"
---
# PriorityClass
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: staging-priority
value: 2000
```

### üöÄ Environnement Production

```yaml
# Philosophie : Stabilit√© maximale, ressources g√©n√©reuses, monitoring intense

# Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: production
---
# Resource Quota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: prod-quota
  namespace: production
spec:
  hard:
    requests.cpu: "100"
    requests.memory: "200Gi"
    limits.cpu: "200"
    limits.memory: "400Gi"
    pods: "500"
---
# LimitRange
apiVersion: v1
kind: LimitRange
metadata:
  name: prod-limits
  namespace: production
spec:
  limits:
  - type: Container
    max:
      cpu: "8"
      memory: "16Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    # PAS de defaults en prod
  - type: Pod
    max:
      cpu: "16"
      memory: "32Gi"
---
# PriorityClasses multiples
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: prod-critical
value: 10000
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: prod-high
value: 5000
globalDefault: true  # Pour ce namespace
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: prod-medium
value: 2000
```

## Checklist de mise en production

### ‚úÖ Avant le premier d√©ploiement

**Infrastructure** :
- [ ] Prometheus + Grafana install√©s
- [ ] Dashboards de monitoring configur√©s
- [ ] Alertes critiques configur√©es
- [ ] Metrics Server actif (`kubectl top` fonctionne)

**Gouvernance** :
- [ ] Resource Quotas d√©finis par namespace
- [ ] LimitRanges d√©finis par namespace
- [ ] PriorityClasses cr√©√©es
- [ ] GlobalDefault PriorityClass d√©finie
- [ ] Documentation publi√©e et accessible

**Configuration** :
- [ ] Tous les Pods ont requests/limits
- [ ] Pas de Pod BestEffort en production
- [ ] PriorityClasses assign√©es correctement
- [ ] PodDisruptionBudgets pour services critiques
- [ ] Network Policies d√©finies

### ‚úÖ Revue hebdomadaire

- [ ] Analyser les alertes de la semaine
- [ ] V√©rifier les OOMKills et √©victions
- [ ] Examiner l'utilisation vs quotas
- [ ] Identifier les Pods probl√©matiques
- [ ] V√©rifier les tendances d'utilisation

### ‚úÖ Revue mensuelle

- [ ] Analyser les co√ªts par namespace
- [ ] V√©rifier les ratios utilisation/requests
- [ ] Identifier les opportunit√©s d'optimisation
- [ ] R√©viser les quotas (augmentation si n√©cessaire)
- [ ] Mettre √† jour la documentation

### ‚úÖ Revue trimestrielle

- [ ] Cycle d'optimisation complet (18.6)
- [ ] R√©vision des PriorityClasses
- [ ] Audit de s√©curit√© (resources sans limits)
- [ ] Tests de charge sur services critiques
- [ ] Planification capacit√© pour 6 mois

### ‚úÖ Revue annuelle

- [ ] R√©√©valuation de la strat√©gie globale
- [ ] Benchmark vs best practices industrie
- [ ] Formation √©quipes sur nouveaut√©s
- [ ] R√©vision de la documentation compl√®te
- [ ] Planning d'am√©lioration pour l'ann√©e

## M√©triques de succ√®s

### üìä KPIs √† suivre

**Efficacit√©** :
```yaml
Taux d'utilisation cluster :
  - Objectif : 60-75% CPU, 70-80% RAM
  - Calcul : (Usage total) / (Capacit√© totale)

Ratio usage/requests :
  - Objectif : 60-80% par Pod
  - Calcul : (Usage r√©el) / (Requests configur√©s)

Densit√© de Pods :
  - Objectif : Maximiser sans d√©grader stabilit√©
  - Calcul : Nombre de Pods / Nombre de n≈ìuds
```

**Stabilit√©** :
```yaml
Taux d'OOMKills :
  - Objectif : < 0.1% par jour
  - Calcul : (OOMKills) / (Total Pods) √ó 100

Taux d'√©victions :
  - Objectif : < 0.5% par jour
  - Calcul : (√âvictions) / (Total Pods) √ó 100

Taux de restarts :
  - Objectif : < 1% par jour
  - Calcul : (Restarts) / (Total Pods) √ó 100
```

**Co√ªt** :
```yaml
Co√ªt par Pod :
  - Objectif : R√©duction de 20-30% apr√®s optimisation
  - Calcul : (Co√ªt infrastructure) / (Nombre de Pods)

Ressources gaspill√©es :
  - Objectif : < 20%
  - Calcul : (Requests - Usage r√©el) / Requests √ó 100

ROI optimisation :
  - Objectif : Positif dans les 3 mois
  - Calcul : √âconomies - Co√ªt temps ing√©nieur
```

### üéØ Objectifs par maturit√©

**Niveau 1 - D√©butant** (0-6 mois) :
```
‚úì Tous les Pods ont requests/limits
‚úì Resource Quotas en place
‚úì Monitoring basique (kubectl top)
‚úì Pas de BestEffort en production
Objectif utilisation : 40-50%
```

**Niveau 2 - Interm√©diaire** (6-12 mois) :
```
‚úì Niveau 1 +
‚úì LimitRanges configur√©s
‚úì PriorityClasses utilis√©es
‚úì Prometheus + Grafana op√©rationnels
‚úì Alertes configur√©es
‚úì Premi√®re optimisation r√©alis√©e
Objectif utilisation : 55-65%
```

**Niveau 3 - Avanc√©** (12-24 mois) :
```
‚úì Niveau 2 +
‚úì Optimisation continue (trimestrielle)
‚úì Dashboards personnalis√©s
‚úì Automation (VPA ou scripts)
‚úì Tests de charge r√©guliers
‚úì Documentation compl√®te
Objectif utilisation : 65-75%
```

**Niveau 4 - Expert** (24+ mois) :
```
‚úì Niveau 3 +
‚úì FinOps int√©gr√©
‚úì Pr√©diction de capacit√©
‚úì Optimisation multi-cluster
‚úì Co√ªt par feature/service
‚úì Continuous optimization pipeline
Objectif utilisation : 70-80%
```

## Outils et ressources

### üõ†Ô∏è Outils essentiels

**Monitoring** :
- ‚úÖ Prometheus (m√©triques)
- ‚úÖ Grafana (visualisation)
- ‚úÖ kube-state-metrics (m√©triques Kubernetes)
- ‚úÖ Metrics Server (kubectl top)

**Optimisation** :
- ‚úÖ Goldilocks (recommandations VPA)
- ‚úÖ Kubecost (analyse co√ªts)
- ‚úÖ kube-resource-report (rapports HTML)
- ‚úÖ Vertical Pod Autoscaler (automatisation)

**Gouvernance** :
- ‚úÖ OPA/Gatekeeper (policies as code)
- ‚úÖ Kyverno (policies Kubernetes-native)
- ‚úÖ Polaris (best practices audit)

### üìö Documentation de r√©f√©rence

**Officielle** :
- [Kubernetes Resource Management](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)
- [Quality of Service for Pods](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)
- [Resource Quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas/)
- [Limit Ranges](https://kubernetes.io/docs/concepts/policy/limit-range/)

**Communaut√©** :
- [Kubernetes Best Practices](https://www.oreilly.com/library/view/kubernetes-best-practices/9781492056461/)
- [CNCF Resource Management SIG](https://github.com/kubernetes/community/tree/master/sig-scheduling)

## Points cl√©s √† retenir

üîë **Les 10 commandements de la gestion des ressources** :

1. **Tu d√©finiras** requests et limits sur tous les conteneurs
2. **Tu prot√©geras** avec Resource Quotas et LimitRanges
3. **Tu prioriseras** avec QoS et PriorityClasses appropri√©es
4. **Tu monitoreras** en continu avec alertes
5. **Tu optimiseras** r√©guli√®rement (tous les 3 mois)
6. **Tu documenteras** toutes les d√©cisions
7. **Tu testeras** apr√®s chaque optimisation
8. **Tu adapteras** selon l'environnement
9. **Tu auditeras** r√©guli√®rement la conformit√©
10. **Tu am√©lioreras** continuellement le processus

## Conclusion du chapitre 18

F√©licitations ! üéâ Vous avez termin√© le chapitre complet sur la **Gestion des Ressources** dans Kubernetes. Vous ma√Ætrisez maintenant :

### Ce que vous savez faire

‚úÖ **Configurer correctement** :
- Requests et Limits adapt√©s √† chaque workload
- Resource Quotas pour gouverner les namespaces
- LimitRanges pour contraindre et fournir des defaults
- PriorityClasses pour g√©rer les priorit√©s m√©tier

‚úÖ **Comprendre en profondeur** :
- Comment Kubernetes schedule les Pods
- L'ordre d'√©viction (QoS + Priority)
- L'impact des ressources sur la stabilit√©
- Le co√ªt r√©el de l'infrastructure

‚úÖ **Optimiser professionnellement** :
- Analyser les m√©triques d'utilisation
- Calculer les ressources optimales
- D√©ployer les changements en s√©curit√©
- Mesurer l'impact et l'am√©lioration

‚úÖ **Maintenir durablement** :
- Monitoring continu et alertes
- Cycles d'optimisation r√©guliers
- Documentation et gouvernance
- Formation et am√©lioration continue

### L'impact sur votre cluster

Avec ces pratiques, vous pouvez vous attendre √† :

üìà **Am√©lioration de 20-50%** de l'utilisation des ressources
üí∞ **R√©duction de 20-40%** des co√ªts d'infrastructure
üéØ **Augmentation de 30-60%** de la densit√© de Pods
üõ°Ô∏è **Stabilit√© accrue** avec moins d'OOMKills et d'√©victions
üöÄ **Performance optimale** pour toutes les applications

### La suite de votre parcours

La gestion des ressources est un **voyage continu**, pas une destination. Continuez √† :
- üìä Monitorer vos m√©triques
- üîÑ Optimiser r√©guli√®rement
- üìñ Vous former sur les nouveaut√©s
- ü§ù Partager vos connaissances avec votre √©quipe

**Vous √™tes maintenant un expert en gestion des ressources Kubernetes !** üéì

Dans le prochain chapitre (19), nous aborderons le **Scaling et Autoscaling** - comment faire √©voluer automatiquement vos applications en fonction de la charge. Les fondations que vous avez acquises ici seront essentielles pour comprendre comment Kubernetes scale efficacement.

---


‚è≠Ô∏è [Scaling et Autoscaling](/19-scaling-et-autoscaling/README.md)
