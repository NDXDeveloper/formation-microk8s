üîù Retour au [Sommaire](/SOMMAIRE.md)

# 18.2 Resource Quotas

## Introduction

Dans le chapitre pr√©c√©dent (18.1), nous avons appris √† g√©rer les ressources **au niveau des Pods individuels** avec les Requests et Limits. Maintenant, nous allons monter d'un niveau pour g√©rer les ressources **au niveau d'un namespace entier** avec les **Resource Quotas**.

Les Resource Quotas sont comme un **budget global** que vous allouez √† un namespace. C'est un m√©canisme de gouvernance qui permet de contr√¥ler la consommation totale de ressources dans un environnement partag√©.

## Pourquoi les Resource Quotas sont-ils importants ?

### Le probl√®me sans Resource Quotas

Imaginez un cluster Kubernetes partag√© par plusieurs √©quipes :

‚ùå **Sans Resource Quotas** :
- L'√©quipe A d√©ploie 100 Pods et monopolise tout le cluster
- L'√©quipe B ne peut plus d√©ployer ses applications
- L'√©quipe C cr√©e un Pod avec des limits √©normes qui bloque les autres
- Aucune pr√©visibilit√© ni √©quit√© dans l'utilisation des ressources

### La solution avec Resource Quotas

‚úÖ **Avec Resource Quotas** :
- Chaque √©quipe a son namespace avec un budget d√©fini
- L'√©quipe A ne peut pas d√©passer son quota
- Les √©quipes B et C ont leurs ressources garanties
- Gestion pr√©visible et √©quitable du cluster

## Analogie pour comprendre

Pensez √† un immeuble avec plusieurs appartements :

**Sans quotas** :
- Un locataire pourrait utiliser toute l'eau de l'immeuble
- Les autres seraient priv√©s d'eau
- Facturation impr√©visible

**Avec quotas** :
- Chaque appartement a un compteur d'eau individuel
- Limite mensuelle d√©finie par appartement
- Si un locataire d√©passe, il ne peut plus consommer
- Les autres restent prot√©g√©s

Dans Kubernetes :
- **Immeuble** = Cluster
- **Appartements** = Namespaces
- **Compteurs** = Resource Quotas
- **Eau** = CPU, RAM, nombre de Pods, etc.

## Diff√©rence avec Requests et Limits

Il est important de comprendre la diff√©rence entre ces trois concepts :

| Concept | Niveau | Objectif |
|---------|--------|----------|
| **Requests** | Pod/Conteneur | Garantir des ressources minimales |
| **Limits** | Pod/Conteneur | Limiter la consommation maximale |
| **Resource Quotas** | Namespace | Limiter la consommation totale du namespace |

**Exemple concret** :

```
Namespace "dev-team-a" avec quota total :
‚îú‚îÄ Quota : 4 CPU, 8Gi RAM maximum
‚îÇ
‚îú‚îÄ Pod 1
‚îÇ  ‚îî‚îÄ Requests: 1 CPU, 2Gi RAM
‚îÇ
‚îú‚îÄ Pod 2
‚îÇ  ‚îî‚îÄ Requests: 1 CPU, 2Gi RAM
‚îÇ
‚îú‚îÄ Pod 3
‚îÇ  ‚îî‚îÄ Requests: 2 CPU, 4Gi RAM
‚îÇ
‚îî‚îÄ Tentative Pod 4 ‚ùå
   ‚îî‚îÄ Requests: 1 CPU, 2Gi RAM
   ‚îî‚îÄ REFUS√â : d√©passerait le quota (5 CPU > 4 CPU)
```

## Types de ressources contr√¥lables

Les Resource Quotas peuvent limiter plusieurs types de ressources :

### 1. Ressources de calcul

**CPU et M√©moire** :
- `requests.cpu` : Total des CPU requests dans le namespace
- `requests.memory` : Total de la RAM requests dans le namespace
- `limits.cpu` : Total des CPU limits dans le namespace
- `limits.memory` : Total de la RAM limits dans le namespace

### 2. Nombre d'objets

**Comptage d'objets Kubernetes** :
- `pods` : Nombre total de Pods
- `services` : Nombre de Services
- `persistentvolumeclaims` : Nombre de PVCs
- `configmaps` : Nombre de ConfigMaps
- `secrets` : Nombre de Secrets
- `replicationcontrollers` : Nombre de ReplicationControllers
- `deployments.apps` : Nombre de Deployments
- `statefulsets.apps` : Nombre de StatefulSets

### 3. Stockage

**Volume persistants** :
- `requests.storage` : Total de stockage demand√©
- `persistentvolumeclaims` : Nombre de PVCs

### 4. Ressources √©tendues

**Ressources sp√©cifiques** :
- `requests.nvidia.com/gpu` : Nombre de GPUs
- Services de type particulier (LoadBalancer, NodePort)

## Configuration de base

### Cr√©er un Resource Quota simple

Voici un exemple de quota basique qui limite les ressources de calcul :

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev-team
spec:
  hard:
    requests.cpu: "4"           # Max 4 CPU en requests
    requests.memory: "8Gi"      # Max 8 Gi RAM en requests
    limits.cpu: "8"             # Max 8 CPU en limits
    limits.memory: "16Gi"       # Max 16 Gi RAM en limits
```

**Appliquer le quota** :

```bash
kubectl apply -f compute-quota.yaml
```

### V√©rifier le quota

```bash
kubectl get resourcequota -n dev-team
```

R√©sultat :
```
NAME            AGE   REQUEST                                      LIMIT
compute-quota   5m    requests.cpu: 0/4, requests.memory: 0/8Gi   limits.cpu: 0/8, limits.memory: 0/16Gi
```

La notation `0/4` signifie : **0 utilis√© sur 4 disponibles**.

### Description d√©taill√©e

```bash
kubectl describe resourcequota compute-quota -n dev-team
```

R√©sultat :
```
Name:            compute-quota
Namespace:       dev-team
Resource         Used  Hard
--------         ----  ----
limits.cpu       0     8
limits.memory    0     16Gi
requests.cpu     0     4
requests.memory  0     8Gi
```

## Exemples pratiques

### Exemple 1 : Quota pour environnement de d√©veloppement

Environnement dev avec ressources limit√©es :

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: development
spec:
  hard:
    # Ressources de calcul
    requests.cpu: "2"
    requests.memory: "4Gi"
    limits.cpu: "4"
    limits.memory: "8Gi"

    # Nombre d'objets
    pods: "10"
    services: "5"
    persistentvolumeclaims: "3"

    # Stockage
    requests.storage: "20Gi"
```

**Caract√©ristiques** :
- Ressources modestes (dev seulement)
- Limitation du nombre de Pods √† 10
- 20Gi de stockage total

### Exemple 2 : Quota pour environnement de production

Environnement prod avec ressources g√©n√©reuses :

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: prod-quota
  namespace: production
spec:
  hard:
    # Ressources de calcul
    requests.cpu: "20"
    requests.memory: "40Gi"
    limits.cpu: "40"
    limits.memory: "80Gi"

    # Nombre d'objets
    pods: "50"
    services: "20"
    persistentvolumeclaims: "15"

    # Stockage
    requests.storage: "200Gi"
```

**Caract√©ristiques** :
- Ressources importantes (production critique)
- Plus de Pods autoris√©s
- Stockage cons√©quent

### Exemple 3 : Quota avec comptage d'objets uniquement

Limiter uniquement le nombre d'objets sans contrainte sur les ressources :

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-count-quota
  namespace: testing
spec:
  hard:
    pods: "20"
    services: "10"
    configmaps: "15"
    secrets: "20"
    persistentvolumeclaims: "5"
    deployments.apps: "10"
    statefulsets.apps: "3"
```

**Cas d'usage** : √âviter qu'une √©quipe ne cr√©e trop d'objets qui encombrent le cluster.

### Exemple 4 : Quota pour services expos√©s

Limiter les services expos√©s (co√ªteux en production cloud) :

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: services-quota
  namespace: web-apps
spec:
  hard:
    services.loadbalancers: "3"    # Max 3 LoadBalancers
    services.nodeports: "5"        # Max 5 NodePort
```

**Cas d'usage** : En production cloud, les LoadBalancers co√ªtent cher. Ce quota √©vite les d√©penses excessives.

### Exemple 5 : Quota avec priorit√© diff√©rente

Combiner plusieurs quotas avec scopes diff√©rents :

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: priority-high-quota
  namespace: critical-apps
spec:
  hard:
    requests.cpu: "10"
    requests.memory: "20Gi"
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values: ["high"]
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: priority-low-quota
  namespace: critical-apps
spec:
  hard:
    requests.cpu: "2"
    requests.memory: "4Gi"
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values: ["low"]
```

**Cas d'usage** : Allouer plus de ressources aux Pods haute priorit√©.

## Comportement avec les Pods

### R√®gle importante : Requests obligatoires

‚ö†Ô∏è **Point crucial** : Quand un namespace a un Resource Quota sur le CPU ou la m√©moire, **tous les Pods doivent d√©finir des requests et limits**.

**Sans quota** - Ce Pod fonctionne :
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mon-pod
spec:
  containers:
  - name: nginx
    image: nginx
    # Pas de resources d√©fini - OK
```

**Avec quota** - Ce m√™me Pod sera **REFUS√â** :
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mon-pod
  namespace: dev-team  # Namespace avec quota
spec:
  containers:
  - name: nginx
    image: nginx
    # Pas de resources d√©fini - ERREUR !
```

**Message d'erreur** :
```
Error from server (Forbidden): error when creating "pod.yaml":
pods "mon-pod" is forbidden: failed quota: compute-quota:
must specify limits.cpu,limits.memory,requests.cpu,requests.memory
```

### Solution : Toujours d√©finir les resources

**Pod correct avec quota** :
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mon-pod
  namespace: dev-team
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "256Mi"
        cpu: "200m"
```

Ce Pod sera accept√© si les resources totales du namespace ne d√©passent pas le quota.

## Sc√©narios de d√©passement de quota

### Sc√©nario 1 : D√©passement lors de la cr√©ation

```bash
# Cr√©er un Deployment qui d√©passerait le quota
kubectl apply -f big-deployment.yaml -n dev-team
```

**R√©sultat** :
```
Error from server (Forbidden): error when creating "big-deployment.yaml":
deployments.apps "big-app" is forbidden: exceeded quota: compute-quota,
requested: requests.cpu=5, used: requests.cpu=3, limited: requests.cpu=4
```

Le Deployment est **rejet√©** car il demanderait 5 CPU alors que le quota est de 4 CPU.

### Sc√©nario 2 : D√©passement lors du scaling

```bash
# Tenter de scaler un Deployment existant
kubectl scale deployment mon-app --replicas=10 -n dev-team
```

Si cette op√©ration d√©passerait le quota :
- Le ReplicaSet tentera de cr√©er les nouveaux Pods
- Les Pods seront **refus√©s** s'ils d√©passent le quota
- Certains Pods resteront en √©tat `Pending`

**V√©rification** :
```bash
kubectl get pods -n dev-team
```

R√©sultat :
```
NAME                      READY   STATUS    RESTARTS   AGE
mon-app-7d8f9c-abc12      1/1     Running   0          5m
mon-app-7d8f9c-def34      1/1     Running   0          5m
mon-app-7d8f9c-ghi56      1/1     Running   0          5m
```

Seulement 3 Pods au lieu de 10 car le quota est atteint.

### Sc√©nario 3 : V√©rifier l'utilisation actuelle

```bash
kubectl describe resourcequota compute-quota -n dev-team
```

```
Name:            compute-quota
Namespace:       dev-team
Resource         Used    Hard
--------         ----    ----
limits.cpu       600m    8
limits.memory    768Mi   16Gi
requests.cpu     3900m   4      ‚ö†Ô∏è Proche du quota !
requests.memory  7.5Gi   8Gi    ‚ö†Ô∏è Proche du quota !
pods             8       10
```

L'√©quipe est proche de la limite - il est temps d'optimiser ou de demander une augmentation.

## Multiples quotas dans un namespace

Vous pouvez avoir **plusieurs Resource Quotas** dans le m√™me namespace pour organiser diff√©rents aspects :

```yaml
# Quota 1 : Ressources de calcul
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: my-namespace
spec:
  hard:
    requests.cpu: "4"
    requests.memory: "8Gi"
---
# Quota 2 : Objets Kubernetes
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-quota
  namespace: my-namespace
spec:
  hard:
    pods: "20"
    services: "10"
---
# Quota 3 : Stockage
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage-quota
  namespace: my-namespace
spec:
  hard:
    requests.storage: "50Gi"
    persistentvolumeclaims: "10"
```

**Tous les quotas s'appliquent simultan√©ment** - un Pod doit respecter tous les quotas pour √™tre cr√©√©.

## LimitRange : Le compl√©ment des Resource Quotas

Les Resource Quotas contr√¥lent le **total**, mais vous voudrez peut-√™tre aussi contr√¥ler les **valeurs individuelles**. C'est le r√¥le des **LimitRanges** (voir section 18.3).

**Combinaison ResourceQuota + LimitRange** :

```yaml
# ResourceQuota : limite TOTALE du namespace
apiVersion: v1
kind: ResourceQuota
metadata:
  name: namespace-quota
  namespace: team-a
spec:
  hard:
    requests.cpu: "10"
    requests.memory: "20Gi"
---
# LimitRange : limites PAR POD
apiVersion: v1
kind: LimitRange
metadata:
  name: pod-limits
  namespace: team-a
spec:
  limits:
  - max:
      cpu: "2"           # Un Pod ne peut pas demander plus de 2 CPU
      memory: "4Gi"      # Un Pod ne peut pas demander plus de 4Gi
    type: Pod
```

**R√©sultat** :
- Un Pod individuel ne peut pas d√©passer 2 CPU / 4Gi
- Le total de tous les Pods ne peut pas d√©passer 10 CPU / 20Gi

Cela √©vite qu'un seul Pod monopolise tout le quota.

## Bonnes pratiques

### 1. D√©finir des quotas pour chaque namespace

‚úÖ **Recommandation** : Chaque namespace devrait avoir un Resource Quota, sauf le namespace `kube-system`.

**Organisation recommand√©e** :

```
Cluster
‚îú‚îÄ kube-system (pas de quota - syst√®me)
‚îú‚îÄ development (quota l√©ger)
‚îú‚îÄ staging (quota moyen)
‚îú‚îÄ production (quota g√©n√©reux)
‚îî‚îÄ monitoring (quota d√©di√©)
```

### 2. Commencer avec des quotas g√©n√©reux

‚úÖ **Approche progressive** :

1. **Phase 1** : D√©finir des quotas tr√®s g√©n√©reux (ne devraient jamais √™tre atteints)
2. **Phase 2** : Monitor l'utilisation r√©elle pendant 2-4 semaines
3. **Phase 3** : Ajuster les quotas √† +30% au-dessus du pic observ√©
4. **Phase 4** : Affiner r√©guli√®rement

**Exemple** :
```yaml
# Phase 1 - Quota initial g√©n√©reux
spec:
  hard:
    requests.cpu: "50"    # Tr√®s large
    requests.memory: "100Gi"
```

Apr√®s observation, r√©duire √† l'usage r√©el + marge :
```yaml
# Phase 3 - Quota ajust√© apr√®s monitoring
spec:
  hard:
    requests.cpu: "10"     # Usage max observ√©: 7 CPU
    requests.memory: "20Gi" # Usage max observ√©: 15Gi
```

### 3. Documenter les quotas

‚úÖ **Documentation dans les annotations** :

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: development
  annotations:
    contact: "team-platform@company.com"
    purpose: "Environnement de d√©veloppement - quota l√©ger"
    last-review: "2025-10-01"
    review-frequency: "quarterly"
spec:
  hard:
    requests.cpu: "4"
    requests.memory: "8Gi"
```

### 4. Monitoring des quotas

‚úÖ **Alertes recommand√©es** :

```yaml
# Exemple d'alerte Prometheus (pseudo-code)
alert: QuotaAlmostFull
expr: |
  (kube_resourcequota{type="used"} / kube_resourcequota{type="hard"}) > 0.80
annotations:
  summary: "Quota {{ $labels.resource }} at {{ $value }}% in namespace {{ $labels.namespace }}"
```

### 5. Pr√©voir un processus d'augmentation

‚úÖ **√âtablir un workflow** :

1. L'√©quipe d√©tecte que son quota est proche de la limite
2. Elle soumet une demande d'augmentation avec justification
3. L'√©quipe plateforme analyse l'usage r√©el
4. D√©cision : optimisation ou augmentation du quota

**Template de demande** :
```
Namespace: production
Quota actuel: 10 CPU, 20Gi RAM
Quota demand√©: 15 CPU, 30Gi RAM
Justification: Nouveau microservice client-api avec 5 replicas
Usage actuel: 9.2 CPU (92%), 18Gi RAM (90%)
Deadline: 2025-11-01
```

### 6. Combiner avec LimitRange

‚úÖ **Protection √† deux niveaux** :

```yaml
# Niveau 1 : ResourceQuota (total namespace)
apiVersion: v1
kind: ResourceQuota
metadata:
  name: total-quota
  namespace: my-team
spec:
  hard:
    requests.cpu: "10"
    requests.memory: "20Gi"
---
# Niveau 2 : LimitRange (par Pod)
apiVersion: v1
kind: LimitRange
metadata:
  name: pod-limits
  namespace: my-team
spec:
  limits:
  - max:
      cpu: "2"
      memory: "4Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    type: Pod
```

Cela assure :
- Pas de Pod trop petit (min)
- Pas de Pod trop gros (max)
- Total contr√¥l√© (quota)

### 7. Environnements diff√©rents = quotas diff√©rents

‚úÖ **Adaptation par environnement** :

```yaml
# D√©veloppement - quota minimal
requests.cpu: "2"
requests.memory: "4Gi"
pods: "10"
---
# Staging - quota moyen
requests.cpu: "8"
requests.memory: "16Gi"
pods: "25"
---
# Production - quota g√©n√©reux
requests.cpu: "30"
requests.memory: "60Gi"
pods: "100"
```

## Commandes utiles

### Lister les quotas

```bash
# Tous les quotas du cluster
kubectl get resourcequota --all-namespaces

# Quotas d'un namespace sp√©cifique
kubectl get resourcequota -n dev-team

# Format d√©taill√©
kubectl get resourcequota -n dev-team -o wide
```

### Voir les d√©tails d'un quota

```bash
kubectl describe resourcequota compute-quota -n dev-team
```

### Surveiller l'utilisation

```bash
# Voir l'utilisation en temps r√©el
kubectl top pods -n dev-team

# Script pour voir le % d'utilisation
kubectl get resourcequota compute-quota -n dev-team -o json | \
  jq '.status.used, .status.hard'
```

### Modifier un quota existant

```bash
# √âditer directement
kubectl edit resourcequota compute-quota -n dev-team

# Ou appliquer un fichier modifi√©
kubectl apply -f compute-quota-updated.yaml
```

### Supprimer un quota

```bash
kubectl delete resourcequota compute-quota -n dev-team
```

‚ö†Ô∏è **Attention** : Supprimer un quota n'affecte pas les Pods existants, mais permet la cr√©ation de nouveaux Pods sans limite.

## Diagnostic et d√©pannage

### Probl√®me 1 : Pod refus√© √† cause du quota

**Sympt√¥me** :
```bash
kubectl apply -f my-pod.yaml -n dev-team
# Error: exceeded quota
```

**Diagnostic** :
```bash
# 1. V√©rifier le quota
kubectl describe resourcequota -n dev-team

# 2. Voir les ressources demand√©es par le Pod
kubectl describe -f my-pod.yaml | grep -A5 "Requests:"

# 3. Calculer si √ßa rentre
# Used + New Requests <= Hard ?
```

**Solutions** :
- R√©duire les requests du nouveau Pod
- Supprimer des Pods existants pour lib√©rer de l'espace
- Augmenter le quota
- Optimiser les Pods existants (r√©duire leurs requests)

### Probl√®me 2 : Impossible de scaler

**Sympt√¥me** :
```bash
kubectl scale deployment my-app --replicas=10 -n dev-team
# Certains Pods ne d√©marrent pas
```

**Diagnostic** :
```bash
# Voir les √©v√©nements
kubectl get events -n dev-team --sort-by='.lastTimestamp'

# V√©rifier les ReplicaSets
kubectl describe replicaset -n dev-team
```

**Solutions** :
- R√©duire le nombre de replicas
- Optimiser les resources des Pods
- Augmenter le quota

### Probl√®me 3 : Pod sans resources refus√©

**Sympt√¥me** :
```
Error: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
```

**Cause** : Le namespace a un quota mais le Pod ne d√©finit pas de resources.

**Solution** : Ajouter la section resources au Pod :
```yaml
resources:
  requests:
    memory: "128Mi"
    cpu: "100m"
  limits:
    memory: "256Mi"
    cpu: "200m"
```

### Probl√®me 4 : Quota atteint mais Pods en Running

**Observation bizarre** :
- Le quota montre `used = hard` (100% utilis√©)
- Mais on ne voit que quelques Pods running
- O√π sont les ressources ?

**Explication** : Les ressources sont compt√©es m√™me pour :
- Pods en √©tat `Pending`
- Pods en cours de cr√©ation
- Pods qui viennent d'√™tre supprim√©s (courte p√©riode)

**Solution** : Attendre quelques secondes que les Pods soient compl√®tement supprim√©s.

## Cas d'usage avanc√©s

### Quotas par √©quipe dans un cluster partag√©

**Sc√©nario** : Un cluster partag√© par 3 √©quipes.

```yaml
# √âquipe Frontend
apiVersion: v1
kind: ResourceQuota
metadata:
  name: frontend-quota
  namespace: team-frontend
spec:
  hard:
    requests.cpu: "8"
    requests.memory: "16Gi"
    pods: "30"
---
# √âquipe Backend
apiVersion: v1
kind: ResourceQuota
metadata:
  name: backend-quota
  namespace: team-backend
spec:
  hard:
    requests.cpu: "12"
    requests.memory: "24Gi"
    pods: "40"
---
# √âquipe Data
apiVersion: v1
kind: ResourceQuota
metadata:
  name: data-quota
  namespace: team-data
spec:
  hard:
    requests.cpu: "20"
    requests.memory: "50Gi"
    pods: "20"
    requests.storage: "500Gi"  # Beaucoup de stockage pour data
```

### Quotas pour environnements √©ph√©m√®res

**Sc√©nario** : Review apps temporaires pour chaque Pull Request.

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: review-app-quota
  namespace: review-apps
spec:
  hard:
    # Ressources limit√©es (env temporaires)
    requests.cpu: "1"
    requests.memory: "2Gi"

    # Dur√©e de vie des objets
    pods: "5"
    services: "2"

    # Pas de stockage persistant
    persistentvolumeclaims: "0"
```

### Quotas avec distinction QoS

**Sc√©nario** : Favoriser les Pods Guaranteed.

```yaml
# Quota pour Pods Guaranteed
apiVersion: v1
kind: ResourceQuota
metadata:
  name: guaranteed-quota
  namespace: prod
spec:
  hard:
    requests.cpu: "15"
    requests.memory: "30Gi"
  scopes:
  - BestEffort: false  # Exclure BestEffort
---
# Quota pour Pods BestEffort
apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort-quota
  namespace: prod
spec:
  hard:
    pods: "5"  # Tr√®s limit√©
  scopes:
  - BestEffort: true
```

## Points cl√©s √† retenir

üîë **Les essentiels** :

1. **Resource Quotas** = budget total pour un namespace
2. **Prot√®ge** les autres namespaces d'un usage excessif
3. **Obligatoire** de d√©finir requests/limits quand un quota existe
4. **Multiple quotas** possibles dans un namespace
5. **Combiner** avec LimitRange pour contr√¥le complet
6. **Monitor** r√©guli√®rement l'utilisation des quotas
7. **Adapter** les quotas selon l'environnement (dev/staging/prod)
8. **Documenter** les quotas et leurs justifications

## Diff√©rences avec d'autres concepts

| Concept | Niveau | Contr√¥le | Exemple |
|---------|--------|----------|---------|
| **Requests/Limits** | Conteneur | Ressources individuelles | Un Pod demande 500m CPU |
| **ResourceQuota** | Namespace | Total namespace | Max 10 CPU pour tout le namespace |
| **LimitRange** | Namespace | Valeurs min/max par objet | Un Pod ne peut pas d√©passer 2 CPU |
| **PriorityClass** | Pod | Ordre d'√©viction | Pods critiques prot√©g√©s en premier |

Tous ces m√©canismes travaillent ensemble pour une gestion optimale des ressources.

## Conclusion

Les Resource Quotas sont essentiels pour :
- **Gouverner** un cluster multi-√©quipes
- **Pr√©voir** les co√ªts et l'utilisation
- **Prot√©ger** les environnements critiques
- **√âviter** la monopolisation des ressources

Commencez par des quotas g√©n√©reux, monitorez l'usage r√©el, puis ajustez progressivement. Les quotas ne doivent pas √™tre une contrainte g√™nante, mais un garde-fou qui assure la stabilit√© du cluster.

**Prochaine √©tape** : Dans la section suivante (18.3), nous verrons les **LimitRanges** qui compl√®tent les Resource Quotas en contr√¥lant les valeurs individuelles des ressources.

---


‚è≠Ô∏è [LimitRanges](/18-gestion-des-ressources/03-limitranges.md)
