ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 1.5 Architecture gÃ©nÃ©rale de Kubernetes

## Introduction

Maintenant que nous comprenons ce qu'est Kubernetes et pourquoi MicroK8s est un excellent choix pour notre lab, il est temps de regarder sous le capot. Comment Kubernetes fonctionne-t-il rÃ©ellement ? Quels sont les diffÃ©rents composants qui travaillent ensemble pour orchestrer vos conteneurs ?

Ne vous inquiÃ©tez pas si l'architecture semble complexe au premier abord. Nous allons la dÃ©composer Ã©tape par Ã©tape, en utilisant des analogies simples pour rendre chaque concept clair et accessible.

## Vue d'ensemble : l'analogie de l'entreprise

Imaginez Kubernetes comme une **entreprise bien organisÃ©e** :

- **Le control plane** (plan de contrÃ´le) = La direction et les bureaux administratifs
- **Les worker nodes** (nÅ“uds travailleurs) = Les entrepÃ´ts oÃ¹ le travail rÃ©el est effectuÃ©
- **Les conteneurs** = Les produits fabriquÃ©s dans les entrepÃ´ts
- **L'API Server** = Le PDG qui reÃ§oit toutes les demandes et prend les dÃ©cisions
- **Le Scheduler** = Le responsable RH qui assigne les tÃ¢ches aux travailleurs
- **Les Controllers** = Les superviseurs qui vÃ©rifient que tout fonctionne correctement

Cette entreprise fonctionne de maniÃ¨re autonome : donnez-lui des instructions sur ce que vous voulez, et elle s'organise pour y arriver !

## Architecture en deux parties

Kubernetes est composÃ© de deux parties principales :

### 1. Le Control Plane (Plan de ContrÃ´le)

C'est le **cerveau** de Kubernetes. Il prend toutes les dÃ©cisions importantes :
- OÃ¹ dÃ©ployer les conteneurs ?
- Quand crÃ©er de nouveaux conteneurs ?
- Que faire si un conteneur tombe en panne ?
- Comment router le trafic rÃ©seau ?

**RÃ´le :** GÃ©rer et coordonner tout le cluster

### 2. Les Worker Nodes (NÅ“uds Travailleurs)

Ce sont les **muscles** de Kubernetes. Ils exÃ©cutent le travail rÃ©el :
- Faire tourner les conteneurs
- HÃ©berger les applications
- GÃ©rer les ressources locales (CPU, mÃ©moire, stockage)

**RÃ´le :** ExÃ©cuter vos applications conteneurisÃ©es

## Visualisation de l'architecture

Voici comment ces Ã©lÃ©ments s'organisent :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CONTROL PLANE                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  API Server  â”‚  â”‚  Scheduler   â”‚  â”‚ Controller   â”‚      â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚  Manager     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                            â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚         â”‚     etcd     â”‚  (Base de donnÃ©es)                â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ Communication
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WORKER NODE 1   â”‚                   â”‚  WORKER NODE 2   â”‚
â”‚                  â”‚                   â”‚                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Kubelet   â”‚  â”‚                   â”‚  â”‚  Kubelet   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Kube-Proxy â”‚  â”‚                   â”‚  â”‚ Kube-Proxy â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Container  â”‚  â”‚                   â”‚  â”‚ Container  â”‚  â”‚
â”‚  â”‚  Runtime   â”‚  â”‚                   â”‚  â”‚  Runtime   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                  â”‚                   â”‚                  â”‚
â”‚  [Pods running]  â”‚                   â”‚  [Pods running]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Les composants du Control Plane

Plongeons maintenant dans chaque composant du control plane pour comprendre son rÃ´le.

### 1. API Server (kube-apiserver)

**Analogie :** Le rÃ©ceptionniste ou le standard tÃ©lÃ©phonique de l'entreprise

**RÃ´le :**
C'est le **point d'entrÃ©e unique** pour toutes les communications dans Kubernetes. Tout passe par lui :
- Vous voulez crÃ©er un pod ? Vous parlez Ã  l'API Server
- Le scheduler veut vÃ©rifier les ressources ? Il demande Ã  l'API Server
- Un controller veut modifier un dÃ©ploiement ? Il passe par l'API Server

**Fonctionnement :**
1. ReÃ§oit les requÃªtes (via kubectl, dashboard, ou autres outils)
2. Valide les requÃªtes (est-ce que vous avez le droit ? est-ce valide ?)
3. Traite les requÃªtes et met Ã  jour l'Ã©tat dans etcd
4. Retourne les rÃ©ponses

**CaractÃ©ristiques :**
- Interface RESTful (HTTP/HTTPS)
- Seul composant qui parle directement Ã  etcd
- Peut Ãªtre mis Ã  l'Ã©chelle horizontalement (plusieurs instances)
- Point central de sÃ©curitÃ© et d'authentification

**Commandes qui parlent Ã  l'API Server :**
```bash
kubectl get pods              # Demande la liste des pods
kubectl create deployment     # Demande de crÃ©er un dÃ©ploiement
kubectl delete service        # Demande de supprimer un service
```

### 2. etcd

**Analogie :** Le systÃ¨me de classement et d'archivage de l'entreprise

**RÃ´le :**
C'est la **base de donnÃ©es** de Kubernetes. Tout l'Ã©tat du cluster est stockÃ© ici :
- Quels pods existent ?
- Quelle est leur configuration ?
- Sur quel nÅ“ud tournent-ils ?
- Quels sont les services exposÃ©s ?
- Toutes les configurations

**Fonctionnement :**
- Base de donnÃ©es clÃ©-valeur distribuÃ©e
- CohÃ©rence forte (tous les nÅ“uds voient la mÃªme chose)
- Rapide et fiable
- Peut Ãªtre rÃ©pliquÃ© pour haute disponibilitÃ©

**CaractÃ©ristiques importantes :**
- Seul l'API Server communique avec etcd (les autres composants passent par l'API)
- Source de vÃ©ritÃ© unique (single source of truth)
- Sauvegarder etcd = sauvegarder tout votre cluster
- Performance critique pour le cluster

**Ce qui est stockÃ© dans etcd :**
- Ã‰tat de tous les objets Kubernetes (pods, services, deployments, etc.)
- Configuration du cluster
- Secrets et ConfigMaps
- Ã‰tat des nÅ“uds
- MÃ©tadonnÃ©es diverses

**Note MicroK8s :** Par dÃ©faut, MicroK8s utilise **Dqlite** au lieu d'etcd. C'est une alternative plus lÃ©gÃ¨re basÃ©e sur SQLite, spÃ©cialement conÃ§ue pour simplifier la haute disponibilitÃ©.

### 3. Scheduler (kube-scheduler)

**Analogie :** Le responsable des ressources humaines qui assigne les employÃ©s aux projets

**RÃ´le :**
Le scheduler dÃ©cide **oÃ¹** exÃ©cuter chaque pod. Quand vous crÃ©ez un nouveau pod, c'est lui qui choisit sur quel nÅ“ud il va tourner.

**Processus de dÃ©cision :**
1. **Filtrage** : Ã‰limine les nÅ“uds impossibles
   - Pas assez de CPU disponible ? Ã‰liminÃ©
   - Pas assez de mÃ©moire ? Ã‰liminÃ©
   - Le pod nÃ©cessite un GPU et ce nÅ“ud n'en a pas ? Ã‰liminÃ©

2. **Scoring** : Note les nÅ“uds restants
   - Quel nÅ“ud a le plus de ressources disponibles ?
   - Quel nÅ“ud rÃ©partira le mieux la charge ?
   - Les affinitÃ©s et anti-affinitÃ©s sont-elles respectÃ©es ?

3. **SÃ©lection** : Choisit le nÅ“ud avec le meilleur score

**CritÃ¨res de dÃ©cision :**
- Ressources disponibles (CPU, mÃ©moire)
- Contraintes matÃ©rielles (SSD, GPU, etc.)
- Labels et selectors
- AffinitÃ©s et anti-affinitÃ©s (placer ensemble ou sÃ©parer)
- Taints et tolerations
- RÃ©partition de la charge
- Topologie (zones de disponibilitÃ©, rÃ©gions)

**Exemple concret :**
```
Vous crÃ©ez un pod qui nÃ©cessite 2 CPU et 4 Go RAM

Le scheduler regarde :
- Node1 : 1 CPU disponible âŒ Ã‰liminÃ©
- Node2 : 4 CPU et 8 Go disponibles âœ… Score 8/10
- Node3 : 8 CPU et 16 Go disponibles âœ… Score 10/10

Le pod est placÃ© sur Node3
```

### 4. Controller Manager (kube-controller-manager)

**Analogie :** L'Ã©quipe de supervision qui vÃ©rifie constamment que tout fonctionne comme prÃ©vu

**RÃ´le :**
Le Controller Manager est en fait une **collection de contrÃ´leurs** qui surveillent l'Ã©tat du cluster et effectuent les actions nÃ©cessaires pour atteindre l'Ã©tat dÃ©sirÃ©.

**Principe fondamental : La boucle de contrÃ´le**
```
1. Observer l'Ã©tat actuel
2. Comparer avec l'Ã©tat dÃ©sirÃ©
3. Si diffÃ©rent : effectuer une action corrective
4. RÃ©pÃ©ter en permanence
```

**ContrÃ´leurs principaux :**

**Node Controller**
- Surveille l'Ã©tat des nÅ“uds
- DÃ©tecte quand un nÅ“ud tombe en panne
- DÃ©cide quand considÃ©rer un nÅ“ud comme "mort"
- Prend des mesures correctives (Ã©vacuer les pods)

**Replication Controller**
- Assure le bon nombre de rÃ©plicas de pods
- Vous voulez 3 rÃ©plicas ? Il s'assure qu'il y en a toujours 3
- En crÃ©e de nouveaux si certains meurent
- En supprime s'il y en a trop

**Endpoints Controller**
- GÃ¨re les endpoints (connexions entre services et pods)
- Met Ã  jour les endpoints quand des pods dÃ©marrent/arrÃªtent
- Assure que le trafic va aux bons pods

**Service Account & Token Controllers**
- GÃ¨rent les comptes de service
- CrÃ©ent les tokens d'authentification
- GÃ¨rent les permissions

**Autres contrÃ´leurs :**
- Deployment Controller
- Job Controller
- DaemonSet Controller
- Et bien d'autres...

**Exemple de boucle de contrÃ´le :**
```
Ã‰tat dÃ©sirÃ© : 3 rÃ©plicas de mon application
Ã‰tat actuel : 2 rÃ©plicas (un a crashÃ©)

Le Replication Controller dÃ©tecte la diffÃ©rence
â†’ Il demande au scheduler de placer un nouveau pod
â†’ Le scheduler choisit un nÅ“ud
â†’ Le kubelet sur ce nÅ“ud dÃ©marre le pod
â†’ Ã‰tat actuel : 3 rÃ©plicas âœ…
```

### 5. Cloud Controller Manager (optionnel)

**RÃ´le :**
GÃ¨re l'intÃ©gration avec les fournisseurs cloud (AWS, Azure, GCP). Il traduit les concepts Kubernetes en ressources cloud.

**Exemples :**
- Service LoadBalancer â†’ CrÃ©er un load balancer AWS ELB
- PersistentVolume â†’ CrÃ©er un volume EBS
- Nodes â†’ GÃ©rer les instances EC2

**Note :** Pas nÃ©cessaire pour MicroK8s dans un lab local, mais important en production cloud.

## Les composants des Worker Nodes

Les worker nodes sont lÃ  oÃ¹ vos applications tournent rÃ©ellement. Chaque worker node contient plusieurs composants.

### 1. Kubelet

**Analogie :** Le contremaÃ®tre de l'entrepÃ´t

**RÃ´le :**
Le kubelet est l'**agent Kubernetes** qui tourne sur chaque nÅ“ud. C'est lui qui fait rÃ©ellement le travail d'exÃ©cution des pods.

**ResponsabilitÃ©s :**
1. **Communication avec le control plane**
   - S'enregistre auprÃ¨s de l'API Server
   - ReÃ§oit les instructions sur les pods Ã  exÃ©cuter
   - Rapporte l'Ã©tat du nÅ“ud et des pods

2. **Gestion des pods**
   - DÃ©marre les conteneurs via le container runtime
   - Surveille l'Ã©tat des conteneurs
   - RedÃ©marre les conteneurs qui crashent
   - ArrÃªte les pods supprimÃ©s

3. **Health checks**
   - ExÃ©cute les liveness probes (le conteneur est-il vivant ?)
   - ExÃ©cute les readiness probes (le conteneur est-il prÃªt Ã  recevoir du trafic ?)
   - Rapporte les rÃ©sultats Ã  l'API Server

4. **Gestion des ressources**
   - Monte les volumes
   - Applique les limites CPU/mÃ©moire
   - GÃ¨re les secrets et ConfigMaps

**Fonctionnement :**
```
1. Kubelet demande Ã  l'API Server : "Quels pods dois-je exÃ©cuter ?"
2. API Server rÃ©pond : "Tu dois exÃ©cuter le pod nginx-xyz"
3. Kubelet demande au container runtime de dÃ©marrer les conteneurs
4. Kubelet surveille constamment les conteneurs
5. Kubelet rapporte l'Ã©tat Ã  l'API Server
```

**Important :** Le kubelet ne gÃ¨re PAS directement les conteneurs. Il demande au container runtime de le faire.

### 2. Kube-Proxy

**Analogie :** Le systÃ¨me de routage et d'aiguillage postal de l'entrepÃ´t

**RÃ´le :**
Kube-proxy gÃ¨re le **rÃ©seau** au niveau du nÅ“ud. Il s'assure que les connexions rÃ©seau arrivent aux bons pods.

**ResponsabilitÃ©s :**
1. **ImplÃ©mentation des Services**
   - Maintient les rÃ¨gles rÃ©seau pour les Services
   - Permet aux pods de communiquer via des Services stables
   - Route le trafic vers les bons pods

2. **Load balancing**
   - Distribue le trafic entre plusieurs rÃ©plicas d'un pod
   - Effectue du load balancing basique

3. **Gestion des rÃ¨gles rÃ©seau**
   - Utilise iptables (ou IPVS, ou userspace selon configuration)
   - Met Ã  jour les rÃ¨gles quand des pods dÃ©marrent/arrÃªtent
   - GÃ¨re la traduction d'adresses (NAT)

**Exemple de fonctionnement :**
```
Vous avez un Service "webapp" avec 3 pods backend

Kube-proxy crÃ©e des rÃ¨gles qui disent :
"Quand quelqu'un appelle webapp:80,
 redirige vers l'un des 3 pods (round-robin)"

Pod A crashe â†’ Kube-proxy met Ã  jour les rÃ¨gles
Maintenant, le trafic va seulement vers Pod B et Pod C
```

**Modes de fonctionnement :**
- **iptables** (mode par dÃ©faut) : Utilise les rÃ¨gles iptables Linux
- **IPVS** : Plus performant pour de nombreux services
- **userspace** : Mode legacy, rarement utilisÃ©

### 3. Container Runtime

**Analogie :** Les machines et outils dans l'entrepÃ´t qui font le travail rÃ©el

**RÃ´le :**
C'est le logiciel qui exÃ©cute rÃ©ellement les conteneurs. Kubernetes ne fait pas tourner les conteneurs directement - il dÃ©lÃ¨gue ce travail au container runtime.

**Options de container runtime :**
- **containerd** (le plus courant) : Runtime standard, utilisÃ© par MicroK8s
- **CRI-O** : Runtime spÃ©cialement conÃ§u pour Kubernetes
- **Docker** (via dockershim, deprecated) : L'ancien standard

**ResponsabilitÃ©s :**
1. TÃ©lÃ©charger les images de conteneurs depuis un registre
2. DÃ©marrer et arrÃªter les conteneurs
3. GÃ©rer le stockage des conteneurs
4. GÃ©rer le rÃ©seau des conteneurs (niveau bas)

**Communication :**
Le kubelet parle au container runtime via une interface standard appelÃ©e **CRI (Container Runtime Interface)**.

```
Kubelet â†’ "Hey containerd, dÃ©marre un conteneur nginx"
containerd â†’ "OK, je tÃ©lÃ©charge l'image et je dÃ©marre le conteneur"
containerd â†’ "C'est fait, le conteneur est en cours d'exÃ©cution"
```

## Comment tout cela fonctionne ensemble

Voyons maintenant un exemple complet pour comprendre comment tous ces composants collaborent.

### ScÃ©nario : DÃ©ployer une application web

**Vous exÃ©cutez :**
```bash
kubectl create deployment webapp --image=nginx --replicas=3
```

**Voici ce qui se passe en coulisses :**

#### Ã‰tape 1 : RÃ©ception de la commande
```
kubectl â†’ API Server : "CrÃ©e un Deployment avec 3 rÃ©plicas nginx"
API Server : Valide la requÃªte, authentification OK
API Server â†’ etcd : "Enregistre ce nouveau Deployment"
etcd : "OK, enregistrÃ©"
```

#### Ã‰tape 2 : Le Deployment Controller dÃ©tecte
```
Deployment Controller (scrute constamment l'API Server)
Deployment Controller : "Nouveau Deployment dÃ©tectÃ© !"
Deployment Controller : "Je dois crÃ©er un ReplicaSet pour gÃ©rer les rÃ©plicas"
Deployment Controller â†’ API Server : "CrÃ©e un ReplicaSet pour ce Deployment"
```

#### Ã‰tape 3 : Le ReplicaSet Controller intervient
```
ReplicaSet Controller : "Nouveau ReplicaSet dÃ©tectÃ© !"
ReplicaSet Controller : "Je dois crÃ©er 3 pods"
ReplicaSet Controller â†’ API Server : "CrÃ©e 3 pods"
API Server â†’ etcd : "Enregistre ces 3 pods (Ã©tat : Pending)"
```

#### Ã‰tape 4 : Le Scheduler place les pods
```
Scheduler (scrute constamment les nouveaux pods)
Scheduler : "3 nouveaux pods sans nÅ“ud assignÃ© !"
Scheduler : Analyse les nÅ“uds disponibles
Scheduler : "Pod1 â†’ Node1, Pod2 â†’ Node2, Pod3 â†’ Node1"
Scheduler â†’ API Server : "Assigne ces pods Ã  ces nÅ“uds"
```

#### Ã‰tape 5 : Les Kubelets exÃ©cutent les pods
```
Kubelet sur Node1 : "L'API Server dit que je dois exÃ©cuter Pod1 et Pod3"
Kubelet â†’ containerd : "DÃ©marre un conteneur nginx pour Pod1"
containerd : TÃ©lÃ©charge l'image nginx si nÃ©cessaire
containerd : DÃ©marre le conteneur
Kubelet : Surveille le conteneur
Kubelet â†’ API Server : "Pod1 est en cours d'exÃ©cution"

(MÃªme processus sur Node2 pour Pod2)
```

#### Ã‰tape 6 : Ã‰tat final
```
API Server â†’ etcd : Met Ã  jour l'Ã©tat des pods (Running)
Vous : kubectl get pods
API Server : Retourne la liste des pods
RÃ©sultat : 3 pods nginx en Ã©tat "Running" âœ…
```

### Toute cette orchestration s'est passÃ©e en quelques secondes !

## La magie de la dÃ©claration d'Ã©tat

Un concept clÃ© de Kubernetes que nous venons d'illustrer :

**Vous ne dites pas Ã  Kubernetes COMMENT faire les choses**
```bash
# Vous ne dites PAS :
# "DÃ©marre un conteneur nginx sur le nÅ“ud 1"
# "Puis dÃ©marre-en un autre sur le nÅ“ud 2"
# "Surveille-les et redÃ©marre-les s'ils crashent"
```

**Vous dites Ã  Kubernetes CE QUE vous voulez**
```bash
# Vous dites :
"Je veux 3 rÃ©plicas de nginx"
```

**Kubernetes se dÃ©brouille pour y arriver et maintenir cet Ã©tat.**

C'est ce qu'on appelle le **paradigme dÃ©claratif** :
- Vous dÃ©clarez l'Ã©tat dÃ©sirÃ©
- Kubernetes fait converger l'Ã©tat actuel vers l'Ã©tat dÃ©sirÃ©
- En permanence et automatiquement

## Architecture single-node vs multi-node

### Single-Node (configuration MicroK8s par dÃ©faut)

Dans un cluster Ã  un seul nÅ“ud, **tous les composants sont sur la mÃªme machine** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            SINGLE NODE                     â”‚
â”‚                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€ CONTROL PLANE â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ API Server | Scheduler | ...   â”‚        â”‚
â”‚  â”‚         etcd/Dqlite            â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€ WORKER COMPONENTS â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Kubelet | Kube-Proxy           â”‚        â”‚
â”‚  â”‚ Container Runtime              â”‚        â”‚
â”‚  â”‚ [Vos pods tournent ici]        â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Avantages :**
- Simple Ã  installer et gÃ©rer
- Peu de ressources requises
- Parfait pour dÃ©veloppement et apprentissage
- Moins de complexitÃ© rÃ©seau

**Limitations :**
- Pas de haute disponibilitÃ©
- Si le nÅ“ud tombe, tout s'arrÃªte
- LimitÃ© aux ressources d'une seule machine

### Multi-Node (Ã©volution possible)

Dans un cluster multi-nÅ“uds, le control plane et les workers sont sÃ©parÃ©s :

```
â”Œâ”€â”€â”€â”€â”€â”€ CONTROL PLANE NODE(S) â”€â”€â”€â”€â”€â”€â”
â”‚  API Server | Scheduler | ...     â”‚
â”‚         etcd/Dqlite               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WORKER NODE 1â”‚  â”‚ WORKER NODE 2 â”‚
â”‚              â”‚  â”‚               â”‚
â”‚ Kubelet      â”‚  â”‚ Kubelet       â”‚
â”‚ Kube-Proxy   â”‚  â”‚ Kube-Proxy    â”‚
â”‚ Container RT â”‚  â”‚ Container RT  â”‚
â”‚ [Pods]       â”‚  â”‚ [Pods]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Avantages :**
- Haute disponibilitÃ© (si un nÅ“ud tombe, les autres continuent)
- Plus de ressources totales
- SÃ©paration des responsabilitÃ©s
- Meilleure rÃ©partition de la charge

**ComplexitÃ© ajoutÃ©e :**
- RÃ©seau plus complexe
- Plus de machines Ã  gÃ©rer
- Configuration plus Ã©laborÃ©e

## Communication entre composants

### Communication interne

Tous les composants communiquent via l'**API Server** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            API SERVER                   â”‚
â”‚    (Point central de communication)     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚     â”‚    â”‚     â”‚    â”‚    â”‚
     â†“     â†“    â†“     â†“    â†“    â†“
  Scheduler â”‚  etcd  â”‚  Kubelet â”‚
      Controller     Kube-Proxy
           Manager
```

**CaractÃ©ristiques importantes :**
- Communication sÃ©curisÃ©e (TLS)
- Authentification et autorisation
- Format standardisÃ© (REST API, JSON/YAML)

### Ports rÃ©seau standard

Pour votre culture gÃ©nÃ©rale, voici les ports utilisÃ©s :

**Control Plane :**
- API Server : 6443 (HTTPS)
- etcd : 2379-2380
- Scheduler : 10251
- Controller Manager : 10252

**Worker Nodes :**
- Kubelet API : 10250
- NodePort Services : 30000-32767

**Note MicroK8s :** Ces ports sont gÃ©rÃ©s automatiquement, vous n'avez gÃ©nÃ©ralement pas besoin de vous en prÃ©occuper !

## SpÃ©cificitÃ©s de MicroK8s

MicroK8s simplifie cette architecture de plusieurs faÃ§ons :

### 1. Tout-en-un
- Control plane et worker sur le mÃªme nÅ“ud par dÃ©faut
- Parfait pour commencer simple

### 2. Dqlite au lieu d'etcd
- Base de donnÃ©es plus lÃ©gÃ¨re
- Plus simple pour la haute disponibilitÃ©
- BasÃ©e sur SQLite

### 3. Installation intÃ©grÃ©e
- Tous les composants installÃ©s d'un coup
- Pas de configuration manuelle de chaque composant
- DÃ©marrage automatique

### 4. Addons prÃ©configurÃ©s
- Composants supplÃ©mentaires activables facilement
- Dashboard, DNS, Registry, etc.
- Installation et configuration automatiques

## Concepts clÃ©s Ã  retenir

### 1. Separation of Concerns (SÃ©paration des responsabilitÃ©s)
Chaque composant a un rÃ´le prÃ©cis et ne fait que ce rÃ´le. C'est ce qui rend Kubernetes robuste et maintenable.

### 2. Ã‰tat dÃ©claratif
Vous dÃ©clarez ce que vous voulez, Kubernetes s'arrange pour y arriver.

### 3. Boucles de contrÃ´le
Les contrÃ´leurs surveillent constamment l'Ã©tat et corrigent les Ã©carts.

### 4. Communication centralisÃ©e
Tout passe par l'API Server, ce qui simplifie la sÃ©curitÃ© et le monitoring.

### 5. Composants interchangeables
Vous pouvez remplacer certains composants (container runtime, rÃ©seau, etc.) selon vos besoins.

## Analogie complÃ¨te : l'orchestre symphonique

Pour conclure, voici une analogie complÃ¨te :

**Kubernetes = un orchestre symphonique**

- **API Server** = Le chef d'orchestre qui coordonne tout
- **etcd** = La partition musicale (la source de vÃ©ritÃ©)
- **Scheduler** = Le rÃ©gisseur qui place les musiciens
- **Controllers** = Les chefs de section qui surveillent leur groupe
- **Kubelet** = Chaque musicien qui joue sa partition
- **Kube-Proxy** = L'acoustique de la salle qui distribue le son
- **Container Runtime** = Les instruments de musique
- **Vos applications** = La musique produite

Vous (l'utilisateur) donnez juste la partition (vos manifestes YAML), et l'orchestre se coordonne automatiquement pour jouer la symphonie !

## Conclusion

L'architecture de Kubernetes peut sembler complexe avec tous ces composants, mais chacun a un rÃ´le simple et bien dÃ©fini. Ensemble, ils crÃ©ent un systÃ¨me robuste, flexible et auto-rÃ©parateur.

**Les points essentiels :**
- **Control Plane** : Le cerveau qui prend les dÃ©cisions
- **Worker Nodes** : Les muscles qui exÃ©cutent le travail
- **API Server** : Le point central de communication
- **Paradigme dÃ©claratif** : Vous dÃ©clarez ce que vous voulez, Kubernetes s'en occupe
- **MicroK8s** : Simplifie tout cela pour vous

Dans la prochaine section, nous explorerons les spÃ©cificitÃ©s de MicroK8s, notamment son systÃ¨me d'addons et Dqlite, qui rendent Kubernetes encore plus accessible !

---

**Points clÃ©s Ã  retenir :**
- Architecture en deux parties : Control Plane (cerveau) + Worker Nodes (muscles)
- Control Plane : API Server, etcd/Dqlite, Scheduler, Controller Manager
- Worker Nodes : Kubelet, Kube-Proxy, Container Runtime
- Communication centralisÃ©e via l'API Server
- Paradigme dÃ©claratif : dÃ©clarez l'Ã©tat dÃ©sirÃ©, Kubernetes y arrive
- Boucles de contrÃ´le : surveillance et correction continues
- MicroK8s simplifie cette architecture pour la rendre accessible
- Single-node parfait pour dÃ©buter, Ã©volutif vers multi-node

â­ï¸ [SpÃ©cificitÃ©s MicroK8s : addons, Dqlite et simplicitÃ©](/01-introduction-a-kubernetes-et-microk8s/06-specificites-microk8s-addons-dqlite-et-simplicite.md)
