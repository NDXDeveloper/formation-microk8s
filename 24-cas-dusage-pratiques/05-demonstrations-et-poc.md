üîù Retour au [Sommaire](/SOMMAIRE.md)

# 24.5 D√©monstrations et PoC

## Introduction

Les d√©monstrations (d√©mos) et les Proof of Concept (PoC) sont des √©l√©ments essentiels pour convaincre, former ou valider des concepts techniques. Avec MicroK8s, vous disposez d'une plateforme id√©ale pour cr√©er des environnements de d√©monstration impressionnants, reproductibles et rapides √† d√©ployer.

Dans ce chapitre, nous allons explorer comment utiliser MicroK8s pour cr√©er des d√©monstrations percutantes et des PoC convaincants, que ce soit pour pr√©senter Kubernetes √† votre √©quipe, valider une architecture avant un projet, ou impressionner un client.

## Qu'est-ce qu'une D√©mo et un PoC ?

### D√©monstration (Demo)

Une **d√©monstration** est une pr√©sentation pratique qui montre le fonctionnement d'une technologie ou d'une solution. L'objectif est de :

- **√âduquer** : Montrer comment quelque chose fonctionne
- **Impressionner** : D√©montrer les capacit√©s d'une solution
- **Convaincre** : Prouver la valeur d'une approche
- **Former** : Enseigner par l'exemple

**Exemples** :
- Pr√©sentation de Kubernetes √† votre √©quipe
- D√©monstration des capacit√©s d'auto-scaling
- Showcase d'une architecture microservices
- Pr√©sentation du d√©ploiement continu

### Proof of Concept (PoC)

Un **PoC** est une impl√©mentation exp√©rimentale qui prouve qu'une id√©e ou une approche est r√©alisable. L'objectif est de :

- **Valider** : Confirmer qu'une solution fonctionne
- **Tester** : √âvaluer les performances et limitations
- **Comparer** : Mesurer diff√©rentes approches
- **D√©cider** : Fournir des donn√©es pour une d√©cision

**Exemples** :
- Tester la migration d'une application vers Kubernetes
- Valider une architecture avant un grand projet
- Comparer diff√©rentes solutions de monitoring
- √âvaluer la faisabilit√© d'une solution cloud-native

### Diff√©rences Cl√©s

| Aspect | D√©monstration | PoC |
|--------|--------------|-----|
| **Dur√©e** | Court (minutes √† heures) | Moyen (jours √† semaines) |
| **Profondeur** | Surface, vue d'ensemble | Approfondi, d√©taill√© |
| **Objectif** | Montrer, impressionner | Prouver, valider |
| **Audience** | Large (management, √©quipes) | Technique (architectes, dev) |
| **Donn√©es** | Fictives ou simplifi√©es | R√©elles ou repr√©sentatives |

## Pourquoi MicroK8s pour les D√©mos et PoC ?

### Avantages de MicroK8s

**Installation rapide** : D√©ploy√© en quelques minutes, parfait pour les d√©mos spontan√©es.

**Environnement complet** : Toutes les fonctionnalit√©s d'un cluster Kubernetes r√©el.

**Portable** : Fonctionne sur laptop, serveur, VM, ou cloud.

**L√©ger** : Consomme peu de ressources, peut tourner en arri√®re-plan.

**Reproductible** : Les d√©mos fonctionnent de la m√™me mani√®re √† chaque fois.

**Addons simples** : Activation de fonctionnalit√©s en une commande.

**Isolation** : Pas de risque pour d'autres environnements.

**Co√ªt z√©ro** : Pas de frais de cloud pour les d√©monstrations.

## Pr√©paration de l'Environnement de D√©mo

### Configuration Syst√®me Recommand√©e

Pour des d√©mos fluides sans ralentissement :

**Configuration Minimale** :
- CPU : 4 c≈ìurs
- RAM : 8 GB
- Stockage : 50 GB
- Bonne connexion r√©seau (pour t√©l√©charger les images)

**Configuration Id√©ale** :
- CPU : 6-8 c≈ìurs
- RAM : 16 GB
- Stockage : 100 GB SSD
- √âcran secondaire (pour pr√©senter)

### Installation et Configuration de Base

```bash
# Installation rapide
sudo snap install microk8s --classic
sudo usermod -a -G microk8s $USER
newgrp microk8s

# Configuration de l'alias
alias kubectl='microk8s kubectl'
echo "alias kubectl='microk8s kubectl'" >> ~/.bashrc

# Activer les addons essentiels pour les d√©mos
microk8s enable dns storage dashboard ingress registry metrics-server

# V√©rifier que tout est pr√™t
microk8s status --wait-ready
```

### Namespace D√©di√© aux D√©mos

```bash
# Cr√©er un namespace pour les d√©mos
kubectl create namespace demo

# Le d√©finir comme namespace par d√©faut
kubectl config set-context --current --namespace=demo

# Cr√©er aussi un namespace pour les PoC
kubectl create namespace poc

# Labels pour identification
kubectl label namespace demo purpose=demonstration environment=showcase
kubectl label namespace poc purpose=proof-of-concept environment=validation
```

### Pre-Pull des Images Courantes

Pour √©viter les temps d'attente pendant les d√©mos, t√©l√©chargez les images √† l'avance :

```bash
# Images couramment utilis√©es
docker pull nginx:latest
docker pull redis:alpine
docker pull postgres:15
docker pull mongo:latest
docker pull mysql:8.0

# Images pour les d√©mos
docker pull nginx:alpine
docker pull hashicorp/http-echo
docker pull curlimages/curl
docker pull busybox

# Images applicatives
docker pull wordpress:latest
docker pull ghost:latest
docker pull grafana/grafana:latest
docker pull prom/prometheus:latest
```

### Script de Pr√©paration Automatique

Cr√©ez un fichier `prepare-demo.sh` :

```bash
#!/bin/bash

echo "üé¨ Preparing demo environment..."

# V√©rifier que MicroK8s est pr√™t
microk8s status --wait-ready

# Cr√©er les namespaces si n√©cessaire
kubectl create namespace demo --dry-run=client -o yaml | kubectl apply -f -
kubectl create namespace poc --dry-run=client -o yaml | kubectl apply -f -

# Nettoyer les anciennes d√©mos
kubectl delete all --all -n demo
kubectl delete all --all -n poc

# Pre-pull des images essentielles
echo "üì¶ Pre-pulling essential images..."
docker pull nginx:alpine
docker pull redis:alpine
docker pull postgres:15

# V√©rifier les addons
echo "üîß Checking addons..."
microk8s status

# V√©rifier les ressources disponibles
echo "üíª System resources:"
kubectl top nodes 2>/dev/null || echo "Metrics server not available"

echo "‚úÖ Demo environment ready!"
echo "üìä Dashboard: kubectl port-forward -n kube-system service/kubernetes-dashboard 10443:443"
echo "üöÄ Happy demoing!"
```

Rendez-le ex√©cutable :

```bash
chmod +x prepare-demo.sh
./prepare-demo.sh
```

## Sc√©narios de D√©monstration

### Sc√©nario 1 : D√©ploiement Rapide d'Application

**Objectif** : Montrer la simplicit√© du d√©ploiement dans Kubernetes

**Dur√©e** : 5 minutes

**Audience** : Managers, √©quipes qui d√©couvrent Kubernetes

#### Script de D√©monstration

```bash
# 1. Montrer l'√©tat du cluster
echo "Voici notre cluster Kubernetes vide..."
kubectl get all -n demo

# 2. D√©ployer une application web en une commande
echo "Je d√©ploie une application web avec une seule commande..."
kubectl create deployment webapp --image=nginx --replicas=3 -n demo

# 3. Observer la cr√©ation
echo "Observons la cr√©ation automatique des pods..."
kubectl get pods -n demo -w
# Arr√™tez avec Ctrl+C apr√®s quelques secondes

# 4. Exposer l'application
echo "Rendons l'application accessible..."
kubectl expose deployment webapp --port=80 --type=NodePort -n demo

# 5. Obtenir l'URL
echo "L'application est accessible ici:"
kubectl get svc webapp -n demo

# 6. Scaler l'application
echo "Augmentons la capacit√© √† 10 instances..."
kubectl scale deployment webapp --replicas=10 -n demo

# 7. Observer le scaling
kubectl get pods -n demo

# 8. Montrer la r√©silience
echo "Supprimons un pod, Kubernetes va automatiquement le recr√©er..."
POD=$(kubectl get pods -n demo -l app=webapp -o jsonpath='{.items[0].metadata.name}')
kubectl delete pod $POD -n demo
kubectl get pods -n demo -w
```

**Points √† souligner** :
- D√©ploiement en quelques secondes
- Scaling facile
- Auto-gu√©rison automatique
- Gestion d√©clarative

### Sc√©nario 2 : Rolling Update Sans Downtime

**Objectif** : D√©montrer les mises √† jour sans interruption de service

**Dur√©e** : 7 minutes

**Audience** : √âquipes DevOps, managers IT

#### Pr√©paration

```yaml
# rolling-demo.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rolling-demo
  namespace: demo
spec:
  replicas: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: rolling-demo
  template:
    metadata:
      labels:
        app: rolling-demo
    spec:
      containers:
      - name: web
        image: hashicorp/http-echo
        args:
        - "-text=Version 1.0"
        - "-listen=:8080"
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: rolling-demo
  namespace: demo
spec:
  selector:
    app: rolling-demo
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
    nodePort: 30090
  type: NodePort
```

#### Script de D√©monstration

```bash
# 1. D√©ployer la version 1
echo "D√©ploiement de la Version 1.0..."
kubectl apply -f rolling-demo.yaml

# Attendre que tout soit pr√™t
kubectl wait --for=condition=available deployment/rolling-demo -n demo --timeout=120s

# 2. Montrer la version actuelle
echo "Version actuelle de l'application:"
curl http://localhost:30090

# 3. G√©n√©rer du trafic continu (dans un terminal s√©par√© ou en arri√®re-plan)
echo "G√©n√©rons du trafic continu pour voir qu'il n'y a pas d'interruption..."
# Dans un terminal s√©par√© : while true; do curl -s http://localhost:30090; sleep 1; done

# 4. Mettre √† jour vers la version 2
echo "Mise √† jour vers la Version 2.0..."
kubectl set image deployment/rolling-demo web=hashicorp/http-echo -n demo
kubectl set env deployment/rolling-demo WEB_TEXT="Version 2.0" -n demo

# Alternative : modifier directement les arguments
kubectl patch deployment rolling-demo -n demo --type='json' \
  -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/args", "value": ["-text=Version 2.0", "-listen=:8080"]}]'

# 5. Observer le rolling update en temps r√©el
echo "Observons le rolling update..."
kubectl rollout status deployment/rolling-demo -n demo

# 6. Voir l'historique
kubectl rollout history deployment/rolling-demo -n demo

# 7. Rollback si n√©cessaire
echo "En cas de probl√®me, on peut revenir en arri√®re instantan√©ment..."
kubectl rollout undo deployment/rolling-demo -n demo
```

**Points √† souligner** :
- Z√©ro downtime pendant la mise √† jour
- Rollback instantan√© si probl√®me
- Contr√¥le fin du processus de d√©ploiement
- Traffic toujours servi pendant la transition

### Sc√©nario 3 : Auto-Scaling en Action

**Objectif** : Montrer l'adaptation automatique aux charges

**Dur√©e** : 10 minutes

**Audience** : Architectes, √©quipes techniques

#### Pr√©paration

```yaml
# autoscaling-demo.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
  namespace: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: php-apache
  template:
    metadata:
      labels:
        app: php-apache
    spec:
      containers:
      - name: php-apache
        image: k8s.gcr.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 200m
          limits:
            cpu: 500m
---
apiVersion: v1
kind: Service
metadata:
  name: php-apache
  namespace: demo
spec:
  selector:
    app: php-apache
  ports:
  - protocol: TCP
    port: 80
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
  namespace: demo
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
```

#### Script de D√©monstration

```bash
# 1. Activer metrics-server si n√©cessaire
microk8s enable metrics-server

# 2. D√©ployer l'application avec HPA
echo "D√©ploiement d'une application avec auto-scaling..."
kubectl apply -f autoscaling-demo.yaml

# Attendre que tout soit pr√™t
sleep 30

# 3. Montrer l'√©tat initial
echo "√âtat initial - 1 replica:"
kubectl get pods -n demo -l app=php-apache
kubectl get hpa -n demo

# 4. Montrer la consommation CPU actuelle
echo "Utilisation CPU actuelle:"
kubectl top pods -n demo -l app=php-apache

# 5. G√©n√©rer de la charge
echo "G√©n√©rons une charge importante..."
kubectl run -it --rm load-generator --image=busybox -n demo --restart=Never -- sh -c "while true; do wget -q -O- http://php-apache; done"

# Dans un terminal s√©par√©, observer le scaling
kubectl get hpa -n demo -w
kubectl get pods -n demo -l app=php-apache -w

# 6. Montrer le scaling en action
echo "Kubernetes ajoute automatiquement des pods..."
watch -n 2 "kubectl get hpa -n demo; echo ''; kubectl get pods -n demo -l app=php-apache"

# 7. Arr√™ter la charge et observer le scale down
# Arr√™tez le load-generator avec Ctrl+C

echo "Sans charge, Kubernetes r√©duit automatiquement le nombre de pods..."
kubectl get hpa -n demo -w
```

**Points √† souligner** :
- Adaptation automatique √† la charge
- Optimisation des co√ªts (scale down)
- Bas√© sur des m√©triques r√©elles (CPU, m√©moire, custom)
- R√©action rapide aux pics de trafic

### Sc√©nario 4 : Application Multi-Tiers Compl√®te

**Objectif** : D√©montrer une architecture r√©aliste avec plusieurs composants

**Dur√©e** : 15 minutes

**Audience** : Stakeholders techniques, clients potentiels

#### Architecture

```
[Frontend Web] ‚Üí [Backend API] ‚Üí [Redis Cache]
                                ‚Üì
                           [Database PostgreSQL]
```

#### D√©ploiement

```yaml
# fullstack-demo.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: fullstack-demo
---
# PostgreSQL Database
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  namespace: fullstack-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
        tier: database
    spec:
      containers:
      - name: postgres
        image: postgres:15
        env:
        - name: POSTGRES_DB
          value: appdb
        - name: POSTGRES_USER
          value: appuser
        - name: POSTGRES_PASSWORD
          value: demopassword
        ports:
        - containerPort: 5432
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: fullstack-demo
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
---
# Redis Cache
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: fullstack-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
        tier: cache
    spec:
      containers:
      - name: redis
        image: redis:alpine
        ports:
        - containerPort: 6379
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
---
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: fullstack-demo
spec:
  selector:
    app: redis
  ports:
  - port: 6379
---
# Backend API
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  namespace: fullstack-demo
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
        tier: backend
    spec:
      containers:
      - name: api
        image: hashicorp/http-echo
        args:
        - "-text=Backend API - Connected to DB: postgres:5432, Cache: redis:6379"
        - "-listen=:8080"
        env:
        - name: DATABASE_URL
          value: "postgresql://appuser:demopassword@postgres:5432/appdb"
        - name: REDIS_URL
          value: "redis://redis:6379"
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
---
apiVersion: v1
kind: Service
metadata:
  name: backend
  namespace: fullstack-demo
spec:
  selector:
    app: backend
  ports:
  - port: 8080
---
# Frontend
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: fullstack-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/conf.d
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
      volumes:
      - name: nginx-config
        configMap:
          name: frontend-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: frontend-config
  namespace: fullstack-demo
data:
  default.conf: |
    server {
        listen 80;
        location /api/ {
            proxy_pass http://backend:8080/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }
        location / {
            root /usr/share/nginx/html;
            index index.html;
            try_files $uri $uri/ /index.html;
        }
    }
---
apiVersion: v1
kind: Service
metadata:
  name: frontend
  namespace: fullstack-demo
spec:
  selector:
    app: frontend
  ports:
  - port: 80
    nodePort: 30100
  type: NodePort
```

#### Script de D√©monstration

```bash
# 1. D√©ployer toute la stack
echo "üöÄ D√©ploiement d'une application full-stack compl√®te..."
kubectl apply -f fullstack-demo.yaml

# 2. Observer le d√©ploiement progressif
echo "üì¶ Observation de la cr√©ation des composants..."
kubectl get all -n fullstack-demo

# 3. Attendre que tout soit pr√™t
echo "‚è≥ Attente de la disponibilit√© de tous les services..."
kubectl wait --for=condition=available deployment --all -n fullstack-demo --timeout=300s

# 4. Montrer l'architecture
echo "üèóÔ∏è  Architecture d√©ploy√©e:"
echo "- Frontend (2 replicas) ‚Üí http://localhost:30100"
echo "- Backend API (3 replicas)"
echo "- Redis Cache (1 replica)"
echo "- PostgreSQL Database (1 replica)"

# 5. Visualiser la topologie
kubectl get pods -n fullstack-demo -o wide --show-labels

# 6. Tester la connectivit√© entre services
echo "üîç Test de connectivit√© inter-services..."
BACKEND_POD=$(kubectl get pod -n fullstack-demo -l app=backend -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n fullstack-demo $BACKEND_POD -- nc -zv postgres 5432
kubectl exec -n fullstack-demo $BACKEND_POD -- nc -zv redis 6379

# 7. Tester l'application
echo "üåê Test de l'application:"
curl http://localhost:30100/api/

# 8. Montrer les logs en temps r√©el
echo "üìù Logs du backend:"
kubectl logs -f -l app=backend -n fullstack-demo --tail=20

# 9. Scaler les diff√©rentes parties
echo "üìà Scaling des composants:"
kubectl scale deployment frontend --replicas=4 -n fullstack-demo
kubectl scale deployment backend --replicas=6 -n fullstack-demo

# 10. Simuler une panne
echo "üí• Simulation d'une panne - suppression d'un pod backend..."
BACKEND_POD=$(kubectl get pod -n fullstack-demo -l app=backend -o jsonpath='{.items[0].metadata.name}')
kubectl delete pod $BACKEND_POD -n fullstack-demo

echo "üîÑ Kubernetes recr√©e automatiquement le pod..."
kubectl get pods -n fullstack-demo -l app=backend -w
```

**Points √† souligner** :
- Architecture r√©aliste multi-tiers
- Communication s√©curis√©e entre services
- S√©paration des responsabilit√©s
- Scaling ind√©pendant de chaque composant
- R√©silience automatique

### Sc√©nario 5 : GitOps et D√©ploiement Continu

**Objectif** : Montrer le d√©ploiement automatis√© depuis Git

**Dur√©e** : 12 minutes

**Audience** : √âquipes DevOps, d√©veloppeurs

#### Pr√©paration avec ArgoCD

```bash
# 1. Installer ArgoCD
microk8s enable argocd

# Attendre que ArgoCD soit pr√™t
kubectl wait --for=condition=available deployment --all -n argocd --timeout=300s

# 2. Obtenir le mot de passe admin
kubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath="{.data.password}" | base64 -d

# 3. Port-forward pour acc√©der √† l'UI
kubectl port-forward svc/argocd-server -n argocd 8080:443 &
```

#### Application de D√©monstration

Cr√©ez un repository Git (peut √™tre local pour la d√©mo) avec cette structure :

```
demo-app/
‚îú‚îÄ‚îÄ deployment.yaml
‚îú‚îÄ‚îÄ service.yaml
‚îî‚îÄ‚îÄ ingress.yaml
```

#### Script de D√©monstration

```bash
# 1. Montrer le repository Git
echo "üìÇ Voici notre repository Git avec la configuration de l'application..."
ls -la demo-app/

# 2. Cr√©er l'application ArgoCD
cat <<EOF | kubectl apply -f -
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: demo-app
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/votre-org/demo-app.git
    targetRevision: HEAD
    path: .
  destination:
    server: https://kubernetes.default.svc
    namespace: demo
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
EOF

# 3. Observer le d√©ploiement automatique
echo "üîÑ ArgoCD synchronise automatiquement depuis Git..."
kubectl get application -n argocd

# 4. Montrer l'UI ArgoCD
echo "üé® Interface ArgoCD disponible sur https://localhost:8080"
echo "Username: admin"
echo "Password: [mot de passe r√©cup√©r√© pr√©c√©demment]"

# 5. Faire un changement dans Git
echo "‚úèÔ∏è  Modification du nombre de replicas dans Git..."
# Modifier deployment.yaml : replicas: 3 ‚Üí replicas: 5
# Commit et push

# 6. Observer la synchronisation automatique
echo "üîÑ ArgoCD d√©tecte le changement et d√©ploie automatiquement..."
kubectl get pods -n demo -w

# 7. Montrer l'historique des d√©ploiements
kubectl get events -n demo --sort-by='.lastTimestamp'
```

**Points √† souligner** :
- Single source of truth (Git)
- D√©ploiement automatique
- Tra√ßabilit√© compl√®te
- Rollback facile via Git
- Audit trail

## PoC Techniques

### PoC 1 : Migration d'Application Legacy

**Objectif** : Prouver qu'une application existante peut √™tre conteneuris√©e et d√©ploy√©e sur Kubernetes

**Dur√©e** : 1-2 semaines

#### Phase 1 : Analyse de l'Application

```bash
# Documentation de l'application existante
- Architecture actuelle
- D√©pendances (base de donn√©es, services externes)
- Volumes et donn√©es persistantes
- Configuration et secrets
- Ports et protocoles utilis√©s
```

#### Phase 2 : Conteneurisation

```dockerfile
# Dockerfile pour l'application legacy
FROM ubuntu:20.04

# Installation des d√©pendances
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Copie de l'application
WORKDIR /app
COPY . /app

# Installation des d√©pendances Python
RUN pip3 install -r requirements.txt

# Configuration
ENV PORT=8080
EXPOSE 8080

# D√©marrage
CMD ["python3", "app.py"]
```

#### Phase 3 : D√©ploiement Kubernetes

```yaml
# legacy-app-poc.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: legacy-migration-poc
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: legacy-app
  namespace: legacy-migration-poc
spec:
  replicas: 2
  selector:
    matchLabels:
      app: legacy-app
  template:
    metadata:
      labels:
        app: legacy-app
    spec:
      containers:
      - name: app
        image: localhost:32000/legacy-app:1.0
        ports:
        - containerPort: 8080
        env:
        - name: DB_HOST
          value: postgres
        - name: DB_NAME
          value: legacydb
        volumeMounts:
        - name: config
          mountPath: /app/config
        - name: data
          mountPath: /app/data
      volumes:
      - name: config
        configMap:
          name: legacy-app-config
      - name: data
        persistentVolumeClaim:
          claimName: legacy-app-data
```

#### Phase 4 : Tests et Validation

```bash
# Script de test
#!/bin/bash

echo "üß™ Tests du PoC de migration..."

# Test 1 : Application d√©marre correctement
echo "Test 1: D√©marrage de l'application"
kubectl wait --for=condition=ready pod -l app=legacy-app -n legacy-migration-poc --timeout=300s

# Test 2 : Connectivit√© base de donn√©es
echo "Test 2: Connexion √† la base de donn√©es"
POD=$(kubectl get pod -n legacy-migration-poc -l app=legacy-app -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n legacy-migration-poc $POD -- curl -f http://localhost:8080/health

# Test 3 : Persistance des donn√©es
echo "Test 3: Persistance des donn√©es"
# √âcrire des donn√©es
# Red√©marrer le pod
# V√©rifier que les donn√©es sont toujours l√†

# Test 4 : Performance
echo "Test 4: Tests de performance"
# Utiliser Apache Bench ou similar
ab -n 1000 -c 10 http://localhost:30000/

# Test 5 : R√©silience
echo "Test 5: Tests de r√©silience"
kubectl delete pod -l app=legacy-app -n legacy-migration-poc
sleep 10
kubectl get pods -n legacy-migration-poc
```

#### Phase 5 : Documentation des R√©sultats

Documentez :
- ‚úÖ Fonctionnalit√©s qui marchent
- ‚ö†Ô∏è Limitations identifi√©es
- üìä M√©triques de performance (avant/apr√®s)
- üí∞ Estimation des co√ªts
- üîÑ Plan de migration complet
- üìù Recommandations

### PoC 2 : Comparaison de Solutions de Monitoring

**Objectif** : √âvaluer diff√©rentes solutions de monitoring pour choisir la meilleure

**Dur√©e** : 1 semaine

#### Solutions √† Comparer

1. **Prometheus + Grafana** (solution open-source)
2. **Datadog** (solution SaaS)
3. **ELK Stack** (logs + m√©triques)

#### Crit√®res d'√âvaluation

```yaml
# Grille d'√©valuation
evaluation:
  facilite_installation:
    weight: 15%
    scoring: 1-10

  facilite_utilisation:
    weight: 20%
    scoring: 1-10

  fonctionnalites:
    weight: 25%
    required:
      - M√©triques syst√®me
      - M√©triques applicatives
      - Alerting
      - Dashboards
      - Logs centralis√©s

  performance:
    weight: 15%
    metrics:
      - Latence de collecte
      - Consommation CPU
      - Consommation m√©moire

  cout:
    weight: 15%
    factors:
      - Licence
      - Infrastructure
      - Maintenance

  integration:
    weight: 10%
    aspects:
      - APIs disponibles
      - Int√©grations tierces
      - Extensibilit√©
```

#### D√©ploiement des Solutions

```bash
# Solution 1 : Prometheus + Grafana
microk8s enable prometheus

# Solution 2 : Datadog (avec agent)
kubectl apply -f datadog-agent.yaml

# Solution 3 : ELK Stack
kubectl apply -f elasticsearch.yaml
kubectl apply -f kibana.yaml
kubectl apply -f filebeat.yaml
```

#### Tests Comparatifs

```bash
# G√©n√©rer une charge applicative
kubectl run load-test --image=williamyeh/wrk -n poc -- \
  wrk -t12 -c400 -d30s http://test-app

# Mesurer les performances de chaque solution
# CPU/M√©moire utilis√©s
kubectl top pods -n monitoring

# Latence de collecte
# (√† mesurer manuellement dans chaque UI)

# Facilit√© d'utilisation
# (score subjectif apr√®s utilisation)
```

#### Rapport de Comparaison

```markdown
# Rapport de Comparaison - Solutions de Monitoring

## R√©sum√© Ex√©cutif
[Recommandation finale bas√©e sur les scores]

## Scores D√©taill√©s

### Prometheus + Grafana
- Installation: 8/10 (addon MicroK8s tr√®s simple)
- Utilisation: 7/10 (courbe d'apprentissage PromQL)
- Fonctionnalit√©s: 9/10 (tr√®s compl√®tes)
- Performance: 9/10 (l√©ger et efficace)
- Co√ªt: 10/10 (open-source, gratuit)
- Int√©gration: 9/10 (excellente)
**Score Total: 8.6/10**

### Datadog
- Installation: 9/10 (simple avec agent)
- Utilisation: 9/10 (interface intuitive)
- Fonctionnalit√©s: 10/10 (toutes pr√©sentes)
- Performance: 8/10 (agent consomme des ressources)
- Co√ªt: 4/10 (cher pour production)
- Int√©gration: 10/10 (excellente)
**Score Total: 7.9/10**

### ELK Stack
- Installation: 6/10 (complexe)
- Utilisation: 7/10 (puissant mais complexe)
- Fonctionnalit√©s: 8/10 (focus sur les logs)
- Performance: 6/10 (gourmand en ressources)
- Co√ªt: 8/10 (open-source mais n√©cessite infrastructure)
- Int√©gration: 8/10 (bonne)
**Score Total: 7.1/10**

## Recommandation
Prometheus + Grafana pour notre cas d'usage, avec possibilit√©
d'ajouter Loki pour les logs centralis√©s.
```

### PoC 3 : Strat√©gie de D√©ploiement Blue-Green

**Objectif** : Valider une strat√©gie de d√©ploiement sans risque

**Dur√©e** : 3-5 jours

#### Configuration

```yaml
# blue-green-poc.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-blue
  namespace: poc
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: blue
  template:
    metadata:
      labels:
        app: myapp
        version: blue
    spec:
      containers:
      - name: app
        image: myapp:1.0
        env:
        - name: VERSION
          value: "Blue (1.0)"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-green
  namespace: poc
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: green
  template:
    metadata:
      labels:
        app: myapp
        version: green
    spec:
      containers:
      - name: app
        image: myapp:2.0
        env:
        - name: VERSION
          value: "Green (2.0)"
---
apiVersion: v1
kind: Service
metadata:
  name: myapp
  namespace: poc
spec:
  selector:
    app: myapp
    version: blue  # Initialement sur blue
  ports:
  - port: 80
    targetPort: 8080
```

#### Script de Bascule

```bash
# switch-blue-green.sh
#!/bin/bash

CURRENT=$(kubectl get svc myapp -n poc -o jsonpath='{.spec.selector.version}')
echo "Version actuelle: $CURRENT"

if [ "$CURRENT" == "blue" ]; then
    NEW="green"
else
    NEW="blue"
fi

echo "Bascule vers: $NEW"

# Tests de smoke sur la nouvelle version
echo "Tests de la version $NEW..."
if [ "$NEW" == "green" ]; then
    TEST_POD=$(kubectl get pod -n poc -l version=green -o jsonpath='{.items[0].metadata.name}')
else
    TEST_POD=$(kubectl get pod -n poc -l version=blue -o jsonpath='{.items[0].metadata.name}')
fi

kubectl exec -n poc $TEST_POD -- curl -f http://localhost:8080/health

if [ $? -eq 0 ]; then
    echo "‚úÖ Tests pass√©s, bascule du trafic..."
    kubectl patch svc myapp -n poc -p "{\"spec\":{\"selector\":{\"version\":\"$NEW\"}}}"
    echo "‚úÖ Trafic bascul√© vers $NEW"
else
    echo "‚ùå Tests √©chou√©s, annulation de la bascule"
    exit 1
fi
```

#### Validation

```bash
# Tests de validation
1. D√©ployer les deux versions
2. V√©rifier blue et green fonctionnent
3. Ex√©cuter le script de bascule
4. V√©rifier que le trafic bascule instantan√©ment
5. V√©rifier qu'aucune requ√™te n'est perdue
6. Mesurer le temps de bascule
7. Tester le rollback
```

## Conseils pour des D√©mos R√©ussies

### Avant la D√©mo

**1. Pr√©parez votre environnement**
```bash
# Checklist de pr√©paration
- [ ] MicroK8s d√©marr√© et fonctionnel
- [ ] Tous les addons n√©cessaires activ√©s
- [ ] Images Docker pre-pulled
- [ ] Scripts test√©s au moins 3 fois
- [ ] Namespaces cr√©√©s et nettoy√©s
- [ ] Connexion r√©seau stable
- [ ] Batterie charg√©e (si laptop)
- [ ] Plan B pr√©par√©
```

**2. Testez votre d√©mo**
- R√©p√©tez la d√©mo au moins 3 fois
- Chronom√©trez chaque √©tape
- Notez les commandes exactes √† utiliser
- Pr√©parez des screenshots de backup

**3. Pr√©parez votre discours**
- D√©finissez les points cl√©s √† couvrir
- Pr√©parez des r√©ponses aux questions probables
- Adaptez le niveau technique √† l'audience
- Pr√©parez des analogies pour expliquer les concepts

### Pendant la D√©mo

**1. Commencez fort**
```bash
# Impression imm√©diate avec un one-liner
kubectl create deployment demo --image=nginx --replicas=5 && \
kubectl expose deployment demo --port=80 --type=NodePort && \
echo "Application d√©ploy√©e en 5 secondes !"
```

**2. Racontez une histoire**
- Contexte : "Imaginez que nous devons..."
- Probl√®me : "Sans Kubernetes, nous devrions..."
- Solution : "Avec Kubernetes, nous pouvons simplement..."
- R√©sultat : "Et voil√†, c'est d√©ploy√© et scalable !"

**3. G√©rez les impr√©vus**
```bash
# Si quelque chose √©choue
echo "Voyons ce qui se passe..."
kubectl describe pod <pod-name>
kubectl logs <pod-name>

# Ou utilisez votre backup
echo "Passons √† la version pr√©-pr√©par√©e..."
```

**4. Interagissez avec l'audience**
- Posez des questions
- Invitez √† participer
- Adaptez le rythme selon les r√©actions

### Apr√®s la D√©mo

**1. R√©capitulez**
- R√©sumez les points cl√©s
- R√©pondez aux questions
- Partagez des ressources

**2. Suivez**
- Envoyez les scripts et configs
- Proposez un atelier hands-on
- Restez disponible pour des questions

**3. Collectez les retours**
- Ce qui a bien fonctionn√©
- Ce qui peut √™tre am√©lior√©
- Questions non r√©solues

## Gestion des Risques en D√©mo

### Plan B : Backup de D√©mo

```bash
# Enregistrer une session compl√®te avec asciinema
asciinema rec demo-backup.cast

# Durant la d√©mo, si probl√®me technique :
asciinema play demo-backup.cast
```

### Screenshots de Backup

Pr√©parez des screenshots de chaque √©tape importante :
- √âtat initial du cluster
- Application d√©ploy√©e
- Scaling en action
- Dashboards de monitoring

### Mode Hors-Ligne

Si la d√©mo ne n√©cessite pas d'acc√®s internet :

```bash
# Sauvegarder toutes les images Docker n√©cessaires
docker save nginx redis postgres > demo-images.tar

# Sur la machine de d√©mo
docker load < demo-images.tar
```

### Script de R√©cup√©ration Rapide

```bash
#!/bin/bash
# emergency-reset.sh

echo "üö® Emergency reset..."

# Supprimer tout dans le namespace demo
kubectl delete all --all -n demo --grace-period=0 --force

# Nettoyer Docker
docker system prune -af

# Red√©marrer MicroK8s
microk8s stop
microk8s start

# Attendre que tout soit pr√™t
microk8s status --wait-ready

echo "‚úÖ System ready for retry!"
```

## Outils Utiles pour les D√©mos

### Enregistrement de Terminal

```bash
# asciinema - Enregistrer et rejouer des sessions terminal
sudo apt install asciinema

# Enregistrer
asciinema rec ma-demo.cast

# Rejouer
asciinema play ma-demo.cast

# Partager en ligne
asciinema upload ma-demo.cast
```

### Pr√©sentation de Code

```bash
# bat - Syntax highlighting pour les fichiers
sudo apt install bat

# Utilisation
bat deployment.yaml

# Avec num√©ros de ligne
bat -n deployment.yaml
```

### Terminal Recorder

```bash
# terminalizer - Cr√©er des GIFs anim√©s de votre terminal
npm install -g terminalizer

# Enregistrer
terminalizer record demo

# G√©n√©rer le GIF
terminalizer render demo
```

### Dashboard Live

```bash
# k9s - TUI interactif pour Kubernetes
# Parfait pour montrer l'√©tat du cluster en temps r√©el
sudo snap install k9s

# Lancer
k9s

# Navigation intuitive pour impressionner l'audience
```

## Mod√®les de Documentation

### Template de Pr√©sentation de D√©mo

```markdown
# D√©mo : [Titre]

## Objectif
[Qu'est-ce que cette d√©mo montre ?]

## Audience
[Qui est le public cible ?]

## Dur√©e
[Temps estim√©]

## Pr√©requis
- MicroK8s install√©
- Addons activ√©s : [liste]
- Images pre-pulled : [liste]

## Sc√©nario

### √âtape 1 : [Titre]
**Dur√©e** : X minutes
**Message cl√©** : [Point important √† transmettre]

```bash
# Commandes
kubectl create ...
```

**Points √† souligner** :
- Point 1
- Point 2

### √âtape 2 : [Titre]
...

## Questions Anticip√©es

**Q: Pourquoi utiliser Kubernetes plut√¥t que Docker Compose ?**
R: [Votre r√©ponse pr√©par√©e]

**Q: Quel est le co√ªt de Kubernetes en production ?**
R: [Votre r√©ponse pr√©par√©e]

## Ressources √† Partager
- [Lien vers la documentation]
- [Lien vers les scripts]
- [Lien vers les exemples]
```

### Template de Rapport de PoC

```markdown
# Rapport de PoC : [Titre]

## R√©sum√© Ex√©cutif
[R√©sum√© en 2-3 phrases de ce qui a √©t√© test√© et de la conclusion]

## Objectif du PoC
[Qu'est-ce qu'on cherchait √† prouver ?]

## M√©thodologie
[Comment le PoC a √©t√© conduit]

## Configuration Technique
- Infrastructure : MicroK8s sur [specs]
- Version Kubernetes : [version]
- Applications test√©es : [liste]
- Dur√©e du PoC : [dates]

## R√©sultats

### Ce Qui Fonctionne ‚úÖ
- Fonctionnalit√© 1 : [d√©tails]
- Fonctionnalit√© 2 : [d√©tails]

### Limitations Identifi√©es ‚ö†Ô∏è
- Limitation 1 : [d√©tails et solution possible]
- Limitation 2 : [d√©tails et solution possible]

### M√©triques
- Performance : [chiffres]
- Ressources utilis√©es : [chiffres]
- Temps de d√©ploiement : [chiffres]

## Recommandations

### Actions Recommand√©es
1. [Action 1]
2. [Action 2]

### Prochaines √âtapes
1. [√âtape 1]
2. [√âtape 2]

## Conclusion
[Conclusion finale : go/no-go pour le projet]

## Annexes
- Logs et traces
- Configurations utilis√©es
- Scripts d√©velopp√©s
```

## Conclusion

Les d√©monstrations et les PoC sont des outils puissants pour convaincre, valider et former. Avec MicroK8s, vous disposez d'une plateforme id√©ale pour :

‚úÖ Cr√©er des d√©mos impressionnantes et reproductibles
‚úÖ Valider des concepts avant investissement
‚úÖ Former vos √©quipes de mani√®re pratique
‚úÖ Convaincre les stakeholders avec des preuves concr√®tes
‚úÖ Tester des architectures en conditions r√©elles

**Points cl√©s √† retenir** :

- **Pr√©parez minutieusement** : Une bonne d√©mo se pr√©pare
- **Testez plusieurs fois** : Rien ne doit √©chouer pendant la pr√©sentation
- **Ayez un plan B** : Les impr√©vus arrivent
- **Racontez une histoire** : Les gens retiennent les histoires, pas les commandes
- **Adaptez au public** : Technique pour les dev, business pour les managers
- **Documentez tout** : Scripts, r√©sultats, recommandations

**Prochaines √©tapes** :

1. Choisissez un sc√©nario de d√©mo √† pr√©parer
2. Testez-le au moins 3 fois
3. Pr√©sentez-le √† votre √©quipe
4. Collectez les retours et am√©liorez
5. Cr√©ez votre biblioth√®que de d√©mos r√©utilisables

**Bonne d√©mo ! üé¨**

‚è≠Ô∏è [Formation et certification](/24-cas-dusage-pratiques/06-formation-et-certification.md)
