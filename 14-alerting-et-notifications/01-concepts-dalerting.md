üîù Retour au [Sommaire](/SOMMAIRE.md)

# 14.1 Concepts d'Alerting

## Introduction

L'alerting (ou syst√®me d'alerte) est un composant essentiel de tout syst√®me de monitoring. Imaginez que vous surveillez votre infrastructure Kubernetes 24h/24 : il serait impossible de rester devant vos dashboards en permanence. C'est l√† qu'intervient l'alerting : il surveille pour vous et vous avertit uniquement lorsque quelque chose n√©cessite votre attention.

Dans l'√©cosyst√®me Kubernetes avec Prometheus, l'alerting permet de d√©tecter automatiquement les probl√®mes, les anomalies ou les situations critiques, et de vous notifier via diff√©rents canaux (email, Slack, SMS, etc.).

## Pourquoi l'Alerting est Crucial

Un syst√®me d'alerting efficace vous permet de :

- **R√©agir rapidement** : √ätre inform√© des probl√®mes avant que vos utilisateurs ne les remarquent
- **Pr√©venir les incidents majeurs** : D√©tecter les signes pr√©curseurs d'une panne
- **Optimiser votre temps** : Se concentrer sur les probl√®mes r√©els plut√¥t que de surveiller constamment
- **Am√©liorer la fiabilit√©** : Maintenir vos services disponibles et performants
- **Dormir tranquille** : Savoir que vous serez alert√© en cas de probl√®me, m√™me la nuit

## Les Composants de l'Alerting dans Prometheus

### 1. Prometheus Server

Le serveur Prometheus collecte les m√©triques et **√©value les r√®gles d'alerte**. Il v√©rifie r√©guli√®rement (par d√©faut toutes les minutes) si les conditions d√©finies dans vos r√®gles d'alerte sont remplies.

**R√¥le** : D√©tection des conditions probl√©matiques

### 2. Alertmanager

Alertmanager est un composant s√©par√© qui **g√®re les alertes** une fois qu'elles sont d√©clench√©es par Prometheus. C'est lui qui d√©cide comment, quand et √† qui envoyer les notifications.

**R√¥le** : Gestion et routage des notifications

## Anatomie d'une Alerte

Une alerte est compos√©e de plusieurs √©l√©ments :

### Le Nom (Alert Name)

Un identifiant unique et descriptif de l'alerte. Par exemple : `HighCPUUsage`, `PodCrashLooping`, `DiskSpaceRunningOut`.

**Bonne pratique** : Utilisez des noms clairs qui d√©crivent imm√©diatement le probl√®me.

### La Condition (Expression PromQL)

La r√®gle qui d√©finit quand l'alerte doit se d√©clencher. Elle utilise le langage PromQL pour interroger les m√©triques.

Exemple simple :
```
container_cpu_usage_seconds_total > 0.8
```
Cette expression v√©rifie si l'utilisation CPU d'un conteneur d√©passe 80%.

### La Dur√©e (For)

Le temps pendant lequel la condition doit rester vraie avant de d√©clencher l'alerte. Cela √©vite les "fausses alertes" caus√©es par des pics temporaires.

Exemple :
```
for: 5m
```
L'alerte ne se d√©clenche que si la condition reste vraie pendant 5 minutes cons√©cutives.

### Les Labels

Des √©tiquettes qui permettent de cat√©goriser et de router les alertes. Les labels courants incluent :

- `severity` : L'importance de l'alerte (critical, warning, info)
- `team` : L'√©quipe responsable
- `component` : Le composant concern√©
- `environment` : L'environnement (production, staging, dev)

### Les Annotations

Des informations suppl√©mentaires destin√©es aux humains, comme :

- `summary` : Un r√©sum√© court du probl√®me
- `description` : Une description d√©taill√©e avec des valeurs concr√®tes
- `runbook_url` : Un lien vers la documentation de r√©solution

## Les √âtats d'une Alerte

Une alerte passe par diff√©rents √©tats dans son cycle de vie :

### 1. Inactive

L'alerte est d√©finie mais sa condition n'est pas remplie. Tout va bien.

### 2. Pending

La condition est remplie, mais la dur√©e d√©finie dans `for` n'est pas encore √©coul√©e. Prometheus surveille la situation.

### 3. Firing

La condition est remplie depuis suffisamment longtemps. L'alerte est d√©clench√©e et envoy√©e √† Alertmanager.

### 4. Resolved

La condition n'est plus remplie. Le probl√®me est r√©solu. Une notification de r√©solution peut √™tre envoy√©e.

## Niveaux de S√©v√©rit√©

Il est important de classifier vos alertes par niveau de gravit√© :

### Critical (Critique)

**Quand l'utiliser** : Probl√®me majeur n√©cessitant une action imm√©diate, impact sur les utilisateurs.

**Exemples** :
- Service principal compl√®tement indisponible
- Perte de donn√©es imminente
- Espace disque critique (>95%)

**Action** : Notification imm√©diate, intervention urgente, peut r√©veiller l'√©quipe d'astreinte.

### Warning (Avertissement)

**Quand l'utiliser** : Situation anormale qui n√©cessite de l'attention mais pas d'urgence imm√©diate.

**Exemples** :
- Performance d√©grad√©e
- Espace disque √©lev√© (>80%)
- Augmentation du taux d'erreur

**Action** : Notification pendant les heures de travail, investigation n√©cessaire.

### Info (Information)

**Quand l'utiliser** : Information utile mais qui ne n√©cessite pas forc√©ment d'action.

**Exemples** :
- Nouveau d√©ploiement effectu√©
- Scaling automatique d√©clench√©
- Certificat qui expire dans 30 jours

**Action** : Notification passive, pour information.

## Bonnes Pratiques Fondamentales

### 1. Alertez sur les Sympt√¥mes, pas les Causes

**Mauvais exemple** : Alerter parce qu'un pod red√©marre.
**Bon exemple** : Alerter parce que le taux d'erreur API augmente.

Les utilisateurs se soucient du service qui fonctionne ou non, pas des d√©tails techniques internes.

### 2. √âvitez les Alertes Bruyantes (Alert Fatigue)

Si vous recevez trop d'alertes non critiques, vous risquez d'ignorer les vraies urgences. Chaque alerte doit √™tre :
- **Actionnable** : Elle doit n√©cessiter une action de votre part
- **Pertinente** : Elle doit indiquer un probl√®me r√©el
- **Non redondante** : Ne pas dupliquer d'autres alertes

### 3. Utilisez des Seuils Appropri√©s

Des seuils trop bas g√©n√®rent des fausses alertes. Des seuils trop hauts vous font rater des probl√®mes. Ajustez-les en fonction de :
- Vos observations historiques
- Vos SLA (Service Level Agreements)
- Le comportement normal de vos services

### 4. Documentez Vos Alertes

Chaque alerte devrait avoir :
- Une description claire du probl√®me
- Un lien vers un runbook expliquant comment la r√©soudre
- Des informations de contexte (m√©triques, logs)

### 5. Testez Vos Alertes

V√©rifiez r√©guli√®rement que vos alertes fonctionnent :
- Simulez des conditions d'alerte
- V√©rifiez que les notifications arrivent
- Testez les diff√©rents canaux de notification

## Le Principe des SLI/SLO

### SLI (Service Level Indicator)

Une m√©trique qui mesure un aspect de votre service. Par exemple :
- Taux de disponibilit√© : 99.9%
- Temps de r√©ponse : 200ms en moyenne
- Taux d'erreur : 0.1%

### SLO (Service Level Objective)

L'objectif que vous vous fixez pour un SLI. Par exemple :
- "Notre API doit avoir un temps de r√©ponse < 500ms dans 95% des cas"
- "Notre service doit √™tre disponible √† 99.5%"

**Alerting bas√© sur les SLO** : Plut√¥t que d'alerter sur des m√©triques techniques, alertez lorsque vous risquez de ne pas atteindre vos SLO. C'est une approche plus orient√©e utilisateur.

## Strat√©gies d'Alerting Communes

### Alerting par Seuil

La m√©thode la plus simple : d√©clencher une alerte quand une m√©trique d√©passe une valeur fixe.

**Exemple** : Alerte si l'utilisation CPU > 80% pendant 5 minutes.

**Avantages** : Simple √† comprendre et √† configurer.
**Inconv√©nients** : Ne s'adapte pas aux variations normales du trafic.

### Alerting par Anomalie

D√©tecter les comportements inhabituels en comparant avec des patterns historiques.

**Exemple** : Alerte si le nombre de requ√™tes est 3x sup√©rieur √† la moyenne des 7 derniers jours.

**Avantages** : S'adapte aux variations naturelles.
**Inconv√©nients** : Plus complexe √† mettre en place.

### Alerting par Taux de Changement

D√©tecter les changements soudains plut√¥t que les valeurs absolues.

**Exemple** : Alerte si le taux d'erreur augmente de 50% en 10 minutes.

**Avantages** : D√©tecte rapidement les r√©gressions.
**Inconv√©nients** : Peut √™tre sensible aux variations normales.

## La Cha√Æne de Responsabilit√©

Un bon syst√®me d'alerting d√©finit clairement :

1. **Qui** doit √™tre notifi√©
2. **Quand** (en fonction de la s√©v√©rit√© et de l'heure)
3. **Comment** (email, SMS, appel, Slack)
4. **Que faire** (runbook, proc√©dure d'escalade)

### Escalade

Si une alerte critique n'est pas acquitt√©e dans un d√©lai donn√©, elle peut √™tre automatiquement escalad√©e vers un niveau sup√©rieur (ex: du d√©veloppeur au manager, puis √† l'√©quipe d'astreinte).

## Vocabulaire Essentiel

**Alert Rule** : La d√©finition d'une condition qui d√©clenche une alerte.

**Firing** : √âtat d'une alerte dont la condition est remplie.

**Grouping** : Regroupement de plusieurs alertes similaires en une seule notification.

**Throttling** : Limitation du nombre de notifications pour √©viter le spam.

**Silencing** : D√©sactivation temporaire d'une alerte (pendant une maintenance par exemple).

**Inhibition** : Suppression automatique d'alertes redondantes (ex: si le serveur est down, ne pas alerter sur les services qui tournent dessus).

**Routing** : Direction des alertes vers les bons destinataires selon leurs labels.

## Exemple Conceptuel : Une Alerte Compl√®te

Imaginons une alerte qui d√©tecte un probl√®me de m√©moire :

**Nom** : `HighMemoryUsage`

**Condition** : Utilisation m√©moire > 85% pendant 10 minutes

**S√©v√©rit√©** : `warning`

**Description** : "Le pod {{ $labels.pod }} utilise {{ $value }}% de sa m√©moire allou√©e"

**Runbook** : Lien vers la proc√©dure de v√©rification des fuites m√©moire

**Routing** : Notification vers le canal Slack #alerts-infra

**Grouping** : Regroup√© avec d'autres alertes du m√™me namespace

**Silence** : Peut √™tre silenc√© manuellement pendant une investigation

## Conclusion

L'alerting est l'art de transformer vos m√©triques en notifications actionnables. Un bon syst√®me d'alerting vous aide √† :

- D√©tecter les probl√®mes rapidement
- R√©agir de mani√®re appropri√©e
- Maintenir vos services en bonne sant√©
- √âviter le stress inutile li√© aux fausses alertes

Dans les sections suivantes, nous verrons comment mettre en pratique ces concepts avec Prometheus Alertmanager dans votre cluster MicroK8s.

## Points Cl√©s √† Retenir

‚úì L'alerting surveille vos services √† votre place et vous avertit des probl√®mes

‚úì Prometheus √©value les r√®gles, Alertmanager g√®re les notifications

‚úì Chaque alerte a un nom, une condition, une dur√©e, des labels et des annotations

‚úì Les alertes passent par les √©tats : Inactive ‚Üí Pending ‚Üí Firing ‚Üí Resolved

‚úì Classez vos alertes par s√©v√©rit√© : Critical, Warning, Info

‚úì Alertez sur les sympt√¥mes (impact utilisateur) plut√¥t que les causes techniques

‚úì √âvitez les alertes bruyantes : chaque alerte doit √™tre actionnable

‚úì Documentez vos alertes avec des runbooks

‚úì Testez r√©guli√®rement votre syst√®me d'alerting

---

**Prochaine section** : 14.2 Prometheus Alertmanager - Nous verrons comment installer et configurer Alertmanager pour g√©rer vos alertes.

‚è≠Ô∏è [Prometheus Alertmanager](/14-alerting-et-notifications/02-prometheus-alertmanager.md)
