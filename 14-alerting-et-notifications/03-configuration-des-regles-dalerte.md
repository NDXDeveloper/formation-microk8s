üîù Retour au [Sommaire](/SOMMAIRE.md)

# 14.3 Configuration des R√®gles d'Alerte

## Introduction

Maintenant que vous comprenez les concepts d'alerting et le fonctionnement d'Alertmanager, il est temps d'apprendre √† cr√©er vos propres **r√®gles d'alerte** dans Prometheus. C'est ici que vous d√©finissez les conditions qui d√©clencheront des alertes dans votre infrastructure Kubernetes.

Les r√®gles d'alerte sont le cerveau de votre syst√®me de monitoring : elles analysent constamment vos m√©triques et d√©tectent les situations anormales. Bien configur√©es, elles vous permettent de d√©tecter les probl√®mes avant qu'ils n'affectent vos utilisateurs.

## Anatomie d'une R√®gle d'Alerte

Une r√®gle d'alerte dans Prometheus est d√©finie en YAML et comprend plusieurs √©l√©ments essentiels.

### Structure de Base

```yaml
groups:
  - name: nom_du_groupe
    interval: 30s
    rules:
      - alert: NomDeLAlerte
        expr: expression_promql
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "R√©sum√© court"
          description: "Description d√©taill√©e"
```

### D√©cortiquons Chaque √âl√©ment

**groups** : Les r√®gles sont organis√©es en groupes. Un groupe peut contenir plusieurs r√®gles li√©es.

**name** : Le nom du groupe (ex: `kubernetes_pods`, `node_alerts`)

**interval** : Fr√©quence d'√©valuation des r√®gles dans ce groupe (par d√©faut: 1m)

**alert** : Le nom unique de votre alerte (ex: `HighPodMemory`, `NodeDown`)

**expr** : L'expression PromQL qui d√©finit la condition de d√©clenchement

**for** : Dur√©e pendant laquelle la condition doit √™tre vraie avant de d√©clencher l'alerte

**labels** : √âtiquettes ajout√©es √† l'alerte (utilis√©es pour le routage dans Alertmanager)

**annotations** : Informations descriptives pour les humains (non utilis√©es pour le routage)

## Expressions PromQL pour les Alertes

Le c≈ìur d'une r√®gle d'alerte est son **expression PromQL**. Cette expression doit retourner une valeur qui, si elle est pr√©sente/non-nulle, d√©clenche l'alerte.

### Principe de Base

Une expression qui retourne **quelque chose** = probl√®me d√©tect√©
Une expression qui ne retourne **rien** = tout va bien

### Exemple Simple

```yaml
expr: up{job="my-service"} == 0
```

Cette expression signifie : "Le service n'est pas accessible (up == 0)"

### Op√©rateurs de Comparaison

- `==` : √âgal √†
- `!=` : Diff√©rent de
- `>` : Sup√©rieur √†
- `<` : Inf√©rieur √†
- `>=` : Sup√©rieur ou √©gal √†
- `<=` : Inf√©rieur ou √©gal √†

### Exemple avec Seuil

```yaml
expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
```

Cette expression calcule le pourcentage de m√©moire disponible et alerte si c'est moins de 10%.

## Exemples de R√®gles d'Alerte Essentielles

Voici des r√®gles d'alerte courantes et utiles pour un cluster Kubernetes.

### 1. Service Indisponible

```yaml
groups:
  - name: service_availability
    interval: 30s
    rules:
      - alert: ServiceDown
        expr: up{job="mon-service"} == 0
        for: 2m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "Le service {{ $labels.job }} est indisponible"
          description: "Le service {{ $labels.job }} sur l'instance {{ $labels.instance }} est down depuis plus de 2 minutes."
          runbook_url: "https://wiki.example.com/runbooks/service-down"
```

**Explication** :
- D√©clenche quand un service ne r√©pond plus (m√©trique `up` = 0)
- Attend 2 minutes avant d'alerter (√©vite les faux positifs)
- S√©v√©rit√© critique car impact direct sur les utilisateurs
- Les `{{ $labels.xxx }}` sont remplac√©s par les vraies valeurs

### 2. Utilisation M√©moire √âlev√©e des Pods

```yaml
- alert: HighPodMemoryUsage
  expr: |
    (
      container_memory_working_set_bytes{container!=""}
      /
      container_spec_memory_limit_bytes{container!=""} * 100
    ) > 80
  for: 10m
  labels:
    severity: warning
    component: pod
  annotations:
    summary: "Pod {{ $labels.pod }} utilise {{ $value | humanize }}% de sa m√©moire"
    description: |
      Le pod {{ $labels.pod }} dans le namespace {{ $labels.namespace }}
      utilise {{ $value | humanize }}% de sa limite m√©moire depuis 10 minutes.
      Cela peut causer un OOMKill (Out Of Memory).
```

**Explication** :
- Calcule le pourcentage d'utilisation m√©moire
- Alerte si > 80% pendant 10 minutes
- Utilise `humanize` pour formater les valeurs
- Description multi-lignes avec contexte

### 3. Pod en Crash Loop

```yaml
- alert: PodCrashLooping
  expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "Pod {{ $labels.pod }} red√©marre fr√©quemment"
    description: |
      Le pod {{ $labels.pod }} dans {{ $labels.namespace }}
      a red√©marr√© {{ $value | humanize }} fois/seconde sur les 15 derni√®res minutes.
      Cela indique un probl√®me applicatif.
```

**Explication** :
- Utilise `rate()` pour calculer le taux de red√©marrage
- D√©tecte les pods qui red√©marrent en boucle
- `rate()` sur 15m pour lisser les variations

### 4. Utilisation CPU √âlev√©e du N≈ìud

```yaml
- alert: HighNodeCPU
  expr: |
    (
      100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
    ) > 85
  for: 15m
  labels:
    severity: warning
    component: node
  annotations:
    summary: "Utilisation CPU √©lev√©e sur {{ $labels.instance }}"
    description: |
      Le n≈ìud {{ $labels.instance }} a une utilisation CPU de {{ $value | humanize }}%
      depuis 15 minutes.
```

**Explication** :
- Calcule l'utilisation CPU (100 - idle = utilis√©)
- Alerte si > 85% pendant 15 minutes
- `avg by(instance)` agr√®ge tous les CPU du n≈ìud

### 5. Espace Disque Faible

```yaml
- alert: LowDiskSpace
  expr: |
    (
      node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.*"}
      /
      node_filesystem_size_bytes{fstype!~"tmpfs|fuse.*"} * 100
    ) < 15
  for: 10m
  labels:
    severity: warning
  annotations:
    summary: "Espace disque faible sur {{ $labels.instance }}"
    description: |
      Le syst√®me de fichiers {{ $labels.mountpoint }} sur {{ $labels.instance }}
      n'a plus que {{ $value | humanize }}% d'espace disponible.
```

**Explication** :
- Calcule le pourcentage d'espace disque disponible
- Exclut tmpfs et fuse (syst√®mes de fichiers temporaires)
- Alerte si < 15% d'espace disponible

### 6. Espace Disque Critique

```yaml
- alert: CriticalDiskSpace
  expr: |
    (
      node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.*"}
      /
      node_filesystem_size_bytes{fstype!~"tmpfs|fuse.*"} * 100
    ) < 5
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "Espace disque CRITIQUE sur {{ $labels.instance }}"
    description: |
      URGENT: Le syst√®me de fichiers {{ $labels.mountpoint }} sur {{ $labels.instance }}
      n'a plus que {{ $value | humanize }}% d'espace disponible.
      Action imm√©diate requise pour √©viter une panne.
    runbook_url: "https://wiki.example.com/runbooks/disk-space-critical"
```

**Explication** :
- M√™me m√©trique que LowDiskSpace mais seuil plus bas
- S√©v√©rit√© critique car risque de panne imminent
- Dur√©e plus courte (5m vs 10m) pour r√©agir vite

### 7. Taux d'Erreur HTTP √âlev√©

```yaml
- alert: HighHTTPErrorRate
  expr: |
    (
      sum by(job, namespace) (rate(http_requests_total{status=~"5.."}[5m]))
      /
      sum by(job, namespace) (rate(http_requests_total[5m]))
    ) * 100 > 5
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "Taux d'erreur HTTP √©lev√© pour {{ $labels.job }}"
    description: |
      Le service {{ $labels.job }} dans {{ $labels.namespace }}
      a un taux d'erreur 5xx de {{ $value | humanize }}% sur les 5 derni√®res minutes.
```

**Explication** :
- Calcule le ratio erreurs 5xx / total des requ√™tes
- Status `5..` = toutes les erreurs 5xx (500, 502, 503, etc.)
- Alerte si > 5% d'erreurs

### 8. Latence API √âlev√©e

```yaml
- alert: HighAPILatency
  expr: |
    histogram_quantile(0.95,
      sum by(job, le) (rate(http_request_duration_seconds_bucket[5m]))
    ) > 1
  for: 10m
  labels:
    severity: warning
  annotations:
    summary: "Latence API √©lev√©e pour {{ $labels.job }}"
    description: |
      Le 95√®me percentile de latence pour {{ $labels.job }}
      est de {{ $value | humanize }}s sur les 5 derni√®res minutes.
      Les utilisateurs peuvent ressentir de la lenteur.
```

**Explication** :
- Utilise `histogram_quantile` pour calculer le p95
- Le p95 repr√©sente la latence pour 95% des requ√™tes
- Alerte si p95 > 1 seconde

### 9. Certificat SSL Proche de l'Expiration

```yaml
- alert: CertificateExpiringSoon
  expr: |
    (probe_ssl_earliest_cert_expiry - time()) / 86400 < 7
  for: 1h
  labels:
    severity: warning
  annotations:
    summary: "Certificat SSL expire bient√¥t pour {{ $labels.instance }}"
    description: |
      Le certificat SSL de {{ $labels.instance }} expire dans
      {{ $value | humanize }} jours.
      Renouvellement n√©cessaire.
```

**Explication** :
- Calcule le nombre de jours avant expiration
- 86400 = nombre de secondes dans un jour
- Alerte 7 jours avant l'expiration

### 10. PersistentVolume Presque Plein

```yaml
- alert: PersistentVolumeAlmostFull
  expr: |
    (
      kubelet_volume_stats_available_bytes
      /
      kubelet_volume_stats_capacity_bytes * 100
    ) < 10
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "PersistentVolume {{ $labels.persistentvolumeclaim }} presque plein"
    description: |
      Le PVC {{ $labels.persistentvolumeclaim }} dans {{ $labels.namespace }}
      n'a plus que {{ $value | humanize }}% d'espace disponible.
```

**Explication** :
- Surveille l'espace disponible dans les PersistentVolumes
- Crucial pour les bases de donn√©es et applications stateful
- Alerte pr√©coce pour √©viter les probl√®mes

## Utilisation du Param√®tre "for"

Le param√®tre `for` est crucial pour √©viter les **fausses alertes** (false positives).

### Sans "for" - Alerte Imm√©diate

```yaml
- alert: ExempleImm√©diat
  expr: m√©trique > seuil
  # Pas de "for"
```
D√©clenche **imm√©diatement** d√®s que la condition est vraie.

**Quand l'utiliser** :
- Alertes critiques n√©cessitant une r√©action imm√©diate
- Situations o√π un pic momentan√© est d√©j√† probl√©matique

### Avec "for" - Alerte Retard√©e

```yaml
- alert: ExempleRetard√©
  expr: m√©trique > seuil
  for: 10m
```
D√©clenche **seulement** si la condition reste vraie pendant 10 minutes.

**Quand l'utiliser** :
- La plupart des alertes (80-90% des cas)
- √âvite les alertes sur des pics temporaires normaux
- D√©tecte les probl√®mes persistants

### Choisir la Bonne Dur√©e

**30s - 2m** : Probl√®mes critiques n√©cessitant une action imm√©diate
- Service compl√®tement down
- Perte de donn√©es imminente

**5m - 10m** : Probl√®mes s√©rieux mais pas imm√©diatement critiques
- Utilisation ressources √©lev√©e
- Latence augment√©e
- Taux d'erreur √©lev√©

**15m - 30m** : Tendances pr√©occupantes
- Utilisation m√©moire croissante
- Espace disque qui diminue
- Performance qui se d√©grade

## Labels et Annotations : Bonnes Pratiques

### Labels : Pour le Routage

Les labels sont utilis√©s par Alertmanager pour router les alertes. Choisissez-les avec soin.

**Labels recommand√©s** :

```yaml
labels:
  severity: critical|warning|info
  team: backend|frontend|infra|data
  component: pod|node|network|storage
  environment: production|staging|dev
  service: nom-du-service
```

**Bonnes pratiques** :
- Utilisez des valeurs standardis√©es (pas de texte libre)
- Minimum : toujours avoir `severity`
- Maximum : 5-7 labels par alerte
- Coh√©rence : m√™mes labels √† travers toutes les alertes

### Annotations : Pour les Humains

Les annotations contiennent des informations descriptives. Elles peuvent contenir du texte libre et des templates.

**Annotations recommand√©es** :

```yaml
annotations:
  summary: "R√©sum√© court (< 100 caract√®res)"
  description: "Description d√©taill√©e avec contexte et valeurs"
  impact: "Impact sur les utilisateurs ou le business"
  action: "Actions recommand√©es pour r√©soudre"
  runbook_url: "https://wiki.example.com/runbooks/alerte"
  dashboard_url: "https://grafana.example.com/d/xxx"
```

### Templating dans les Annotations

Vous pouvez utiliser des templates Go pour ins√©rer des valeurs dynamiques :

**Variables disponibles** :
- `{{ $labels.nom_label }}` : Valeur d'un label
- `{{ $value }}` : Valeur num√©rique de l'alerte
- `{{ $value | humanize }}` : Valeur format√©e lisiblement
- `{{ $value | humanizePercentage }}` : Formatage en pourcentage
- `{{ $value | humanizeDuration }}` : Formatage en dur√©e

**Exemple complet** :

```yaml
annotations:
  summary: "CPU √©lev√© sur {{ $labels.instance }}"
  description: |
    Le n≈ìud {{ $labels.instance }} ({{ $labels.node }})
    a une utilisation CPU de {{ $value | humanizePercentage }}
    depuis {{ $activeAt | humanizeDuration }}.

    Valeur actuelle: {{ $value | humanize }}%
    Seuil configur√©: 85%

    Dashboard: https://grafana.example.com/d/nodes?var-node={{ $labels.node }}
  action: |
    1. V√©rifier les pods consommant le plus de CPU
    2. V√©rifier s'il y a un pic de trafic inhabituel
    3. Envisager de scaler horizontalement si n√©cessaire
```

## Organisation des Fichiers de R√®gles

### Structure Recommand√©e

Organisez vos r√®gles en plusieurs fichiers th√©matiques :

```
/etc/prometheus/rules/
‚îú‚îÄ‚îÄ node-alerts.yml          # Alertes li√©es aux n≈ìuds
‚îú‚îÄ‚îÄ pod-alerts.yml           # Alertes li√©es aux pods
‚îú‚îÄ‚îÄ service-alerts.yml       # Alertes li√©es aux services
‚îú‚îÄ‚îÄ storage-alerts.yml       # Alertes li√©es au stockage
‚îú‚îÄ‚îÄ network-alerts.yml       # Alertes r√©seau
‚îî‚îÄ‚îÄ business-alerts.yml      # Alertes m√©tier/applicatives
```

### Exemple : node-alerts.yml

```yaml
groups:
  - name: node_alerts
    interval: 30s
    rules:
      - alert: NodeDown
        expr: up{job="node-exporter"} == 0
        for: 2m
        labels:
          severity: critical
          component: node
        annotations:
          summary: "N≈ìud {{ $labels.instance }} inaccessible"
          description: "Le n≈ìud {{ $labels.instance }} ne r√©pond plus depuis 2 minutes."

      - alert: HighNodeCPU
        expr: (100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 85
        for: 15m
        labels:
          severity: warning
          component: node
        annotations:
          summary: "CPU √©lev√© sur {{ $labels.instance }}"
          description: "Utilisation CPU de {{ $value | humanize }}% depuis 15 minutes."

      - alert: HighNodeMemory
        expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100) < 10
        for: 5m
        labels:
          severity: warning
          component: node
        annotations:
          summary: "M√©moire faible sur {{ $labels.instance }}"
          description: "Seulement {{ $value | humanize }}% de m√©moire disponible."
```

### Exemple : pod-alerts.yml

```yaml
groups:
  - name: pod_alerts
    interval: 1m
    rules:
      - alert: PodNotReady
        expr: kube_pod_status_phase{phase!~"Running|Succeeded"} > 0
        for: 5m
        labels:
          severity: warning
          component: pod
        annotations:
          summary: "Pod {{ $labels.pod }} n'est pas Ready"
          description: |
            Le pod {{ $labels.pod }} dans {{ $labels.namespace }}
            est en phase {{ $labels.phase }} depuis 5 minutes.

      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: warning
          component: pod
        annotations:
          summary: "Pod {{ $labels.pod }} crash loop"
          description: "Le pod red√©marre fr√©quemment."

      - alert: HighPodMemory
        expr: |
          (
            container_memory_working_set_bytes{container!=""}
            /
            container_spec_memory_limit_bytes{container!=""} * 100
          ) > 90
        for: 10m
        labels:
          severity: critical
          component: pod
        annotations:
          summary: "M√©moire critique pour {{ $labels.pod }}"
          description: "Utilisation de {{ $value | humanize }}% - risque d'OOMKill."
```

## D√©ploiement des R√®gles sur MicroK8s

### M√©thode 1 : Via ConfigMap

C'est la m√©thode recommand√©e sur Kubernetes/MicroK8s.

**Cr√©er le ConfigMap** :

```bash
# Cr√©er un fichier avec vos r√®gles
cat > my-alerts.yml <<EOF
groups:
  - name: my_custom_alerts
    interval: 30s
    rules:
      - alert: ExampleAlert
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service down"
EOF

# Cr√©er le ConfigMap
microk8s kubectl create configmap prometheus-additional-rules \
  --from-file=my-alerts.yml \
  -n monitoring
```

**Configurer Prometheus pour charger ces r√®gles** :

Vous devez modifier la configuration de Prometheus pour qu'il charge ce ConfigMap. Cela d√©pend de votre installation, mais g√©n√©ralement :

```bash
# √âditer le StatefulSet ou Deployment de Prometheus
microk8s kubectl edit statefulset prometheus-k8s -n monitoring
```

Ajoutez un volume et un volumeMount pour votre ConfigMap.

### M√©thode 2 : Via PrometheusRule (Operator)

Si vous utilisez le Prometheus Operator, vous pouvez cr√©er un objet `PrometheusRule` :

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: my-custom-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus
spec:
  groups:
    - name: my_custom_alerts
      interval: 30s
      rules:
        - alert: ExampleAlert
          expr: up{job="my-service"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Service my-service is down"
            description: "Check the service immediately"
```

Appliquer la r√®gle :

```bash
microk8s kubectl apply -f my-prometheus-rule.yml
```

Le Prometheus Operator d√©tectera automatiquement cette nouvelle r√®gle et la chargera.

### V√©rifier que les R√®gles sont Charg√©es

**Via l'interface Prometheus** :

1. Exposer Prometheus :
```bash
microk8s kubectl port-forward -n monitoring svc/prometheus-k8s 9090:9090
```

2. Ouvrir `http://localhost:9090`

3. Aller dans **Status** ‚Üí **Rules**

Vous devriez voir toutes vos r√®gles list√©es.

**Via kubectl** :

```bash
# Voir les PrometheusRules
microk8s kubectl get prometheusrules -n monitoring

# D√©tail d'une r√®gle
microk8s kubectl describe prometheusrule my-custom-alerts -n monitoring
```

## Validation et Test des R√®gles

### Validation de la Syntaxe

Avant de d√©ployer, validez vos fichiers de r√®gles :

**Outil promtool** :

```bash
# Installer promtool (si pas d√©j√† pr√©sent)
# Sur le n≈ìud o√π Prometheus tourne, ou t√©l√©chargez-le

# Valider le fichier
promtool check rules my-alerts.yml
```

Si tout va bien, vous verrez :
```
Checking my-alerts.yml
  SUCCESS: 3 rules found
```

### Test des Expressions PromQL

Avant de cr√©er une alerte, testez votre expression PromQL dans l'interface Prometheus :

1. Ouvrir Prometheus UI : `http://localhost:9090`
2. Aller dans l'onglet **Graph**
3. Entrer votre expression PromQL
4. Cliquer sur **Execute**

V√©rifiez que :
- L'expression retourne des r√©sultats quand le probl√®me existe
- L'expression ne retourne rien quand tout va bien
- Les labels sont corrects

### Simuler une Alerte

Pour tester qu'une alerte se d√©clenche correctement :

**1. Provoquer volontairement la condition** :

Par exemple, pour tester une alerte de CPU :
```bash
# D√©ployer un pod qui consomme beaucoup de CPU
kubectl run cpu-stress --image=progrium/stress \
  -- --cpu 2 --timeout 60s
```

**2. V√©rifier dans Prometheus UI** :

- Aller dans **Alerts**
- Chercher votre alerte
- V√©rifier qu'elle passe √† l'√©tat **Pending** puis **Firing**

**3. V√©rifier dans Alertmanager** :

- Ouvrir Alertmanager UI : `http://localhost:9093`
- V√©rifier que l'alerte appara√Æt
- V√©rifier qu'une notification est envoy√©e

## Techniques PromQL Avanc√©es pour les Alertes

### Agr√©gations

**sum** : Additionner des valeurs
```yaml
expr: sum by(namespace) (kube_pod_container_resource_requests_memory_bytes)
```

**avg** : Moyenne
```yaml
expr: avg by(instance) (node_load1)
```

**max** : Maximum
```yaml
expr: max by(pod) (container_cpu_usage_seconds_total)
```

**count** : Compter
```yaml
expr: count(up{job="my-service"} == 0) > 3
# Alerte si plus de 3 instances sont down
```

### Fonctions Temporelles

**rate()** : Taux par seconde
```yaml
expr: rate(http_requests_total[5m]) > 1000
# Plus de 1000 requ√™tes/seconde sur 5 minutes
```

**irate()** : Taux instantan√© (plus sensible)
```yaml
expr: irate(node_network_receive_bytes_total[5m]) > 100000000
# Trafic r√©seau > 100MB/s
```

**increase()** : Augmentation totale
```yaml
expr: increase(http_requests_total[1h]) > 10000
# Plus de 10000 requ√™tes dans la derni√®re heure
```

### Op√©rations Math√©matiques

**Pourcentage** :
```yaml
expr: (m√©trique_utilis√©e / m√©trique_totale) * 100 > 80
```

**Pr√©diction** :
```yaml
expr: predict_linear(node_filesystem_free_bytes[1h], 4*3600) < 0
# Pr√©dit que le disque sera plein dans 4 heures
```

### Comparaisons Temporelles

**Comparer avec le pass√©** :
```yaml
expr: |
  rate(http_requests_total[5m])
  /
  rate(http_requests_total[5m] offset 1d)
  > 2
# Trafic 2x sup√©rieur √† la m√™me heure hier
```

### Conditions Multiples (AND, OR)

**AND** (les deux conditions doivent √™tre vraies) :
```yaml
expr: |
  (
    rate(http_requests_total{status="500"}[5m]) > 10
  )
  and
  (
    rate(http_requests_total[5m]) > 100
  )
# Erreurs 500 ET trafic √©lev√©
```

**OR** (au moins une condition vraie) :
```yaml
expr: |
  (node_filesystem_avail_bytes / node_filesystem_size_bytes * 100 < 10)
  or
  (predict_linear(node_filesystem_avail_bytes[1h], 4*3600) < 0)
# Disque faible OU sera bient√¥t plein
```

**UNLESS** (sauf si) :
```yaml
expr: |
  up{job="my-service"} == 0
  unless
  on(instance) kube_pod_status_phase{phase="Pending"}
# Service down sauf s'il est en d√©marrage
```

## Bonnes Pratiques de Configuration

### 1. Nommage des Alertes

**Bonne pratique** :
- Utilisez le CamelCase : `HighPodMemory`, `NodeDown`
- Commencez par la condition : `High...`, `Low...`, `Critical...`
- Soyez descriptif mais concis
- √âvitez les abr√©viations obscures

**Exemples** :
- ‚úì `HighPodCPUUsage`
- ‚úì `CertificateExpiringSoon`
- ‚úì `DatabaseConnectionPoolExhausted`
- ‚úó `alert1`
- ‚úó `prob_cpu`

### 2. D√©finir les Bonnes Dur√©es "for"

**R√®gle g√©n√©rale** :
- Critique + imm√©diat : `for: 1m` ou pas de `for`
- Critique + tol√©rable : `for: 5m`
- Warning : `for: 10m` √† `15m`
- Info : `for: 30m` √† `1h`

### 3. Calibrer les Seuils

**M√©thode** :
1. Observez vos m√©triques pendant 1-2 semaines
2. Identifiez les valeurs normales
3. D√©finissez des seuils avec une marge
4. Ajustez apr√®s les premi√®res alertes

**Exemple** :
- CPU normal : 20-40%
- Pic occasionnel : 60-70%
- Seuil warning : 80%
- Seuil critical : 95%

### 4. Alertes Multicouches

Cr√©ez plusieurs niveaux d'alerte pour le m√™me probl√®me :

```yaml
# Niveau 1 : Avertissement pr√©coce
- alert: HighDiskUsage
  expr: disk_used_percent > 75
  for: 30m
  labels:
    severity: warning

# Niveau 2 : Critique
- alert: CriticalDiskUsage
  expr: disk_used_percent > 90
  for: 5m
  labels:
    severity: critical
```

### 5. Documentation Obligatoire

Chaque alerte devrait avoir au minimum :
- `summary` : Ce qui se passe
- `description` : D√©tails et contexte
- `runbook_url` : Lien vers la proc√©dure de r√©solution

### 6. √âviter les Alertes Bruyantes

**Checklist** :
- ‚òê L'alerte indique-t-elle un probl√®me r√©el ?
- ‚òê L'alerte n√©cessite-t-elle une action humaine ?
- ‚òê L'action √† prendre est-elle document√©e ?
- ‚òê Le seuil est-il appropri√© ?
- ‚òê La dur√©e `for` √©vite-t-elle les faux positifs ?

Si vous r√©pondez "non" √† l'une de ces questions, revoyez votre alerte.

### 7. Tester Avant de D√©ployer en Production

**Process recommand√©** :
1. Tester en environnement de dev/staging
2. Observer pendant quelques jours
3. Ajuster les seuils si n√©cessaire
4. D√©ployer en production
5. Continuer √† affiner

## Exemples de R√®gles pour Cas Sp√©cifiques

### Application E-commerce

```yaml
groups:
  - name: ecommerce_alerts
    rules:
      - alert: CheckoutAPIHighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{endpoint="/checkout"}[5m])) > 2
        for: 5m
        labels:
          severity: critical
          team: checkout
        annotations:
          summary: "Latence checkout √©lev√©e"
          description: "Le checkout met {{ $value }}s (p95)"

      - alert: PaymentFailureRateHigh
        expr: |
          (
            sum(rate(payment_attempts_total{status="failed"}[5m]))
            /
            sum(rate(payment_attempts_total[5m]))
          ) * 100 > 5
        for: 10m
        labels:
          severity: critical
          team: payments
        annotations:
          summary: "Taux d'√©chec de paiement √©lev√©"
          description: "{{ $value }}% des paiements √©chouent"
```

### Base de Donn√©es

```yaml
groups:
  - name: database_alerts
    rules:
      - alert: DatabaseConnectionPoolNearLimit
        expr: (mysql_global_status_threads_connected / mysql_global_variables_max_connections) * 100 > 80
        for: 5m
        labels:
          severity: warning
          team: data
        annotations:
          summary: "Pool de connexions DB satur√©"
          description: "{{ $value }}% des connexions utilis√©es"

      - alert: DatabaseSlowQueries
        expr: rate(mysql_global_status_slow_queries[5m]) > 10
        for: 10m
        labels:
          severity: warning
          team: data
        annotations:
          summary: "Requ√™tes SQL lentes d√©tect√©es"
          description: "{{ $value }} requ√™tes lentes/seconde"
```

### File d'Attente (Queue)

```yaml
groups:
  - name: queue_alerts
    rules:
      - alert: QueueBacklogGrowing
        expr: rate(queue_depth[10m]) > 0
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "File d'attente {{ $labels.queue }} en croissance"
          description: "La profondeur de la queue augmente de {{ $value }} msgs/s"

      - alert: QueueProcessingStalled
        expr: rate(queue_messages_processed[5m]) == 0 and queue_depth > 0
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Traitement de la queue {{ $labels.queue }} bloqu√©"
          description: "Aucun message trait√© alors que {{ $value }} messages en attente"
```

## Maintenance et √âvolution des R√®gles

### Audit R√©gulier

**Tous les 3 mois** :
- R√©viser les alertes qui se d√©clenchent fr√©quemment
- Supprimer les alertes qui ne se d√©clenchent jamais
- Ajuster les seuils selon les nouvelles observations
- Mettre √† jour la documentation

### Versioning

Gardez vos r√®gles d'alerte dans un syst√®me de contr√¥le de version (Git) :

```bash
/monitoring-config/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ alerts/
‚îÇ   ‚îú‚îÄ‚îÄ v1.0/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ node-alerts.yml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pod-alerts.yml
‚îÇ   ‚îî‚îÄ‚îÄ v1.1/
‚îÇ       ‚îú‚îÄ‚îÄ node-alerts.yml
‚îÇ       ‚îî‚îÄ‚îÄ pod-alerts.yml
‚îî‚îÄ‚îÄ CHANGELOG.md
```

### Changelog

Documentez chaque changement :

```markdown
## [1.1.0] - 2025-01-15
### Added
- Alerte DatabaseConnectionPoolNearLimit
- Alerte QueueBacklogGrowing

### Changed
- HighPodMemory : seuil augment√© de 80% √† 85%
- PodCrashLooping : dur√©e r√©duite de 10m √† 5m

### Removed
- Alerte ObsoleteService (service supprim√©)
```

## M√©triques sur les Alertes

Cr√©ez des alertes pour surveiller votre syst√®me d'alerting lui-m√™me :

```yaml
groups:
  - name: meta_alerts
    rules:
      - alert: TooManyAlerts
        expr: count(ALERTS{alertstate="firing"}) > 20
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Trop d'alertes actives"
          description: "{{ $value }} alertes en cours - possible probl√®me syst√©mique"

      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Alertmanager inaccessible"
          description: "Le syst√®me d'alerting ne fonctionne plus !"

      - alert: AlertNeverFiring
        expr: time() - max(ALERTS_FOR_STATE{alertstate="firing"}) by(alertname) > 86400 * 30
        labels:
          severity: info
        annotations:
          summary: "L'alerte {{ $labels.alertname }} ne s'est jamais d√©clench√©e"
          description: "Peut-√™tre inutile ou mal configur√©e ?"
```

## Conclusion

La configuration des r√®gles d'alerte est un art qui s'affine avec l'exp√©rience. Les points essentiels :

- **Commencez simple** : Quelques alertes essentielles bien configur√©es valent mieux que des dizaines d'alertes bruyantes
- **It√©rez** : Ajustez vos r√®gles en fonction des retours
- **Documentez** : Chaque alerte doit avoir une proc√©dure de r√©solution
- **Testez** : Validez vos alertes avant de les d√©ployer en production
- **Maintenez** : R√©visez r√©guli√®rement vos r√®gles

Avec des r√®gles d'alerte bien pens√©es, vous transformez votre monitoring passif en syst√®me proactif qui prot√®ge vos services.

## Points Cl√©s √† Retenir

‚úì Une r√®gle d'alerte = nom + expression PromQL + dur√©e + labels + annotations

‚úì Le param√®tre `for` √©vite les fausses alertes en attendant que le probl√®me persiste

‚úì Les labels servent au routage, les annotations √† la documentation

‚úì Organisez vos r√®gles en groupes th√©matiques dans des fichiers s√©par√©s

‚úì Testez vos expressions PromQL avant de cr√©er les alertes

‚úì Calibrez vos seuils en observant vos m√©triques normales

‚úì Cr√©ez des alertes multicouches (warning ‚Üí critical)

‚úì Documentez chaque alerte avec un runbook

‚úì Versionez vos r√®gles et auditez-les r√©guli√®rement

‚úì Surveillez votre syst√®me d'alerting lui-m√™me

---

**Prochaine section** : 14.4 Routing et grouping - Nous approfondirons les strat√©gies de routage et de regroupement dans Alertmanager.

‚è≠Ô∏è [Routing et grouping](/14-alerting-et-notifications/04-routing-et-grouping.md)
