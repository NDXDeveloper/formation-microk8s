üîù Retour au [Sommaire](/SOMMAIRE.md)

# 14.2 Prometheus Alertmanager

## Introduction

Maintenant que vous comprenez les concepts fondamentaux de l'alerting, il est temps de d√©couvrir **Prometheus Alertmanager**, le composant qui transforme vos alertes en notifications concr√®tes. Si Prometheus est le d√©tective qui identifie les probl√®mes, Alertmanager est le messager qui vous informe de la mani√®re la plus appropri√©e.

Alertmanager est bien plus qu'un simple service d'envoi de notifications. C'est un syst√®me sophistiqu√© capable de :
- Regrouper les alertes similaires
- √âviter les notifications en double
- Router les alertes vers les bonnes personnes
- G√©rer les p√©riodes de silence
- Escalader les alertes non trait√©es

## Architecture d'Alertmanager

### Le Flux de Donn√©es

Voici comment les alertes circulent dans votre infrastructure :

```
[Prometheus] ‚Üí √âvalue les r√®gles d'alerte
     ‚Üì
[Alertmanager] ‚Üí Re√ßoit les alertes
     ‚Üì
Grouping ‚Üí Regroupe les alertes similaires
     ‚Üì
Throttling ‚Üí √âvite les notifications r√©p√©t√©es
     ‚Üì
Silencing ‚Üí Applique les silences d√©finis
     ‚Üì
Routing ‚Üí Dirige vers les bons canaux
     ‚Üì
[Notifications] ‚Üí Email, Slack, PagerDuty, etc.
```

### Les Composants Internes

**Receiver** : La configuration d'un canal de notification (email, Slack, webhook, etc.)

**Route** : Les r√®gles qui d√©terminent quel receiver utiliser pour quelle alerte

**Inhibition** : Les r√®gles qui suppriment certaines alertes en fonction d'autres

**Silence** : Les p√©riodes pendant lesquelles certaines alertes sont temporairement d√©sactiv√©es

**Grouping** : Le m√©canisme qui regroupe plusieurs alertes en une seule notification

## Installation sur MicroK8s

La bonne nouvelle : si vous avez d√©j√† activ√© l'addon Prometheus sur MicroK8s, Alertmanager est probablement d√©j√† install√© !

### V√©rifier si Alertmanager est Pr√©sent

```bash
microk8s kubectl get pods -n monitoring
```

Vous devriez voir un pod nomm√© `alertmanager-...` s'il est d√©j√† d√©ploy√©.

### Installation avec l'Addon Prometheus

Si vous n'avez pas encore activ√© Prometheus :

```bash
microk8s enable prometheus
```

Cette commande installe :
- Prometheus Server
- Prometheus Alertmanager
- Grafana
- Node Exporter
- Kube-state-metrics

### Acc√©der √† l'Interface Web d'Alertmanager

Alertmanager dispose d'une interface web pour visualiser les alertes actives et les silences.

**V√©rifier le service** :
```bash
microk8s kubectl get svc -n monitoring | grep alertmanager
```

**Exposer temporairement l'interface** :
```bash
microk8s kubectl port-forward -n monitoring svc/alertmanager-operated 9093:9093
```

Puis ouvrez votre navigateur sur : `http://localhost:9093`

### Exploration de l'Interface Web

L'interface d'Alertmanager comporte plusieurs sections :

**Alerts** : Affiche toutes les alertes actives (firing), regroup√©es selon votre configuration

**Silences** : G√®re les silences actifs et permet d'en cr√©er de nouveaux

**Status** : Affiche la configuration en cours et l'√©tat du cluster Alertmanager

## Configuration de Base

La configuration d'Alertmanager se fait via un fichier YAML. Dans MicroK8s, ce fichier est g√©n√©ralement stock√© dans un ConfigMap.

### Structure du Fichier de Configuration

```yaml
global:
  # Configuration globale (serveurs SMTP, URLs API, etc.)

route:
  # Arbre de routage des alertes

receivers:
  # D√©finition des canaux de notification

inhibit_rules:
  # R√®gles de suppression d'alertes redondantes
```

### Localiser la Configuration dans MicroK8s

```bash
microk8s kubectl get configmap -n monitoring | grep alertmanager
```

Pour voir la configuration actuelle :
```bash
microk8s kubectl get configmap alertmanager-config -n monitoring -o yaml
```

## Les Receivers : Canaux de Notification

Un **receiver** d√©finit comment et o√π envoyer les notifications. Alertmanager supporte de nombreux types de receivers.

### Types de Receivers Disponibles

**Email** : Notifications par email via SMTP

**Slack** : Messages dans des canaux Slack

**PagerDuty** : Int√©gration avec le syst√®me d'astreinte PagerDuty

**Webhook** : Envoi vers une URL HTTP personnalis√©e

**OpsGenie, VictorOps, WeChat, Telegram** : Autres int√©grations populaires

### Exemple : Receiver Email

```yaml
receivers:
  - name: 'email-admin'
    email_configs:
      - to: 'admin@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alertmanager@example.com'
        auth_password: 'votre-mot-de-passe'
        require_tls: true
```

**Explication** :
- `name` : Identifiant unique du receiver
- `to` : Adresse email du destinataire
- `from` : Adresse email de l'exp√©diteur
- `smarthost` : Serveur SMTP et port
- `auth_username/auth_password` : Identifiants SMTP
- `require_tls` : Utilisation de TLS (recommand√©)

### Exemple : Receiver Slack

```yaml
receivers:
  - name: 'slack-alerts'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/XXXXX/YYYYY/ZZZZZ'
        channel: '#alerts'
        title: 'Alerte Kubernetes'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
```

**Explication** :
- `api_url` : Webhook URL fournie par Slack (Incoming Webhooks)
- `channel` : Canal Slack de destination
- `title` : Titre du message
- `text` : Corps du message (utilise le templating Go)

### Exemple : Receiver Webhook

```yaml
receivers:
  - name: 'webhook-custom'
    webhook_configs:
      - url: 'http://mon-service:8080/alerts'
        send_resolved: true
```

**Explication** :
- `url` : URL de votre endpoint qui recevra les alertes au format JSON
- `send_resolved` : Envoie aussi une notification quand l'alerte est r√©solue

## Le Routage des Alertes

Le **routing** est le m√©canisme qui d√©termine quel receiver utiliser pour chaque alerte. C'est comme un syst√®me d'aiguillage ferroviaire pour vos notifications.

### Structure d'une Route

Une route est d√©finie par :
- **match** ou **match_re** : Conditions pour qu'une alerte corresponde √† cette route
- **receiver** : Le receiver √† utiliser si l'alerte correspond
- **continue** : Si `true`, continue √† √©valuer les routes suivantes
- **routes** : Routes enfants (sous-routes)

### Exemple Simple : Routage par S√©v√©rit√©

```yaml
route:
  receiver: 'default-receiver'
  group_by: ['alertname', 'cluster']
  group_wait: 10s
  group_interval: 5m
  repeat_interval: 3h

  routes:
    - match:
        severity: 'critical'
      receiver: 'pagerduty-critical'

    - match:
        severity: 'warning'
      receiver: 'slack-warnings'

    - match:
        severity: 'info'
      receiver: 'email-info'
```

**Explication** :
- Route par d√©faut : Utilise `default-receiver` pour toutes les alertes
- Route 1 : Les alertes critiques vont vers PagerDuty
- Route 2 : Les warnings vont vers Slack
- Route 3 : Les infos vont par email

### Exemple Avanc√© : Routage Multi-crit√®res

```yaml
route:
  receiver: 'default'

  routes:
    # Alertes critiques de production
    - match:
        severity: 'critical'
        environment: 'production'
      receiver: 'pagerduty-oncall'
      continue: false

    # Alertes de l'√©quipe backend
    - match_re:
        service: '(api|database|cache)'
      receiver: 'slack-backend-team'
      continue: true

    # Alertes de l'√©quipe frontend
    - match_re:
        service: '(webapp|mobile-app)'
      receiver: 'slack-frontend-team'
```

**Explication** :
- Les alertes critiques en production vont directement √† l'astreinte
- `continue: false` emp√™che l'√©valuation des routes suivantes
- Les routes backend et frontend utilisent des regex pour matcher plusieurs services
- `continue: true` permet d'envoyer une alerte √† plusieurs receivers

## Le Grouping : Regroupement d'Alertes

Le **grouping** √©vite que vous receviez 50 notifications individuelles quand 50 pods d'une m√™me application ont un probl√®me. Au lieu de cela, vous recevez une seule notification group√©e.

### Param√®tres de Grouping

**group_by** : Liste des labels utilis√©s pour regrouper les alertes
```yaml
group_by: ['alertname', 'namespace', 'cluster']
```
Les alertes ayant les m√™mes valeurs pour ces labels seront regroup√©es.

**group_wait** : Temps d'attente avant d'envoyer la premi√®re notification d'un nouveau groupe
```yaml
group_wait: 10s
```
Attend 10 secondes pour rassembler les alertes similaires avant d'envoyer.

**group_interval** : Temps d'attente avant d'envoyer des mises √† jour pour un groupe existant
```yaml
group_interval: 5m
```
Si de nouvelles alertes arrivent dans un groupe existant, attend 5 minutes avant de notifier.

**repeat_interval** : Intervalle entre les r√©p√©titions de notification
```yaml
repeat_interval: 4h
```
Renvoie une notification toutes les 4 heures tant que l'alerte est active.

### Exemple Pratique de Grouping

Imaginons ces alertes qui se d√©clenchent simultan√©ment :
- `HighCPU` sur le pod `api-1` dans `production`
- `HighCPU` sur le pod `api-2` dans `production`
- `HighCPU` sur le pod `api-3` dans `production`

Avec `group_by: ['alertname', 'cluster']`, vous recevrez **une seule notification** qui liste les 3 pods concern√©s, plut√¥t que 3 notifications s√©par√©es.

## Les Inhibitions : Supprimer les Alertes Redondantes

Les **inhibitions** permettent de supprimer certaines alertes lorsque d'autres alertes plus importantes sont actives. C'est comme une r√®gle de priorit√©.

### Cas d'Usage Typique

Si un serveur entier est down, pas besoin d'√™tre alert√© sur tous les services qui tournent dessus - vous le savez d√©j√†.

### Structure d'une R√®gle d'Inhibition

```yaml
inhibit_rules:
  - source_match:
      severity: 'critical'
      alertname: 'NodeDown'
    target_match:
      severity: 'warning'
    target_match_re:
      alertname: '(ServiceUnavailable|HighLatency)'
    equal: ['node']
```

**Explication** :
- **source_match** : L'alerte "source" qui, si active, inhibe les autres
- **target_match** : Les alertes "cibles" qui seront inhib√©es
- **equal** : Les labels qui doivent √™tre identiques entre source et cible
- Ici : Si `NodeDown` est actif sur un n≈ìud, les alertes `ServiceUnavailable` et `HighLatency` sur ce m√™me n≈ìud sont inhib√©es

### Exemple Pratique

```yaml
inhibit_rules:
  # Si le cluster est down, ne pas alerter sur les pods
  - source_match:
      alertname: 'ClusterDown'
    target_match_re:
      alertname: 'Pod.*'
    equal: ['cluster']

  # Si une alerte critique est active, masquer les warnings similaires
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'namespace']
```

## Les Silences : D√©sactiver Temporairement des Alertes

Un **silence** permet de d√©sactiver temporairement certaines alertes, typiquement pendant :
- Une maintenance planifi√©e
- Un d√©ploiement
- Une investigation en cours

### Cr√©er un Silence via l'Interface Web

1. Acc√©dez √† l'interface web d'Alertmanager : `http://localhost:9093`
2. Cliquez sur l'onglet **"Silences"**
3. Cliquez sur **"New Silence"**
4. Configurez les matchers (ex: `alertname=HighCPU`)
5. D√©finissez la dur√©e (ex: 2 heures)
6. Ajoutez un commentaire expliquant pourquoi
7. Cliquez sur **"Create"**

### Cr√©er un Silence via l'API

```bash
curl -X POST http://localhost:9093/api/v2/silences \
  -H 'Content-Type: application/json' \
  -d '{
    "matchers": [
      {
        "name": "alertname",
        "value": "HighCPU",
        "isRegex": false
      },
      {
        "name": "namespace",
        "value": "production",
        "isRegex": false
      }
    ],
    "startsAt": "2025-01-01T10:00:00.000Z",
    "endsAt": "2025-01-01T12:00:00.000Z",
    "createdBy": "admin",
    "comment": "Maintenance planifi√©e"
  }'
```

### Bonnes Pratiques pour les Silences

‚úì Toujours ajouter un commentaire expliquant la raison

‚úì D√©finir une dur√©e appropri√©e (pas trop longue)

‚úì Utiliser des matchers sp√©cifiques (√©viter de silencer trop largement)

‚úì Nettoyer les silences une fois la maintenance termin√©e

‚úó Ne jamais cr√©er de silence permanent pour masquer un probl√®me r√©current

## Templates de Notifications

Alertmanager utilise le syst√®me de templating de Go pour personnaliser le contenu des notifications.

### Variables Disponibles

Dans vos templates, vous pouvez utiliser :

- `.Alerts` : Liste de toutes les alertes dans le groupe
- `.GroupLabels` : Les labels utilis√©s pour grouper
- `.CommonLabels` : Les labels communs √† toutes les alertes du groupe
- `.CommonAnnotations` : Les annotations communes
- `.ExternalURL` : L'URL d'Alertmanager
- `.Status` : √âtat du groupe (firing/resolved)

### Exemple de Template pour Slack

```yaml
receivers:
  - name: 'slack-custom'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/XXX'
        channel: '#alerts'
        title: '{{ .Status | toUpper }} : {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alerte:* {{ .Labels.alertname }}
          *S√©v√©rit√©:* {{ .Labels.severity }}
          *Description:* {{ .Annotations.description }}
          *Pod:* {{ .Labels.pod }}
          *Namespace:* {{ .Labels.namespace }}
          {{ end }}
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
```

**R√©sultat** : Messages Slack avec des couleurs (rouge pour firing, vert pour resolved) et des informations structur√©es.

### Exemple de Template pour Email

```yaml
receivers:
  - name: 'email-detailed'
    email_configs:
      - to: 'oncall@example.com'
        headers:
          Subject: '[{{ .Status }}] {{ .GroupLabels.alertname }}'
        html: |
          <!DOCTYPE html>
          <html>
            <body>
              <h2>Alertes Kubernetes</h2>
              <p><strong>Statut:</strong> {{ .Status }}</p>
              <table border="1">
                <tr>
                  <th>Alerte</th>
                  <th>S√©v√©rit√©</th>
                  <th>Description</th>
                </tr>
                {{ range .Alerts }}
                <tr>
                  <td>{{ .Labels.alertname }}</td>
                  <td>{{ .Labels.severity }}</td>
                  <td>{{ .Annotations.description }}</td>
                </tr>
                {{ end }}
              </table>
            </body>
          </html>
```

## Configuration Compl√®te : Exemple R√©aliste

Voici un exemple de configuration Alertmanager complet et r√©aliste pour un environnement de production :

```yaml
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alertmanager@example.com'
  smtp_auth_username: 'alertmanager@example.com'
  smtp_auth_password: 'votre-mot-de-passe'
  slack_api_url: 'https://hooks.slack.com/services/XXX/YYY/ZZZ'

# Route principale
route:
  receiver: 'default-email'
  group_by: ['alertname', 'cluster', 'namespace']
  group_wait: 10s
  group_interval: 10m
  repeat_interval: 12h

  routes:
    # Alertes critiques en production
    - match:
        severity: 'critical'
        environment: 'production'
      receiver: 'pagerduty-oncall'
      group_wait: 10s
      repeat_interval: 5m
      continue: false

    # Warnings en production
    - match:
        severity: 'warning'
        environment: 'production'
      receiver: 'slack-prod-warnings'
      group_wait: 30s
      repeat_interval: 3h

    # Alertes de staging
    - match:
        environment: 'staging'
      receiver: 'slack-staging'
      group_wait: 5m
      repeat_interval: 24h

    # Infos (pas urgent)
    - match:
        severity: 'info'
      receiver: 'email-info'
      group_wait: 10m
      repeat_interval: 24h

# D√©finition des receivers
receivers:
  - name: 'default-email'
    email_configs:
      - to: 'team@example.com'

  - name: 'pagerduty-oncall'
    pagerduty_configs:
      - service_key: 'votre-service-key'

  - name: 'slack-prod-warnings'
    slack_configs:
      - channel: '#prod-alerts'
        title: 'Warning Production'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'slack-staging'
    slack_configs:
      - channel: '#staging-alerts'

  - name: 'email-info'
    email_configs:
      - to: 'info@example.com'

# R√®gles d'inhibition
inhibit_rules:
  # Si un n≈ìud est down, ne pas alerter sur les pods
  - source_match:
      alertname: 'NodeDown'
    target_match_re:
      alertname: '(PodCrashLooping|HighMemory|HighCPU)'
    equal: ['node']

  # Si critique, masquer les warnings similaires
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'namespace']
```

## Appliquer une Nouvelle Configuration

### M√©thode 1 : Modifier le ConfigMap

```bash
# √âditer le ConfigMap
microk8s kubectl edit configmap alertmanager-config -n monitoring

# Recharger la configuration sans red√©marrer
microk8s kubectl exec -n monitoring alertmanager-0 -- \
  curl -X POST http://localhost:9093/-/reload
```

### M√©thode 2 : Cr√©er un Nouveau ConfigMap

```bash
# Cr√©er le ConfigMap depuis un fichier
microk8s kubectl create configmap alertmanager-config \
  --from-file=alertmanager.yml=./mon-config.yml \
  -n monitoring \
  --dry-run=client -o yaml | microk8s kubectl apply -f -

# Red√©marrer les pods Alertmanager
microk8s kubectl rollout restart statefulset alertmanager -n monitoring
```

## V√©rification et D√©bogage

### V√©rifier que la Configuration est Valide

```bash
# V√©rifier les logs d'Alertmanager
microk8s kubectl logs -n monitoring alertmanager-0 --tail=50

# V√©rifier le statut via l'API
curl http://localhost:9093/api/v2/status
```

### Tester une Alerte Manuellement

Vous pouvez envoyer une alerte de test √† Alertmanager :

```bash
curl -X POST http://localhost:9093/api/v2/alerts \
  -H 'Content-Type: application/json' \
  -d '[
    {
      "labels": {
        "alertname": "TestAlert",
        "severity": "warning",
        "instance": "test"
      },
      "annotations": {
        "summary": "Ceci est un test",
        "description": "Alerte de test pour v√©rifier la configuration"
      }
    }
  ]'
```

### Probl√®mes Courants et Solutions

**Probl√®me** : Les notifications ne sont pas envoy√©es

**Solutions** :
- V√©rifiez les logs d'Alertmanager
- V√©rifiez que le receiver est correctement configur√©
- Testez la connectivit√© r√©seau (SMTP, webhooks)
- V√©rifiez les credentials (mot de passe SMTP, tokens API)

**Probl√®me** : Trop de notifications (spam)

**Solutions** :
- Augmentez `group_interval` et `repeat_interval`
- Utilisez le grouping plus agressivement
- Ajoutez des inhibitions pour supprimer les alertes redondantes
- Revoyez vos seuils d'alerte dans Prometheus

**Probl√®me** : Les alertes ne sont pas rout√©es correctement

**Solutions** :
- V√©rifiez que les labels de vos alertes correspondent aux matchers
- V√©rifiez l'ordre de vos routes (premi√®re route qui match gagne)
- Utilisez `continue: true` si vous voulez plusieurs notifications
- Testez vos regex avec des outils en ligne

## Haute Disponibilit√© d'Alertmanager

Pour un environnement critique, vous pouvez d√©ployer plusieurs instances d'Alertmanager en cluster.

### Principe du Clustering

Plusieurs instances d'Alertmanager se synchronisent entre elles pour :
- Partager l'√©tat des silences
- D√©duplication des notifications
- Tol√©rance aux pannes

### Configuration de Base du Cluster

```yaml
alertmanager:
  replicas: 3
  cluster:
    peers:
      - alertmanager-0.alertmanager:9094
      - alertmanager-1.alertmanager:9094
      - alertmanager-2.alertmanager:9094
```

Avec 3 instances, si une tombe en panne, les deux autres continuent de fonctionner.

## Bonnes Pratiques de Production

### 1. Strat√©gie de Routage Multi-niveaux

Organisez vos routes par :
- Environnement (prod > staging > dev)
- S√©v√©rit√© (critical > warning > info)
- √âquipe (backend, frontend, infra)

### 2. Notifications Graduelles

- **Critical** : Notification imm√©diate (PagerDuty, appel)
- **Warning** : Notification pendant les heures de travail (Slack, email)
- **Info** : Notification passive (email quotidien group√©)

### 3. Documentation et Runbooks

Chaque alerte devrait avoir :
- Un lien vers un runbook dans les annotations
- Une description claire du probl√®me
- Les √©tapes de r√©solution sugg√©r√©es

### 4. Monitoring d'Alertmanager Lui-m√™me

Cr√©ez des alertes pour surveiller Alertmanager :
- Alertmanager down
- Configuration invalide
- √âchec d'envoi de notifications

### 5. Tests R√©guliers

- Testez vos canaux de notification mensuellement
- Simulez des alertes de chaque niveau de s√©v√©rit√©
- V√©rifiez le routage et l'escalade
- Pratiquez votre proc√©dure d'astreinte

## Int√©grations Populaires

### PagerDuty

Service d'astreinte professionnel avec escalade automatique.

```yaml
receivers:
  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'votre-integration-key'
        description: '{{ .GroupLabels.alertname }}'
```

### Slack

Communication d'√©quipe avec notifications riches.

```yaml
receivers:
  - name: 'slack'
    slack_configs:
      - api_url: 'webhook-url'
        channel: '#alerts'
        username: 'Alertmanager'
        icon_emoji: ':fire:'
```

### Microsoft Teams

```yaml
receivers:
  - name: 'teams'
    webhook_configs:
      - url: 'teams-webhook-url'
        send_resolved: true
```

### OpsGenie

Plateforme d'incident management.

```yaml
receivers:
  - name: 'opsgenie'
    opsgenie_configs:
      - api_key: 'votre-api-key'
        teams: 'backend-team'
```

## M√©triques d'Alertmanager

Alertmanager expose ses propres m√©triques que Prometheus peut scraper :

- `alertmanager_notifications_total` : Nombre total de notifications envoy√©es
- `alertmanager_notifications_failed_total` : Notifications √©chou√©es
- `alertmanager_alerts` : Nombre d'alertes actives
- `alertmanager_silences` : Nombre de silences actifs

Ces m√©triques vous permettent de surveiller la sant√© de votre syst√®me d'alerting.

## Conclusion

Prometheus Alertmanager est un composant puissant et flexible qui transforme vos m√©triques en actions concr√®tes. Les concepts cl√©s √† retenir :

- **Receivers** : O√π envoyer les notifications
- **Routing** : Comment diriger les alertes
- **Grouping** : Regrouper pour √©viter le spam
- **Inhibitions** : Supprimer les alertes redondantes
- **Silences** : D√©sactiver temporairement pendant les maintenances

Avec une configuration bien pens√©e, Alertmanager devient votre alli√© pour maintenir vos services en bonne sant√© tout en pr√©servant votre tranquillit√© d'esprit.

## Points Cl√©s √† Retenir

‚úì Alertmanager g√®re les notifications envoy√©es par Prometheus

‚úì Les receivers d√©finissent les canaux de notification (email, Slack, etc.)

‚úì Le routing dirige les alertes vers les bons destinataires

‚úì Le grouping √©vite le spam en regroupant les alertes similaires

‚úì Les inhibitions suppriment les alertes redondantes

‚úì Les silences d√©sactivent temporairement certaines alertes

‚úì Les templates permettent de personnaliser les messages

‚úì Testez r√©guli√®rement votre configuration d'alerting

‚úì Documentez vos alertes avec des runbooks

‚úì Surveillez Alertmanager lui-m√™me avec des m√©triques

---

**Prochaine section** : 14.3 Configuration des r√®gles d'alerte - Nous apprendrons √† cr√©er des r√®gles d'alerte dans Prometheus.

‚è≠Ô∏è [Configuration des r√®gles d'alerte](/14-alerting-et-notifications/03-configuration-des-regles-dalerte.md)
