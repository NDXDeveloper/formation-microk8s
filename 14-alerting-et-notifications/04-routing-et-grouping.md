üîù Retour au [Sommaire](/SOMMAIRE.md)

# 14.4 Routing et Grouping

## Introduction

Le **routing** (routage) et le **grouping** (regroupement) sont deux m√©canismes fondamentaux d'Alertmanager qui d√©terminent **comment** et **quand** vos alertes sont envoy√©es. Bien ma√Ætris√©s, ils transforment un flux chaotique d'alertes individuelles en notifications organis√©es et pertinentes.

Imaginez un syst√®me sans routing ni grouping : vous recevriez une notification s√©par√©e pour chaque alerte, potentiellement des centaines par jour, sans distinction de priorit√©. Avec une bonne configuration, vous recevez √† la place des notifications group√©es, intelligemment rout√©es vers les bonnes personnes, au bon moment.

## Le Routing : Diriger les Alertes

Le routing est le syst√®me qui d√©cide **o√π envoyer** chaque alerte en fonction de ses caract√©ristiques (labels).

### Analogie : Le Tri Postal

Pensez au routing comme un centre de tri postal :
- Chaque alerte est une lettre
- Les labels sont l'adresse
- Les routes sont les r√®gles de tri
- Les receivers sont les destinations finales

Une lettre adress√©e √† Paris va √† Paris, une lettre urgente prend un canal prioritaire - c'est exactement ce que fait le routing avec vos alertes.

### Concepts Fondamentaux

**Route** : Une r√®gle qui dit "si l'alerte correspond √† ces crit√®res, envoie-la √† ce receiver"

**Match** : Les crit√®res pour qu'une alerte corresponde √† une route

**Receiver** : La destination finale (email, Slack, PagerDuty, etc.)

**Default Route** : La route par d√©faut utilis√©e si aucune autre route ne correspond

**Continue** : Permet √† une alerte d'√™tre trait√©e par plusieurs routes

## Structure d'une Configuration de Routing

### Configuration Minimale

Voici la structure la plus simple possible :

```yaml
route:
  receiver: 'default-receiver'

receivers:
  - name: 'default-receiver'
    email_configs:
      - to: 'admin@example.com'
```

**Explication** :
- Toutes les alertes vont vers `default-receiver`
- `default-receiver` envoie un email √† admin@example.com
- Aucun tri, aucune logique

### Configuration avec Routes Multiples

```yaml
route:
  receiver: 'default-receiver'
  routes:
    - match:
        severity: 'critical'
      receiver: 'pagerduty'

    - match:
        severity: 'warning'
      receiver: 'slack'

receivers:
  - name: 'default-receiver'
    email_configs:
      - to: 'team@example.com'

  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'xxx'

  - name: 'slack'
    slack_configs:
      - channel: '#alerts'
```

**Explication** :
- Route 1 : Les alertes `critical` vont vers PagerDuty
- Route 2 : Les alertes `warning` vont vers Slack
- Autres : Vont vers l'email par d√©faut

### L'Arbre de Routage

Le routing fonctionne comme un **arbre de d√©cision**. Alertmanager √©value les routes dans l'ordre et utilise la premi√®re qui correspond.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Alerte arrive  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Route par d√©faut   ‚îÇ ‚îÄ‚îÄ‚ñ∫ Receiver par d√©faut
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚ñ∫ Route 1: severity=critical ? ‚îÄ‚îÄ‚ñ∫ PagerDuty
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚ñ∫ Route 2: severity=warning ?  ‚îÄ‚îÄ‚ñ∫ Slack
         ‚îÇ
         ‚îî‚îÄ‚îÄ‚ñ∫ Route 3: team=frontend ?     ‚îÄ‚îÄ‚ñ∫ Email frontend
```

## Match : Filtrer les Alertes

Le `match` d√©termine si une alerte correspond √† une route. Il y a deux types de match.

### Match Exact

Compare la valeur d'un label avec une cha√Æne exacte :

```yaml
match:
  severity: 'critical'
  environment: 'production'
```

**Se d√©clenche si** :
- Le label `severity` vaut exactement `critical`
- **ET** le label `environment` vaut exactement `production`

**Ne se d√©clenche PAS si** :
- `severity: 'Critical'` (casse diff√©rente)
- `severity: 'critical-high'` (valeur diff√©rente)
- `environment: 'prod'` (valeur diff√©rente)

### Match avec Regex (match_re)

Compare la valeur d'un label avec une expression r√©guli√®re :

```yaml
match_re:
  severity: '(critical|warning)'
  service: 'api.*'
```

**Se d√©clenche si** :
- Le label `severity` vaut `critical` OU `warning`
- **ET** le label `service` commence par `api` (api-v1, api-backend, etc.)

### Exemples de Patterns Regex Utiles

**Commence par** :
```yaml
match_re:
  service: '^frontend.*'
# Match: frontend, frontend-api, frontend-web
```

**Termine par** :
```yaml
match_re:
  pod: '.*-production$'
# Match: app-production, web-production
```

**Contient** :
```yaml
match_re:
  namespace: '.*-prod-.*'
# Match: team-prod-eu, service-prod-us
```

**Liste de valeurs** :
```yaml
match_re:
  team: '(backend|data|infra)'
# Match: backend, data, ou infra
```

**Exclure des valeurs** :
```yaml
match_re:
  environment: '(?!dev|test).*'
# Match: tout sauf dev et test
```

## Hi√©rarchie de Routes

Les routes peuvent √™tre **imbriqu√©es** pour cr√©er une logique complexe.

### Exemple : Routage Multi-niveaux

```yaml
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster']

  routes:
    # Niveau 1 : Par environnement
    - match:
        environment: 'production'
      receiver: 'prod-default'

      routes:
        # Niveau 2 : Par s√©v√©rit√© en production
        - match:
            severity: 'critical'
          receiver: 'prod-pagerduty'

        - match:
            severity: 'warning'
          receiver: 'prod-slack'

    # Staging : moins urgent
    - match:
        environment: 'staging'
      receiver: 'staging-slack'
      group_wait: 5m
      repeat_interval: 12h
```

**Logique** :
1. Si `environment=production` :
   - Et `severity=critical` ‚Üí PagerDuty (urgent)
   - Et `severity=warning` ‚Üí Slack production
   - Sinon ‚Üí Email production par d√©faut
2. Si `environment=staging` ‚Üí Slack staging (moins urgent)
3. Sinon ‚Üí Email par d√©faut

### Visualisation de l'Arbre

```
Route racine (default)
‚îÇ
‚îú‚îÄ‚ñ∫ production
‚îÇ   ‚îú‚îÄ‚ñ∫ critical ‚Üí PagerDuty
‚îÇ   ‚îú‚îÄ‚ñ∫ warning  ‚Üí Slack prod
‚îÇ   ‚îî‚îÄ‚ñ∫ autres   ‚Üí Email prod
‚îÇ
‚îú‚îÄ‚ñ∫ staging ‚Üí Slack staging
‚îÇ
‚îî‚îÄ‚ñ∫ autres ‚Üí Email default
```

## Le Param√®tre Continue

Par d√©faut, Alertmanager s'arr√™te √† la premi√®re route qui correspond. Le param√®tre `continue: true` permet de continuer l'√©valuation.

### Sans Continue (comportement par d√©faut)

```yaml
routes:
  - match:
      severity: 'critical'
    receiver: 'pagerduty'
    # continue: false (par d√©faut)

  - match:
      team: 'backend'
    receiver: 'slack-backend'
```

**R√©sultat** : Une alerte `critical` du `team=backend` va **uniquement** vers PagerDuty.

### Avec Continue

```yaml
routes:
  - match:
      severity: 'critical'
    receiver: 'pagerduty'
    continue: true  # Continue l'√©valuation

  - match:
      team: 'backend'
    receiver: 'slack-backend'
```

**R√©sultat** : Une alerte `critical` du `team=backend` va vers **PagerDuty ET Slack**.

### Cas d'Usage pour Continue

**Notifications multiples** : Envoyer les alertes critiques √† la fois √† l'astreinte ET au canal Slack de l'√©quipe

**Logging centralis√©** : Toutes les alertes vont vers un syst√®me de logging en plus de leur destination normale

**Escalade** : Alertes importantes notifi√©es √† plusieurs niveaux

### Exemple Pratique

```yaml
route:
  receiver: 'default'

  routes:
    # Toutes les alertes critiques vont en astreinte
    - match:
        severity: 'critical'
      receiver: 'oncall-pagerduty'
      continue: true

    # ET aussi vers l'√©quipe concern√©e
    - match:
        team: 'backend'
      receiver: 'slack-backend'

    - match:
        team: 'frontend'
      receiver: 'slack-frontend'

    # ET toutes les alertes sont logg√©es
    - match: {}  # Match toutes les alertes
      receiver: 'webhook-logging'
```

**R√©sultat** : Une alerte `critical` de l'√©quipe `backend` sera envoy√©e √† :
1. PagerDuty (astreinte)
2. Slack backend (√©quipe)
3. Webhook de logging

## Le Grouping : Regrouper les Alertes

Le grouping √©vite que vous receviez 50 notifications individuelles quand 50 pods ont le m√™me probl√®me. Au lieu de cela, vous recevez une seule notification qui les regroupe.

### Le Probl√®me sans Grouping

Imaginez ce sc√©nario :
- Vous d√©ployez une nouvelle version d'une application
- 10 pods red√©marrent simultan√©ment
- Chaque pod g√©n√®re 3 alertes diff√©rentes

**Sans grouping** : 30 notifications arrivent en 1 minute. Votre t√©l√©phone ne s'arr√™te pas de sonner.

**Avec grouping** : 1 notification group√©e vous informe que "10 pods ont des probl√®mes de m√©moire".

### Les Param√®tres de Grouping

Il y a 4 param√®tres qui contr√¥lent le grouping :

**group_by** : Sur quels labels regrouper
**group_wait** : Temps d'attente avant la premi√®re notification
**group_interval** : Temps entre les mises √† jour d'un groupe
**repeat_interval** : Temps entre les r√©p√©titions

### group_by : Crit√®res de Regroupement

Le param√®tre `group_by` d√©finit les labels utilis√©s pour cr√©er les groupes.

```yaml
group_by: ['alertname', 'namespace']
```

**Signification** : Les alertes ayant les m√™mes valeurs pour `alertname` ET `namespace` seront regroup√©es.

#### Exemple Concret

Vous avez ces alertes :

| Alert | alertname | namespace | pod |
|-------|-----------|-----------|-----|
| A1 | HighMemory | production | app-1 |
| A2 | HighMemory | production | app-2 |
| A3 | HighMemory | production | app-3 |
| A4 | HighMemory | staging | app-1 |
| A5 | HighCPU | production | api-1 |

**Avec `group_by: ['alertname', 'namespace']`** :

- **Groupe 1** : A1, A2, A3 (HighMemory + production)
- **Groupe 2** : A4 (HighMemory + staging)
- **Groupe 3** : A5 (HighCPU + production)

Vous recevrez **3 notifications** au lieu de 5.

**Avec `group_by: ['alertname']`** :

- **Groupe 1** : A1, A2, A3, A4 (tous HighMemory)
- **Groupe 2** : A5 (HighCPU)

Vous recevrez **2 notifications** au lieu de 5.

**Avec `group_by: ['...']`** (groupe sp√©cial) :

- **Groupe 1** : Toutes les alertes regroup√©es

Vous recevrez **1 seule notification** pour toutes les alertes.

### group_wait : Temps d'Attente Initial

Quand une **nouvelle alerte** arrive et cr√©e un nouveau groupe, Alertmanager attend un certain temps avant d'envoyer la notification. Cela permet de collecter d'autres alertes similaires qui pourraient arriver juste apr√®s.

```yaml
group_wait: 30s
```

**Timeline** :
```
T+0s    : Alerte 1 arrive ‚Üí Nouveau groupe cr√©√©
T+5s    : Alerte 2 arrive ‚Üí Ajout√©e au groupe
T+10s   : Alerte 3 arrive ‚Üí Ajout√©e au groupe
T+30s   : Notification envoy√©e avec les 3 alertes
```

**Quand utiliser** :
- **court (10s-30s)** : Alertes critiques n√©cessitant une r√©action rapide
- **moyen (1m-2m)** : Alertes importantes mais non urgentes
- **long (5m-10m)** : Alertes d'information

### group_interval : Intervalle de Mise √† Jour

Pour un groupe **existant**, si de nouvelles alertes arrivent, Alertmanager attend `group_interval` avant d'envoyer une mise √† jour.

```yaml
group_interval: 5m
```

**Timeline** :
```
T+0s     : Groupe cr√©√© (Alerte 1, 2, 3)
T+30s    : Premi√®re notification envoy√©e
T+2m     : Alerte 4 arrive ‚Üí Ajout√©e au groupe
T+5m30s  : Notification de mise √† jour envoy√©e (maintenant 4 alertes)
T+7m     : Alerte 5 arrive ‚Üí Ajout√©e au groupe
T+10m30s : Notification de mise √† jour envoy√©e (maintenant 5 alertes)
```

**Quand utiliser** :
- **court (2m-5m)** : Situations qui √©voluent rapidement
- **moyen (10m-15m)** : Situations normales
- **long (30m-1h)** : Situations stables

### repeat_interval : R√©p√©tition de Notification

Tant que les alertes d'un groupe sont actives, Alertmanager renvoie une notification √† intervalles r√©guliers.

```yaml
repeat_interval: 4h
```

**Timeline** :
```
T+0      : Premi√®re notification
T+4h     : Rappel (alertes toujours actives)
T+8h     : Rappel
T+12h    : Rappel
```

**Quand utiliser** :
- **court (30m-1h)** : Alertes critiques qui doivent √™tre r√©solues rapidement
- **moyen (4h-12h)** : Alertes importantes mais tol√©rables temporairement
- **long (24h)** : Alertes d'information ou tendances √† long terme

### Configuration Compl√®te d'un Groupe

```yaml
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'namespace']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
```

**Comportement** :
1. Les alertes sont regroup√©es par `alertname`, `cluster` et `namespace`
2. Quand un nouveau groupe se forme, on attend 30s avant d'envoyer
3. Les mises √† jour sont envoy√©es toutes les 5 minutes max
4. Les rappels sont envoy√©s toutes les 4 heures

## Exemples de Configurations R√©alistes

### Configuration 1 : Startup Simple

Pour une petite √©quipe qui d√©bute :

```yaml
route:
  receiver: 'team-slack'
  group_by: ['alertname']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h

receivers:
  - name: 'team-slack'
    slack_configs:
      - api_url: 'webhook-url'
        channel: '#alerts'
```

**Caract√©ristiques** :
- Tout va sur Slack
- Grouping simple par nom d'alerte
- Rappels toutes les 12h

**Adapt√© pour** : Petites √©quipes, environnement de d√©veloppement

### Configuration 2 : Production S√©rieuse

Pour un environnement de production avec une √©quipe d'astreinte :

```yaml
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'namespace']
  group_wait: 10s
  group_interval: 5m
  repeat_interval: 3h

  routes:
    # Critiques en production : astreinte imm√©diate
    - match:
        severity: 'critical'
        environment: 'production'
      receiver: 'pagerduty-oncall'
      group_wait: 10s
      repeat_interval: 5m
      continue: false

    # Warnings en production : Slack
    - match:
        severity: 'warning'
        environment: 'production'
      receiver: 'slack-prod-warnings'
      group_wait: 1m
      group_interval: 10m
      repeat_interval: 4h

    # Staging et dev : email quotidien
    - match_re:
        environment: '(staging|dev)'
      receiver: 'email-dev-team'
      group_wait: 10m
      group_interval: 1h
      repeat_interval: 24h

receivers:
  - name: 'default'
    email_configs:
      - to: 'team@example.com'

  - name: 'pagerduty-oncall'
    pagerduty_configs:
      - service_key: 'xxx'

  - name: 'slack-prod-warnings'
    slack_configs:
      - channel: '#prod-alerts'

  - name: 'email-dev-team'
    email_configs:
      - to: 'dev-team@example.com'
```

**Caract√©ristiques** :
- S√©paration claire par environnement et s√©v√©rit√©
- Production critique ‚Üí PagerDuty rapide
- Production warning ‚Üí Slack mod√©r√©
- Non-production ‚Üí Email lent

**Adapt√© pour** : √âquipes matures avec astreinte

### Configuration 3 : Multi-√©quipes

Pour une organisation avec plusieurs √©quipes :

```yaml
route:
  receiver: 'default'
  group_by: ['alertname', 'team']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h

  routes:
    # √âquipe Backend
    - match:
        team: 'backend'
      receiver: 'slack-backend'

      routes:
        - match:
            severity: 'critical'
          receiver: 'pagerduty-backend'

    # √âquipe Frontend
    - match:
        team: 'frontend'
      receiver: 'slack-frontend'

      routes:
        - match:
            severity: 'critical'
          receiver: 'pagerduty-frontend'

    # √âquipe Data
    - match:
        team: 'data'
      receiver: 'slack-data'

      routes:
        - match:
            severity: 'critical'
          receiver: 'pagerduty-data'

    # Infrastructure (tout ce qui n'a pas de team)
    - match_re:
        alertname: '(Node|Disk|Network).*'
      receiver: 'slack-infra'

      routes:
        - match:
            severity: 'critical'
          receiver: 'pagerduty-infra'

receivers:
  - name: 'default'
    email_configs:
      - to: 'all-teams@example.com'

  - name: 'slack-backend'
    slack_configs:
      - channel: '#backend-alerts'

  - name: 'pagerduty-backend'
    pagerduty_configs:
      - service_key: 'backend-key'

  # ... autres receivers
```

**Caract√©ristiques** :
- Routage par √©quipe
- Chaque √©quipe a son Slack et son PagerDuty
- Hi√©rarchie : team ‚Üí severity
- Infrastructure g√©r√©e s√©par√©ment

**Adapt√© pour** : Grandes organisations avec √©quipes autonomes

### Configuration 4 : Heures de Travail

Pour diff√©rencier les heures ouvrables des heures non-ouvrables :

```yaml
route:
  receiver: 'default'
  group_by: ['alertname']

  routes:
    # Critiques : toujours PagerDuty
    - match:
        severity: 'critical'
      receiver: 'pagerduty-24-7'
      repeat_interval: 5m

    # Warnings : heures de travail ‚Üí Slack, nuit ‚Üí email
    - match:
        severity: 'warning'
      receiver: 'slack-business-hours'
      group_wait: 2m
      repeat_interval: 4h

      # Note : Le time-based routing n√©cessite une configuration externe
      # ou des labels ajout√©s par Prometheus

receivers:
  - name: 'default'
    email_configs:
      - to: 'team@example.com'

  - name: 'pagerduty-24-7'
    pagerduty_configs:
      - service_key: 'xxx'

  - name: 'slack-business-hours'
    slack_configs:
      - channel: '#alerts'
```

**Note** : Alertmanager ne supporte pas nativement le routing bas√© sur l'heure. Vous devez :
- Utiliser des outils externes (scripts, Prometheus recording rules)
- Ajouter des labels temporels √† vos alertes
- Utiliser des solutions tierces comme Grafana OnCall

## Strat√©gies de Grouping Avanc√©es

### Grouping par Criticit√©

Diff√©rents niveaux d'urgence = diff√©rents param√®tres de grouping :

```yaml
route:
  receiver: 'default'

  routes:
    # Critical : notification rapide, peu de grouping
    - match:
        severity: 'critical'
      receiver: 'pagerduty'
      group_by: ['alertname']
      group_wait: 10s
      group_interval: 2m
      repeat_interval: 5m

    # Warning : plus de grouping, moins urgent
    - match:
        severity: 'warning'
      receiver: 'slack'
      group_by: ['alertname', 'namespace', 'cluster']
      group_wait: 2m
      group_interval: 10m
      repeat_interval: 4h

    # Info : grouping maximal, tr√®s lent
    - match:
        severity: 'info'
      receiver: 'email'
      group_by: ['alertname', 'namespace', 'cluster', 'team']
      group_wait: 10m
      group_interval: 1h
      repeat_interval: 24h
```

**Logique** :
- Plus c'est critique, moins on groupe
- Plus c'est critique, plus c'est rapide
- Plus c'est critique, plus on rappelle souvent

### Grouping Dynamique par Label

```yaml
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster']

  routes:
    # Alertes pod : grouper par namespace aussi
    - match_re:
        alertname: 'Pod.*'
      receiver: 'slack-pods'
      group_by: ['alertname', 'cluster', 'namespace']

    # Alertes node : grouper par datacenter
    - match_re:
        alertname: 'Node.*'
      receiver: 'slack-nodes'
      group_by: ['alertname', 'datacenter']

    # Alertes r√©seau : grouper au maximum
    - match_re:
        alertname: 'Network.*'
      receiver: 'slack-network'
      group_by: ['alertname']
```

**Adaptation** : Le grouping s'adapte au type d'alerte

### Grouping Minimal (group_by: [...])

Le groupe sp√©cial `[...]` groupe **toutes les alertes ensemble** :

```yaml
route:
  receiver: 'digest-email'
  group_by: ['...']
  group_wait: 1h
  group_interval: 24h
  repeat_interval: 24h

receivers:
  - name: 'digest-email'
    email_configs:
      - to: 'daily-digest@example.com'
        headers:
          Subject: 'R√©sum√© quotidien des alertes'
```

**Usage** : Rapports quotidiens, digests, environnements non-critiques

## Patterns Anti-patterns et Bonnes Pratiques

### ‚úì Bonnes Pratiques

**1. Routes sp√©cifiques avant routes g√©n√©rales**

```yaml
routes:
  # ‚úì Sp√©cifique d'abord
  - match:
      severity: 'critical'
      team: 'backend'
    receiver: 'backend-pagerduty'

  # G√©n√©ral apr√®s
  - match:
      severity: 'critical'
    receiver: 'general-pagerduty'
```

**2. Utiliser continue pour notifications multiples**

```yaml
routes:
  - match:
      severity: 'critical'
    receiver: 'pagerduty'
    continue: true  # ‚úì Continue pour aussi notifier l'√©quipe

  - match:
      team: 'backend'
    receiver: 'slack-backend'
```

**3. Adapter le grouping √† la s√©v√©rit√©**

```yaml
# ‚úì Critical : grouping minimal, rapide
- match:
    severity: 'critical'
  group_by: ['alertname']
  group_wait: 10s

# ‚úì Warning : grouping plus large, plus lent
- match:
    severity: 'warning'
  group_by: ['alertname', 'namespace', 'cluster']
  group_wait: 2m
```

**4. Documenter les routes complexes**

```yaml
routes:
  # Route 1: Alertes critiques production
  # Envoi imm√©diat vers PagerDuty pour l'astreinte
  # R√©p√©tition toutes les 5 minutes jusqu'√† r√©solution
  - match:
      severity: 'critical'
      environment: 'production'
    receiver: 'pagerduty-oncall'
    group_wait: 10s
    repeat_interval: 5m
```

### ‚úó Anti-patterns √† √âviter

**1. Grouping trop agressif sur les alertes critiques**

```yaml
# ‚úó Mauvais : toutes les alertes critiques dans un seul groupe
- match:
    severity: 'critical'
  group_by: ['...']
  group_wait: 10m
```

**Probl√®me** : Vous perdez le d√©tail et r√©agissez lentement

**Solution** :
```yaml
# ‚úì Bon : grouping sp√©cifique
- match:
    severity: 'critical'
  group_by: ['alertname', 'cluster']
  group_wait: 10s
```

**2. R√©p√©titions trop fr√©quentes**

```yaml
# ‚úó Mauvais : spam toutes les minutes
- match:
    severity: 'warning'
  repeat_interval: 1m
```

**Probl√®me** : Fatigue d'alerte, notifications ignor√©es

**Solution** :
```yaml
# ‚úì Bon : intervalle raisonnable
- match:
    severity: 'warning'
  repeat_interval: 4h
```

**3. Routes trop g√©n√©riques avant les sp√©cifiques**

```yaml
# ‚úó Mauvais : ordre incorrect
routes:
  - match: {}  # Matche tout
    receiver: 'default'

  - match:
      severity: 'critical'
    receiver: 'pagerduty'  # Ne sera jamais atteint !
```

**Probl√®me** : Les routes sp√©cifiques ne sont jamais atteintes

**Solution** :
```yaml
# ‚úì Bon : sp√©cifique d'abord
routes:
  - match:
      severity: 'critical'
    receiver: 'pagerduty'

  - match: {}
    receiver: 'default'
```

**4. Oublier le param√®tre continue**

```yaml
# ‚úó Mauvais : l'√©quipe ne sera pas notifi√©e
routes:
  - match:
      severity: 'critical'
    receiver: 'pagerduty'
    # Pas de continue

  - match:
      team: 'backend'
    receiver: 'slack-backend'
```

**Solution** :
```yaml
# ‚úì Bon : avec continue
routes:
  - match:
      severity: 'critical'
    receiver: 'pagerduty'
    continue: true  # Continue l'√©valuation

  - match:
      team: 'backend'
    receiver: 'slack-backend'
```

**5. Group_wait trop long pour les alertes critiques**

```yaml
# ‚úó Mauvais : attend 10 minutes avant de notifier
- match:
    severity: 'critical'
  group_wait: 10m
```

**Probl√®me** : Retard de 10 minutes pour les alertes urgentes

**Solution** :
```yaml
# ‚úì Bon : notification rapide
- match:
    severity: 'critical'
  group_wait: 10s
```

## Debugging du Routing

### V√©rifier Quelle Route Sera Utilis√©e

Alertmanager fournit une API pour tester vos routes sans envoyer de vraies alertes.

**Simuler une alerte** :

```bash
curl -X POST http://localhost:9093/api/v2/alerts \
  -H 'Content-Type: application/json' \
  -d '[
    {
      "labels": {
        "alertname": "TestAlert",
        "severity": "critical",
        "team": "backend",
        "environment": "production"
      },
      "annotations": {
        "summary": "Test de routing"
      }
    }
  ]'
```

**V√©rifier dans l'interface** :
1. Ouvrir `http://localhost:9093`
2. Aller dans l'onglet **Alerts**
3. Voir comment l'alerte a √©t√© rout√©e

### Activer le Mode Debug

Pour voir les d√©cisions de routing en d√©tail :

```bash
# Voir les logs d'Alertmanager
microk8s kubectl logs -n monitoring alertmanager-0 --tail=100 -f
```

Les logs montrent :
- Quelle route a match√©
- Quel receiver a √©t√© utilis√©
- Les groupes cr√©√©s
- Les notifications envoy√©es

### Utiliser amtool

`amtool` est l'outil CLI officiel pour Alertmanager :

```bash
# Installer amtool
go install github.com/prometheus/alertmanager/cmd/amtool@latest

# V√©rifier la configuration
amtool config routes --config.file=alertmanager.yml

# Tester une alerte
amtool alert add alertname=test severity=critical team=backend
```

## Exemples de Sc√©narios R√©els

### Sc√©nario 1 : D√©ploiement qui √âchoue

**Situation** :
- Vous d√©ployez une nouvelle version
- 20 pods crashent en m√™me temps
- Chaque pod g√©n√®re 3 alertes : CrashLoop, HighMemory, HighRestarts

**Sans grouping** : 60 notifications

**Avec grouping** :

```yaml
group_by: ['alertname', 'deployment']
group_wait: 30s
```

**R√©sultat** :
- 1 notification pour CrashLoop (20 pods)
- 1 notification pour HighMemory (20 pods)
- 1 notification pour HighRestarts (20 pods)

Total : **3 notifications** au lieu de 60

### Sc√©nario 2 : N≈ìud qui Tombe

**Situation** :
- Un n≈ìud Kubernetes tombe en panne
- D√©clenche 50 alertes : NodeDown, 30 pods NotReady, 15 services Unavailable

**Configuration intelligente** :

```yaml
inhibit_rules:
  - source_match:
      alertname: 'NodeDown'
    target_match_re:
      alertname: '(PodNotReady|ServiceUnavailable)'
    equal: ['node']
```

**R√©sultat** :
- 1 notification : "NodeDown"
- Les 45 autres alertes sont inhib√©es car la cause est connue

### Sc√©nario 3 : Pic de Trafic

**Situation** :
- Pic de trafic inattendu
- Latence augmente progressivement
- 10 alertes HighLatency sur diff√©rents services en 5 minutes

**Configuration** :

```yaml
group_by: ['alertname']
group_wait: 1m
group_interval: 5m
```

**Timeline** :
```
T+0   : 1√®re alerte ‚Üí Groupe cr√©√©
T+30s : 5 alertes de plus
T+1m  : Notification envoy√©e (6 alertes)
T+2m  : 4 alertes de plus
T+6m  : Mise √† jour envoy√©e (10 alertes)
```

**R√©sultat** : 2 notifications au lieu de 10

## M√©triques de Routing

Surveillez l'efficacit√© de votre routing avec ces m√©triques Alertmanager :

```promql
# Nombre de notifications par receiver
alertmanager_notifications_total{receiver="slack"}

# Taux d'√©chec des notifications
rate(alertmanager_notifications_failed_total[5m])

# Alertes par √©tat
alertmanager_alerts{state="active"}

# Silences actifs
alertmanager_silences{state="active"}
```

Cr√©ez des alertes sur ces m√©triques :

```yaml
- alert: NotificationFailureHigh
  expr: |
    rate(alertmanager_notifications_failed_total[5m]) > 0.1
  for: 10m
  annotations:
    summary: "√âchec de notifications Alertmanager"
    description: "{{ $value }} notifications/sec √©chouent"
```

## Migration et √âvolution

### Tester avant de D√©ployer

1. **Cr√©er un Alertmanager de test**
2. **Copier votre configuration de production**
3. **Modifier la configuration**
4. **Envoyer des alertes de test**
5. **V√©rifier le comportement**
6. **D√©ployer en production**

### Rollout Progressif

Pour des changements majeurs :

```yaml
# Phase 1 : Envoyer √† l'ancienne ET √† la nouvelle destination
routes:
  - match:
      severity: 'critical'
    receiver: 'old-pagerduty'
    continue: true  # Continuer

  - match:
      severity: 'critical'
    receiver: 'new-pagerduty'  # Nouvelle destination en parall√®le

# Phase 2 (apr√®s validation) : Supprimer l'ancienne
routes:
  - match:
      severity: 'critical'
    receiver: 'new-pagerduty'
```

### Versioning de Configuration

```bash
/alertmanager-config/
‚îú‚îÄ‚îÄ v1.0/
‚îÇ   ‚îî‚îÄ‚îÄ alertmanager.yml
‚îú‚îÄ‚îÄ v1.1/
‚îÇ   ‚îî‚îÄ‚îÄ alertmanager.yml
‚îî‚îÄ‚îÄ current -> v1.1/
```

Gardez un historique de vos configurations pour revenir en arri√®re si n√©cessaire.

## Conclusion

Le routing et le grouping sont les piliers d'un syst√®me d'alerting efficace. Bien configur√©s, ils transforment le chaos d'alertes individuelles en flux organis√©s de notifications pertinentes.

**Principes cl√©s** :
- Le routing dirige les alertes vers les bonnes personnes
- Le grouping √©vite le spam en regroupant intelligemment
- Adaptez les param√®tres √† la s√©v√©rit√© des alertes
- Testez avant de d√©ployer en production
- It√©rez et am√©liorez continuellement

Avec une configuration bien pens√©e, vous maintenez vos √©quipes inform√©es sans les submerger, et vous r√©agissez rapidement aux probl√®mes critiques tout en g√©rant efficacement les alertes moins urgentes.

## Points Cl√©s √† Retenir

‚úì Le routing utilise match/match_re pour diriger les alertes vers les bons receivers

‚úì Les routes sont √©valu√©es dans l'ordre, la premi√®re qui matche gagne (sauf si continue: true)

‚úì Le param√®tre continue permet d'envoyer une alerte √† plusieurs destinations

‚úì group_by d√©finit les crit√®res de regroupement des alertes

‚úì group_wait d√©finit l'attente avant la premi√®re notification (collecte des alertes)

‚úì group_interval d√©finit l'intervalle entre les mises √† jour d'un groupe

‚úì repeat_interval d√©finit la fr√©quence des rappels

‚úì Adaptez les param√®tres √† la s√©v√©rit√© : critique = rapide, info = lent

‚úì Routes sp√©cifiques avant routes g√©n√©rales dans la configuration

‚úì Testez votre configuration avant de d√©ployer en production

---

**Prochaine section** : 14.5 Notifications (Slack, email, webhook) - Nous d√©taillerons la configuration des diff√©rents canaux de notification.

‚è≠Ô∏è [Notifications (Slack, email, webhook)](/14-alerting-et-notifications/05-notifications-slack-email-webhook.md)
