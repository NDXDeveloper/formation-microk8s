üîù Retour au [Sommaire](/SOMMAIRE.md)

# 14.6 Silences et Inhibitions

## Introduction

Deux m√©canismes essentiels d'Alertmanager permettent de g√©rer intelligemment vos alertes : les **silences** et les **inhibitions**. Bien que leurs objectifs soient similaires (r√©duire le bruit), ils fonctionnent de mani√®re tr√®s diff√©rente.

**Silences** : D√©sactivation **temporaire et manuelle** de certaines alertes
- Vous d√©cidez de ne pas √™tre notifi√© pour certaines alertes pendant une p√©riode donn√©e
- Typiquement utilis√© pendant les maintenances planifi√©es
- Action manuelle et intentionnelle

**Inhibitions** : Suppression **automatique** d'alertes redondantes
- Le syst√®me supprime automatiquement certaines alertes quand d'autres sont actives
- Typiquement utilis√© pour √©viter les alertes en cascade
- Configuration permanente bas√©e sur des r√®gles

Ces deux m√©canismes sont compl√©mentaires et essentiels pour maintenir un syst√®me d'alerting sain et non-intrusif.

## Les Silences : D√©sactivation Temporaire

Un **silence** est comme un "mode ne pas d√©ranger" pour vos alertes. Vous d√©finissez des crit√®res et une p√©riode pendant laquelle les alertes correspondantes ne g√©n√©reront pas de notifications.

### Pourquoi Utiliser les Silences ?

**Maintenance planifi√©e** : Vous red√©marrez un serveur, vous savez qu'il sera down
- Sans silence : 50 alertes "ServiceDown", notifications constantes
- Avec silence : Aucune notification pendant la maintenance

**Investigation en cours** : Vous travaillez d√©j√† sur un probl√®me
- Sans silence : Rappels toutes les heures alors que vous savez d√©j√†
- Avec silence : Vous travaillez tranquillement sans √™tre d√©rang√©

**D√©ploiement** : Vous d√©ployez une nouvelle version
- Sans silence : Alertes de pods qui red√©marrent
- Avec silence : D√©ploiement serein

**Tests de charge** : Vous effectuez des tests qui vont d√©clencher des alertes
- Sans silence : Fausses alarmes pendant les tests
- Avec silence : Tests sans pollution du syst√®me d'alerting

### Anatomie d'un Silence

Un silence est compos√© de :

**Matchers** : Les crit√®res pour identifier les alertes √† silencer
```yaml
alertname = "HighCPU"
namespace = "production"
```

**P√©riode** : D√©but et fin du silence
```yaml
startsAt: "2025-01-15T20:00:00Z"
endsAt: "2025-01-15T22:00:00Z"
```

**Cr√©ateur** : Qui a cr√©√© le silence
```yaml
createdBy: "john@example.com"
```

**Commentaire** : Pourquoi ce silence existe
```yaml
comment: "Maintenance planifi√©e du serveur database-01"
```

### Cr√©er un Silence via l'Interface Web

C'est la m√©thode la plus simple pour les d√©butants.

**√âtapes** :

1. **Exposer Alertmanager** :
```bash
microk8s kubectl port-forward -n monitoring svc/alertmanager-operated 9093:9093
```

2. **Ouvrir l'interface** : `http://localhost:9093`

3. **Aller dans l'onglet "Silences"**

4. **Cliquer sur "New Silence"**

5. **Remplir le formulaire** :
   - **Matchers** : Cliquez sur "+" pour ajouter des crit√®res
     - Name: `alertname`
     - Value: `HighCPU`
     - IsRegex: coch√© ou non selon le besoin

   - **Start** : Date et heure de d√©but

   - **End** : Date et heure de fin (ou Duration)

   - **Creator** : Votre nom/email

   - **Comment** : Raison du silence (OBLIGATOIRE)
     - Exemple : "Maintenance database - ticket #1234"

6. **Cliquer sur "Create"**

### Cr√©er un Silence via l'API

Pour automatiser ou scripter la cr√©ation de silences :

**Cr√©er un silence pour 2 heures** :

```bash
curl -X POST http://localhost:9093/api/v2/silences \
  -H 'Content-Type: application/json' \
  -d '{
    "matchers": [
      {
        "name": "alertname",
        "value": "HighCPU",
        "isRegex": false,
        "isEqual": true
      },
      {
        "name": "instance",
        "value": "server-01",
        "isRegex": false,
        "isEqual": true
      }
    ],
    "startsAt": "2025-01-15T20:00:00.000Z",
    "endsAt": "2025-01-15T22:00:00.000Z",
    "createdBy": "automation@example.com",
    "comment": "Maintenance planifi√©e - ticket #1234"
  }'
```

**R√©ponse** :
```json
{
  "silenceID": "abc123-def456-ghi789"
}
```

Conservez cet ID pour supprimer le silence plus tard si n√©cessaire.

### Cr√©er un Silence via amtool

L'outil CLI officiel d'Alertmanager :

**Installation** :
```bash
# Via Go
go install github.com/prometheus/alertmanager/cmd/amtool@latest

# Ou t√©l√©charger depuis GitHub releases
```

**Cr√©er un silence** :
```bash
amtool silence add \
  alertname=HighCPU \
  instance=server-01 \
  --duration=2h \
  --author=john@example.com \
  --comment="Maintenance planifi√©e"
```

**Lister les silences actifs** :
```bash
amtool silence query
```

**Supprimer un silence** :
```bash
amtool silence expire abc123-def456-ghi789
```

### Matchers : Cibler les Alertes

Les **matchers** d√©finissent quelles alertes seront silenc√©es. C'est comme un filtre.

#### Match Exact

Silencer une alerte sp√©cifique :

```json
{
  "name": "alertname",
  "value": "HighCPU",
  "isRegex": false,
  "isEqual": true
}
```

**Effet** : Silencer uniquement les alertes nomm√©es exactement "HighCPU"

#### Match avec Regex

Silencer plusieurs alertes avec un pattern :

```json
{
  "name": "alertname",
  "value": "High.*",
  "isRegex": true,
  "isEqual": true
}
```

**Effet** : Silencer toutes les alertes commen√ßant par "High" (HighCPU, HighMemory, HighDisk, etc.)

#### Multiple Matchers (ET logique)

Combiner plusieurs crit√®res :

```json
{
  "matchers": [
    {
      "name": "alertname",
      "value": "HighCPU",
      "isRegex": false
    },
    {
      "name": "namespace",
      "value": "production",
      "isRegex": false
    },
    {
      "name": "cluster",
      "value": "eu-west",
      "isRegex": false
    }
  ]
}
```

**Effet** : Silencer uniquement les alertes HighCPU dans le namespace production du cluster eu-west

#### Matcher N√©gatif (NOT)

Silencer tout sauf certaines alertes :

```json
{
  "name": "severity",
  "value": "critical",
  "isRegex": false,
  "isEqual": false  // NOT equal
}
```

**Effet** : Silencer toutes les alertes sauf celles de s√©v√©rit√© critical

#### Exemples de Matchers Courants

**Silencer un serveur sp√©cifique** :
```json
{"name": "instance", "value": "server-01"}
```

**Silencer un namespace** :
```json
{"name": "namespace", "value": "staging"}
```

**Silencer un cluster entier** :
```json
{"name": "cluster", "value": "dev-cluster"}
```

**Silencer toutes les alertes de type "Pod"** :
```json
{"name": "alertname", "value": "Pod.*", "isRegex": true}
```

**Silencer un niveau de s√©v√©rit√©** :
```json
{"name": "severity", "value": "warning"}
```

### Dur√©e des Silences

#### Silence Imm√©diat

Commence maintenant :

```bash
curl -X POST http://localhost:9093/api/v2/silences \
  -d '{
    "matchers": [...],
    "startsAt": "'$(date -u +"%Y-%m-%dT%H:%M:%S.000Z")'",
    "endsAt": "'$(date -u -d "+2 hours" +"%Y-%m-%dT%H:%M:%S.000Z")'",
    "comment": "Maintenance en cours"
  }'
```

#### Silence Planifi√©

Commence dans le futur :

```json
{
  "startsAt": "2025-01-20T02:00:00.000Z",
  "endsAt": "2025-01-20T06:00:00.000Z",
  "comment": "Maintenance planifi√©e dimanche 2h-6h"
}
```

#### Silence Long

Pour des p√©riodes prolong√©es :

```json
{
  "startsAt": "2025-01-15T00:00:00.000Z",
  "endsAt": "2025-02-01T00:00:00.000Z",
  "comment": "Migration infrastructure - 2 semaines"
}
```

**‚ö†Ô∏è Attention** : Les silences longs sont risqu√©s - vous pourriez manquer de vrais probl√®mes.

### Expirer un Silence Manuellement

Si votre maintenance se termine plus t√¥t que pr√©vu :

**Via l'interface web** :
1. Onglet "Silences"
2. Trouver votre silence
3. Cliquer sur "Expire"

**Via l'API** :
```bash
curl -X DELETE http://localhost:9093/api/v2/silence/SILENCE_ID
```

**Via amtool** :
```bash
amtool silence expire SILENCE_ID
```

### Visualiser les Silences Actifs

**Interface web** :
- Onglet "Silences"
- Voir tous les silences actifs, en attente et expir√©s

**API** :
```bash
curl http://localhost:9093/api/v2/silences
```

**amtool** :
```bash
amtool silence query
```

**Filtrer les silences** :
```bash
amtool silence query alertname=HighCPU
```

### Bonnes Pratiques pour les Silences

#### ‚úì Toujours Ajouter un Commentaire D√©taill√©

```yaml
comment: "Maintenance DB - Ticket #1234 - John Doe - Migration schema"
```

**Bon commentaire contient** :
- Raison du silence
- R√©f√©rence ticket/issue
- Qui est responsable
- Contexte additionnel

#### ‚úì Utiliser des Dur√©es Raisonnables

- **Maintenance courte** : 1-2 heures
- **Maintenance longue** : 4-6 heures maximum
- **√âviter** : silences de plusieurs jours

Si vous avez besoin de plus, cr√©ez plusieurs silences successifs ou revoyez votre approche.

#### ‚úì √ätre Sp√©cifique

**Mauvais** (trop large) :
```json
{"name": "alertname", "value": ".*", "isRegex": true}
// Silencer TOUTES les alertes - DANGEREUX !
```

**Bon** (sp√©cifique) :
```json
[
  {"name": "alertname", "value": "DatabaseConnectionIssue"},
  {"name": "instance", "value": "db-01"}
]
// Silencer uniquement cette alerte sur ce serveur
```

#### ‚úì Nettoyer les Silences Expir√©s

L'interface conserve l'historique. Nettoyez p√©riodiquement pour maintenir la clart√©.

#### ‚úó Ne Jamais Silencer les Alertes Critiques de Mani√®re Permanente

Si une alerte critique se d√©clenche constamment :
- ‚úó Ne la silencez pas ind√©finiment
- ‚úì Corrigez le probl√®me sous-jacent
- ‚úì Ajustez le seuil de l'alerte si inappropri√©
- ‚úì Documentez pourquoi elle se d√©clenche

## Les Inhibitions : Suppression Automatique

Les **inhibitions** suppriment automatiquement certaines alertes lorsque d'autres alertes plus importantes sont actives. C'est un m√©canisme permanent configur√© dans Alertmanager.

### Pourquoi Utiliser les Inhibitions ?

**√âviter les alertes en cascade** : Quand un probl√®me en cause d'autres

**Sc√©nario sans inhibition** :
1. Serveur database tombe en panne
2. 50 alertes se d√©clenchent :
   - DatabaseDown
   - APIResponseSlow (car attend la DB)
   - WebsiteUnavailable (car pas de DB)
   - CacheInvalid (car pas de DB)
   - BackgroundJobsFailing (car pas de DB)
   - etc.

**R√©sultat** : Vous √™tes submerg√© de notifications pour des effets secondaires alors que le vrai probl√®me est simple : la DB est down.

**Sc√©nario avec inhibition** :
1. Serveur database tombe en panne
2. Alerte DatabaseDown se d√©clenche
3. Toutes les autres alertes li√©es sont automatiquement inhib√©es
4. Vous recevez 1 seule notification : DatabaseDown

### Anatomie d'une R√®gle d'Inhibition

Une r√®gle d'inhibition dans Alertmanager :

```yaml
inhibit_rules:
  - source_match:
      # L'alerte "source" qui, si active, inhibe les autres
      alertname: 'NodeDown'
      severity: 'critical'

    target_match:
      # Les alertes "cibles" qui seront inhib√©es
      severity: 'warning'

    target_match_re:
      # Avec regex pour matcher plusieurs alertes
      alertname: '(PodNotReady|ServiceUnavailable)'

    equal:
      # Les labels qui doivent √™tre identiques entre source et cible
      - 'instance'
      - 'cluster'
```

**Composants** :

**source_match** : L'alerte "inhibitrice" (plus importante)
- Si cette alerte est active (firing), elle peut inhiber d'autres alertes

**target_match** : Les alertes qui seront inhib√©es (moins importantes)
- Match exact sur les labels

**target_match_re** : Les alertes qui seront inhib√©es (avec regex)
- Match avec expressions r√©guli√®res

**equal** : Labels qui doivent √™tre identiques
- Pour garantir que les alertes sont li√©es (m√™me serveur, m√™me cluster, etc.)

### Principe de Fonctionnement

```
[Source Alert: NodeDown, instance=server-01] ‚Üí Active (firing)
                    ‚Üì
         V√©rifie les r√®gles d'inhibition
                    ‚Üì
    Les labels 'instance' sont √©gaux ?
                    ‚Üì
              Oui : server-01
                    ‚Üì
[Target Alert: PodNotReady, instance=server-01] ‚Üí INHIB√âE (pas de notification)
```

### Exemples d'Inhibitions Courantes

#### 1. N≈ìud Down Inhibe les Alertes de Pods

Quand un n≈ìud Kubernetes tombe, tous ses pods sont affect√©s.

```yaml
inhibit_rules:
  - source_match:
      alertname: 'NodeDown'
    target_match_re:
      alertname: '(PodNotReady|PodCrashLooping|ContainerRestarting)'
    equal:
      - 'node'
```

**Logique** :
- Si NodeDown est actif sur "node-01"
- Alors inhiber toutes les alertes de pods sur "node-01"
- Car la cause root est le n≈ìud, pas les pods

#### 2. Alerte Critique Inhibe les Warnings Similaires

Si vous avez d√©j√† une alerte critique, pas besoin du warning √©quivalent.

```yaml
inhibit_rules:
  - source_match:
      alertname: 'CriticalDiskSpace'
      severity: 'critical'
    target_match:
      alertname: 'LowDiskSpace'
      severity: 'warning'
    equal:
      - 'instance'
      - 'mountpoint'
```

**Logique** :
- Si CriticalDiskSpace est actif (< 5% libre)
- Alors inhiber LowDiskSpace (< 15% libre)
- Car le critique inclut d√©j√† le warning

#### 3. Cluster Down Inhibe Tout

Si un cluster entier est inaccessible :

```yaml
inhibit_rules:
  - source_match:
      alertname: 'ClusterDown'
    target_match_re:
      alertname: '.*'
    equal:
      - 'cluster'
```

**Logique** :
- Si ClusterDown est actif
- Inhiber TOUTES les autres alertes du m√™me cluster
- Car elles sont toutes des cons√©quences

#### 4. Maintenance Mode Automatique

Si un label "maintenance" existe :

```yaml
inhibit_rules:
  - source_match:
      maintenance: 'true'
    target_match:
      severity: 'warning'
    equal:
      - 'instance'
```

**Note** : N√©cessite que vos alertes aient un label `maintenance` ajout√© par Prometheus.

#### 5. Service Principal Down Inhibe les Services D√©pendants

```yaml
inhibit_rules:
  - source_match:
      alertname: 'DatabaseDown'
      service: 'postgres'
    target_match_re:
      alertname: '(APIDown|WebsiteDown|BackendDown)'
    equal:
      - 'environment'
```

**Logique** :
- Si la base de donn√©es est down en production
- Inhiber les alertes API/Website/Backend en production
- Car ils d√©pendent tous de la DB

#### 6. Alertes R√©seau

```yaml
inhibit_rules:
  - source_match:
      alertname: 'NetworkPartition'
    target_match_re:
      alertname: '(NodeUnreachable|ServiceTimeout|ProbeFailure)'
    equal:
      - 'datacenter'
```

#### 7. S√©v√©rit√© en Cascade

```yaml
inhibit_rules:
  # Critical inhibe Warning
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal:
      - 'alertname'
      - 'instance'

  # Warning inhibe Info
  - source_match:
      severity: 'warning'
    target_match:
      severity: 'info'
    equal:
      - 'alertname'
      - 'instance'
```

**Logique** :
- Si m√™me alerte sur m√™me instance
- La s√©v√©rit√© sup√©rieure masque les inf√©rieures

### Configuration Compl√®te d'Inhibitions

```yaml
inhibit_rules:
  # 1. N≈ìud down ‚Üí inhibe alertes pods/containers
  - source_match:
      alertname: 'NodeDown'
      severity: 'critical'
    target_match_re:
      alertname: '(Pod.*|Container.*|Kubelet.*)'
    equal:
      - 'node'
      - 'cluster'

  # 2. Cluster down ‚Üí inhibe tout
  - source_match:
      alertname: 'ClusterUnreachable'
    target_match_re:
      alertname: '.*'
    equal:
      - 'cluster'

  # 3. Database down ‚Üí inhibe services d√©pendants
  - source_match:
      alertname: 'DatabaseConnectionFailure'
      component: 'database'
    target_match_re:
      alertname: '(APILatencyHigh|BackendError|CacheFailure)'
    equal:
      - 'environment'
      - 'datacenter'

  # 4. Critical masque Warning (m√™me alerte)
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal:
      - 'alertname'
      - 'instance'

  # 5. Warning masque Info (m√™me alerte)
  - source_match:
      severity: 'warning'
    target_match:
      severity: 'info'
    equal:
      - 'alertname'
      - 'instance'

  # 6. Alerte haute priorit√© masque basse priorit√©
  - source_match:
      priority: 'P1'
    target_match_re:
      priority: '(P2|P3|P4)'
    equal:
      - 'alertname'
      - 'service'
```

### Appliquer les Inhibitions

Les inhibitions sont configur√©es dans le fichier principal d'Alertmanager.

**√âditer la configuration** :

```bash
# Sur MicroK8s
microk8s kubectl edit configmap alertmanager-config -n monitoring
```

**Ajouter/modifier la section inhibit_rules** :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m

    route:
      receiver: 'default'
      # ... routing config

    inhibit_rules:
      - source_match:
          alertname: 'NodeDown'
        target_match_re:
          alertname: 'Pod.*'
        equal:
          - 'node'

    receivers:
      # ... receivers config
```

**Recharger la configuration** :

```bash
# M√©thode 1 : Rechargement √† chaud (si support√©)
microk8s kubectl exec -n monitoring alertmanager-0 -- \
  curl -X POST http://localhost:9093/-/reload

# M√©thode 2 : Red√©marrer le pod
microk8s kubectl delete pod -n monitoring alertmanager-0
```

### Tester les Inhibitions

#### 1. Cr√©er des Alertes de Test

**Alerte source (inhibitrice)** :
```bash
curl -X POST http://localhost:9093/api/v2/alerts \
  -H 'Content-Type: application/json' \
  -d '[
    {
      "labels": {
        "alertname": "NodeDown",
        "node": "worker-01",
        "severity": "critical"
      },
      "annotations": {
        "summary": "Node worker-01 is down"
      }
    }
  ]'
```

**Alerte cible (devrait √™tre inhib√©e)** :
```bash
curl -X POST http://localhost:9093/api/v2/alerts \
  -H 'Content-Type: application/json' \
  -d '[
    {
      "labels": {
        "alertname": "PodNotReady",
        "node": "worker-01",
        "pod": "app-123",
        "severity": "warning"
      },
      "annotations": {
        "summary": "Pod not ready"
      }
    }
  ]'
```

#### 2. V√©rifier l'Inhibition

**Interface web** :
1. Aller sur `http://localhost:9093`
2. Onglet "Alerts"
3. L'alerte PodNotReady devrait appara√Ætre mais avec une indication d'inhibition

**API** :
```bash
curl http://localhost:9093/api/v2/alerts
```

Cherchez `"status": "suppressed"` dans la r√©ponse.

### Visualiser les Inhibitions Actives

**API Status** :
```bash
curl http://localhost:9093/api/v2/status
```

Affiche la configuration actuelle incluant les inhibit_rules.

**Logs** :
```bash
microk8s kubectl logs -n monitoring alertmanager-0 | grep inhibit
```

Recherchez les lignes mentionnant les inhibitions appliqu√©es.

### Diff√©rence entre Silences et Inhibitions

| Aspect | Silences | Inhibitions |
|--------|----------|-------------|
| **Type** | Manuel | Automatique |
| **Dur√©e** | Temporaire | Permanent |
| **Configuration** | Via UI/API √† chaque fois | R√®gles dans config YAML |
| **Cas d'usage** | Maintenances, investigations | Alertes en cascade |
| **D√©cision** | Humaine | Syst√®me |
| **Flexibilit√©** | Tr√®s flexible, ad-hoc | R√®gles fixes |
| **Visibilit√©** | Visible dans interface | Transparent pour l'utilisateur |

### Bonnes Pratiques pour les Inhibitions

#### ‚úì Cr√©er une Hi√©rarchie Claire

Organisez vos inhibitions par niveaux :

```
Niveau 1: Infrastructure
  ‚îú‚îÄ DatacenterDown
  ‚îî‚îÄ ClusterDown
      ‚îú‚îÄ Niveau 2: N≈ìuds
      ‚îÇ   ‚îú‚îÄ NodeDown
      ‚îÇ   ‚îî‚îÄ NodeUnreachable
      ‚îÇ       ‚îî‚îÄ Niveau 3: Applications
      ‚îÇ           ‚îú‚îÄ PodNotReady
      ‚îÇ           ‚îú‚îÄ ServiceDown
      ‚îÇ           ‚îî‚îÄ ContainerCrashing
```

#### ‚úì Utiliser le Label 'equal' Judicieusement

**Trop large** (probl√©matique) :
```yaml
equal: []  # Pas de filtre - inhibe tout partout
```

**Trop sp√©cifique** (inefficace) :
```yaml
equal:
  - 'node'
  - 'pod'
  - 'container'
  - 'namespace'
  - 'deployment'
```

**Juste bien** :
```yaml
equal:
  - 'node'
  - 'cluster'
```

#### ‚úì Tester Avant de D√©ployer en Production

1. D√©ployer dans environnement de test
2. Simuler des alertes
3. V√©rifier que les inhibitions fonctionnent
4. Ajuster si n√©cessaire
5. D√©ployer en production

#### ‚úì Documenter Vos Inhibitions

```yaml
inhibit_rules:
  # Inhibition 1: N≈ìud down masque les alertes de pods
  # Raison: Quand un n≈ìud est down, tous ses pods sont affect√©s
  # Propri√©taire: √âquipe SRE
  # Date: 2025-01-15
  - source_match:
      alertname: 'NodeDown'
    target_match_re:
      alertname: 'Pod.*'
    equal:
      - 'node'
```

#### ‚úì Monitorer l'Efficacit√©

Cr√©ez des m√©triques pour suivre :
- Nombre d'alertes inhib√©es
- Ratio alertes inhib√©es / alertes totales
- Alertes jamais inhib√©es (peut-√™tre inutiles ?)

```promql
# Alertes actuellement inhib√©es
count(ALERTS{alertstate="suppressed"})

# Ratio d'inhibition
count(ALERTS{alertstate="suppressed"}) / count(ALERTS)
```

#### ‚úó Ne Pas Sur-utiliser les Inhibitions

**Sympt√¥me** : Trop d'alertes inhib√©es en permanence

**Cause** : Mauvaise configuration d'alertes ou inhibitions trop agressives

**Solution** : Revoir vos r√®gles d'alerte plut√¥t que de tout inhiber

## Sc√©narios Pratiques Combin√©s

### Sc√©nario 1 : Maintenance Planifi√©e d'un N≈ìud

**Objectif** : Red√©marrer le n≈ìud worker-02 pour des mises √† jour

**Approche combin√©e** :

1. **Inhibition permanente** (d√©j√† configur√©e) :
```yaml
inhibit_rules:
  - source_match:
      alertname: 'NodeDown'
    target_match_re:
      alertname: 'Pod.*'
    equal: ['node']
```
G√®re automatiquement les alertes de pods quand le n≈ìud red√©marre.

2. **Silence temporaire** (cr√©√© manuellement) :
```bash
amtool silence add \
  alertname=NodeDown \
  node=worker-02 \
  --duration=1h \
  --comment="Maintenance worker-02 - Mises √† jour syst√®me"
```
√âvite l'alerte NodeDown elle-m√™me pendant la maintenance.

**R√©sultat** :
- Pas d'alerte NodeDown (silence)
- Pas d'alertes de pods (inhibition automatique)
- Maintenance sereine

### Sc√©nario 2 : Investigation d'un Probl√®me R√©current

**Objectif** : Une alerte HighMemory se d√©clenche r√©guli√®rement, vous enqu√™tez

**Approche** :

1. **Silence pendant investigation** :
```bash
amtool silence add \
  alertname=HighMemory \
  pod=problematic-app-xyz \
  namespace=production \
  --duration=4h \
  --comment="Investigation fuite m√©moire - Ticket #5678 - Alice"
```

2. **Ne PAS cr√©er d'inhibition** :
- L'inhibition serait permanente
- Le probl√®me doit √™tre r√©solu, pas masqu√©

**R√©sultat** :
- Vous travaillez sans √™tre d√©rang√© par les rappels
- Apr√®s r√©solution, le silence expire naturellement

### Sc√©nario 3 : D√©ploiement Canary

**Objectif** : D√©ployer progressivement une nouvelle version

**Approche** :

```bash
# Silencer les alertes de l'ancienne version en cours de suppression
amtool silence add \
  namespace=production \
  version=v1.2.3 \
  --duration=30m \
  --comment="D√©ploiement canary v1.2.4 - Suppression v1.2.3"

# Silencer temporairement les alertes de d√©marrage de la nouvelle version
amtool silence add \
  namespace=production \
  version=v1.2.4 \
  --duration=15m \
  --comment="D√©ploiement canary v1.2.4 - Warmup period"
```

### Sc√©nario 4 : Incident en Cascade

**Situation** :
1. Database primaire tombe
2. 100 alertes se d√©clenchent en cascade

**Inhibitions (configur√©es √† l'avance)** :
```yaml
inhibit_rules:
  - source_match:
      alertname: 'DatabaseDown'
      role: 'primary'
    target_match_re:
      alertname: '(API.*|Backend.*|Frontend.*|Cache.*)'
    equal:
      - 'environment'
```

**R√©sultat** :
- 1 seule alerte : DatabaseDown
- Toutes les alertes d√©pendantes inhib√©es automatiquement
- Focus sur la vraie cause

## Outils de Gestion

### amtool : L'Outil en Ligne de Commande

**Installation** :
```bash
go install github.com/prometheus/alertmanager/cmd/amtool@latest
```

**Configuration** :
```bash
# Cr√©er ~/.config/amtool/config.yml
alertmanager.url: http://localhost:9093
```

**Commandes utiles** :

```bash
# Silences
amtool silence query                    # Liste tous les silences
amtool silence query alertname=HighCPU  # Filtrer les silences
amtool silence add alertname=Test --duration=1h --comment="Test"
amtool silence expire SILENCE_ID        # Supprimer un silence

# Alertes
amtool alert query                      # Toutes les alertes actives
amtool alert query severity=critical    # Filtrer par label

# Configuration
amtool config routes                    # Afficher les routes
amtool config routes test \             # Tester le routing
  severity=critical team=backend

# Check de sant√©
amtool check-config alertmanager.yml    # Valider une config
```

### Scripts d'Automatisation

**Script de silence automatique pour maintenance** :

```bash
#!/bin/bash
# maintenance-silence.sh

INSTANCE=$1
DURATION=$2
TICKET=$3

if [ -z "$INSTANCE" ] || [ -z "$DURATION" ] || [ -z "$TICKET" ]; then
  echo "Usage: $0 <instance> <duration> <ticket>"
  echo "Example: $0 server-01 2h TICKET-1234"
  exit 1
fi

amtool silence add \
  instance="$INSTANCE" \
  --duration="$DURATION" \
  --author="$(whoami)@example.com" \
  --comment="Maintenance $INSTANCE - Ticket $TICKET"

echo "Silence cr√©√© pour $INSTANCE pendant $DURATION"
```

**Utilisation** :
```bash
./maintenance-silence.sh server-01 2h TICKET-1234
```

### API Python pour G√©rer les Silences

```python
import requests
from datetime import datetime, timedelta
import json

ALERTMANAGER_URL = "http://localhost:9093"

def create_silence(matchers, duration_hours, comment, created_by):
    """Cr√©er un silence dans Alertmanager"""

    now = datetime.utcnow()
    start = now.isoformat() + "Z"
    end = (now + timedelta(hours=duration_hours)).isoformat() + "Z"

    silence = {
        "matchers": matchers,
        "startsAt": start,
        "endsAt": end,
        "createdBy": created_by,
        "comment": comment
    }

    response = requests.post(
        f"{ALERTMANAGER_URL}/api/v2/silences",
        json=silence
    )

    if response.status_code == 200:
        silence_id = response.json()["silenceID"]
        print(f"‚úì Silence cr√©√©: {silence_id}")
        return silence_id
    else:
        print(f"‚úó Erreur: {response.text}")
        return None

def delete_silence(silence_id):
    """Supprimer un silence"""
    response = requests.delete(
        f"{ALERTMANAGER_URL}/api/v2/silence/{silence_id}"
    )

    if response.status_code == 200:
        print(f"‚úì Silence {silence_id} supprim√©")
        return True
    else:
        print(f"‚úó Erreur: {response.text}")
        return False

def list_silences():
    """Lister tous les silences actifs"""
    response = requests.get(f"{ALERTMANAGER_URL}/api/v2/silences")

    if response.status_code == 200:
        silences = response.json()
        print(f"Silences actifs: {len(silences)}")
        for s in silences:
            print(f"  - {s['id']}: {s['comment']}")
        return silences
    else:
        print(f"‚úó Erreur: {response.text}")
        return []

# Exemple d'utilisation
if __name__ == "__main__":
    # Cr√©er un silence pour maintenance
    matchers = [
        {"name": "alertname", "value": "HighCPU", "isRegex": False},
        {"name": "instance", "value": "server-01", "isRegex": False}
    ]

    silence_id = create_silence(
        matchers=matchers,
        duration_hours=2,
        comment="Maintenance server-01 - Ticket #1234",
        created_by="automation@example.com"
    )

    # Lister les silences
    list_silences()
```

## M√©triques et Monitoring

### M√©triques Importantes

**Alertmanager expose des m√©triques sur les silences et inhibitions** :

```promql
# Nombre de silences actifs
alertmanager_silences{state="active"}

# Nombre d'alertes inhib√©es
count(ALERTS{alertstate="suppressed"})

# Alertes supprim√©es par les silences
count(ALERTS{silenced="true"})

# Ratio d'alertes inhib√©es
count(ALERTS{alertstate="suppressed"}) / count(ALERTS) * 100
```

### Alertes sur les Silences

Cr√©ez des alertes pour monitorer l'utilisation des silences :

```yaml
groups:
  - name: alertmanager_health
    rules:
      - alert: TooManySilences
        expr: alertmanager_silences{state="active"} > 10
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Trop de silences actifs"
          description: "{{ $value }} silences actifs - v√©rifier si normal"

      - alert: LongRunningSilence
        expr: |
          time() - alertmanager_silence_start_timestamp > 86400
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "Silence actif depuis > 24h"
          description: "Silence {{ $labels.id }} actif depuis longtemps"

      - alert: HighInhibitionRate
        expr: |
          count(ALERTS{alertstate="suppressed"})
          /
          count(ALERTS) * 100 > 50
        for: 15m
        labels:
          severity: info
        annotations:
          summary: "Taux d'inhibition √©lev√©"
          description: "{{ $value }}% des alertes sont inhib√©es"
```

## Troubleshooting

### Probl√®me : Les Silences ne Fonctionnent Pas

**Sympt√¥mes** :
- Silence cr√©√© mais alertes toujours notifi√©es
- Matchers ne correspondent pas

**V√©rifications** :

1. **V√©rifier les matchers** :
```bash
# Voir les labels de l'alerte
curl http://localhost:9093/api/v2/alerts | jq '.[] | .labels'

# Comparer avec les matchers du silence
amtool silence query
```

2. **V√©rifier l'orthographe** :
```yaml
# ‚úó Mauvais
{"name": "alertName", "value": "HighCPU"}

# ‚úì Bon
{"name": "alertname", "value": "HighCPU"}
```

3. **V√©rifier la p√©riode** :
```bash
# Le silence est-il actif maintenant ?
amtool silence query
```

4. **Logs Alertmanager** :
```bash
microk8s kubectl logs -n monitoring alertmanager-0 | grep -i silence
```

### Probl√®me : Les Inhibitions ne S'Appliquent Pas

**Sympt√¥mes** :
- Alertes en cascade toujours notifi√©es
- Inhibition configur√©e mais inefficace

**V√©rifications** :

1. **V√©rifier les labels 'equal'** :
```bash
# Les alertes source et cible ont-elles les m√™mes valeurs pour les labels 'equal' ?
curl http://localhost:9093/api/v2/alerts | \
  jq '.[] | select(.labels.alertname == "NodeDown" or .labels.alertname == "PodNotReady") | .labels'
```

2. **V√©rifier la syntaxe** :
```yaml
# ‚úó Mauvais (target_match avec regex sans _re)
target_match:
  alertname: 'Pod.*'

# ‚úì Bon
target_match_re:
  alertname: 'Pod.*'
```

3. **Tester la configuration** :
```bash
# Valider la syntaxe
amtool check-config alertmanager.yml

# Voir les inhibitions actives
curl http://localhost:9093/api/v2/status | jq '.config.inhibit_rules'
```

4. **Logs d√©taill√©s** :
```bash
microk8s kubectl logs -n monitoring alertmanager-0 | grep -i inhibit
```

### Probl√®me : Trop d'Alertes Inhib√©es

**Sympt√¥mes** :
- 80%+ des alertes inhib√©es
- Alertes importantes manqu√©es

**Diagnostic** :

```promql
# V√©rifier le ratio
count(ALERTS{alertstate="suppressed"}) / count(ALERTS) * 100
```

**Solutions** :

1. Revoir vos r√®gles d'inhibition (trop agressives ?)
2. Revoir vos r√®gles d'alerte (trop d'alertes redondantes ?)
3. Am√©liorer la hi√©rarchie des alertes

## Conclusion

Les silences et inhibitions sont deux outils compl√©mentaires essentiels pour g√©rer intelligemment vos alertes :

**Silences** : Pour les situations temporaires et planifi√©es
- Maintenances
- Investigations
- D√©ploiements
- Tests

**Inhibitions** : Pour les situations r√©currentes et pr√©visibles
- Alertes en cascade
- Hi√©rarchie d'infrastructure
- D√©pendances entre services

Bien utilis√©s, ils transforment un syst√®me d'alerting bruyant en un assistant intelligent qui ne vous d√©range que quand c'est vraiment n√©cessaire.

## Points Cl√©s √† Retenir

‚úì Silences = d√©sactivation temporaire et manuelle des alertes

‚úì Inhibitions = suppression automatique d'alertes redondantes

‚úì Toujours documenter les silences avec un commentaire d√©taill√©

‚úì Utiliser des dur√©es de silence raisonnables (quelques heures max)

‚úì Configurer les inhibitions pour √©viter les alertes en cascade

‚úì Le param√®tre 'equal' dans les inhibitions garantit que les alertes sont li√©es

‚úì Tester les inhibitions avant de les d√©ployer en production

‚úì Monitorer l'utilisation des silences et l'efficacit√© des inhibitions

‚úì Ne jamais masquer d√©finitivement un probl√®me avec des silences

‚úì Les inhibitions sont permanentes, les silences sont temporaires

---

**Prochaine section** : 14.7 Bonnes pratiques d'alerting - Nous consoliderons tout ce que nous avons appris avec des recommandations compl√®tes pour un syst√®me d'alerting optimal.

‚è≠Ô∏è [Bonnes pratiques d'alerting](/14-alerting-et-notifications/07-bonnes-pratiques-dalerting.md)
