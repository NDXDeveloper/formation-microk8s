ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 15.4 CorrÃ©lation Logs-MÃ©triques avec Loki

## Introduction Ã  Loki

Dans la section prÃ©cÃ©dente, nous avons vu la stack EFK pour centraliser les logs. Mais il existe une alternative **plus lÃ©gÃ¨re** et **parfaitement intÃ©grÃ©e Ã  l'Ã©cosystÃ¨me Prometheus/Grafana** : **Loki**.

### Qu'est-ce que Loki ?

**Loki** est un systÃ¨me d'agrÃ©gation de logs crÃ©Ã© par Grafana Labs, conÃ§u pour Ãªtre :
- **LÃ©ger** : Consomme beaucoup moins de ressources qu'Elasticsearch
- **Simple** : Plus facile Ã  dÃ©ployer et maintenir
- **IntÃ©grÃ©** : Natif dans Grafana, aux cÃ´tÃ©s de Prometheus
- **Efficace** : OptimisÃ© pour les logs applicatifs de Kubernetes

**Slogan officiel** : "Like Prometheus, but for logs"

### Pourquoi Loki pour un Lab MicroK8s ?

| Aspect | Elasticsearch (EFK) | Loki |
|--------|---------------------|------|
| **MÃ©moire** | 2-4 GB minimum | 200-500 MB |
| **CPU** | Intensif | LÃ©ger |
| **Stockage** | Index complets | Labels seulement |
| **Recherche** | Full-text rapide | Par labels + grep |
| **Setup** | Complexe (3 composants) | Simple (2 composants) |
| **CoÃ»t** | Ã‰levÃ© pour petit lab | Faible |
| **Grafana** | IntÃ©gration possible | Natif |
| **Courbe d'apprentissage** | Moyenne | Facile si vous connaissez Prometheus |

**Recommandation** :
- **Lab personnel** ou ressources limitÃ©es â†’ **Loki** âœ…
- Production avec recherche complexe â†’ EFK
- Besoin de corrÃ©lation logs-mÃ©triques dans Grafana â†’ **Loki** âœ…

## Philosophie de Loki vs Elasticsearch

### Elasticsearch : Index Everything

Elasticsearch **indexe tout le contenu** des logs :

```
Log: "User john.doe@example.com logged in from IP 192.168.1.100"

Index Elasticsearch:
- "User" â†’ Doc 1
- "john" â†’ Doc 1
- "doe" â†’ Doc 1
- "example" â†’ Doc 1
- "com" â†’ Doc 1
- "logged" â†’ Doc 1
- "in" â†’ Doc 1
- "from" â†’ Doc 1
- "IP" â†’ Doc 1
- "192" â†’ Doc 1
- "168" â†’ Doc 1
... (chaque mot est indexÃ©)
```

**RÃ©sultat** :
- âœ… Recherche full-text ultra-rapide
- âŒ Ã‰norme consommation de stockage
- âŒ Beaucoup de CPU/mÃ©moire pour indexer

### Loki : Index Labels Only

Loki **n'indexe que les labels** (mÃ©tadonnÃ©es), pas le contenu :

```
Log: "User john.doe@example.com logged in from IP 192.168.1.100"

Index Loki:
Labels:
- namespace: "production"
- pod: "auth-service-abc123"
- app: "auth"
- level: "INFO"

Le contenu du log est stockÃ© brut, non indexÃ©.
```

**RÃ©sultat** :
- âœ… Stockage minimal
- âœ… Faible consommation CPU/mÃ©moire
- âš ï¸ Recherche full-text plus lente (grep sur les logs)

### Analogie

**Elasticsearch** = BibliothÃ¨que oÃ¹ chaque mot de chaque livre est indexÃ©
- Trouvez "dragon" instantanÃ©ment dans tous les livres
- NÃ©cessite un systÃ¨me d'indexation massif

**Loki** = BibliothÃ¨que organisÃ©e par catÃ©gories (labels)
- Trouvez tous les livres fantasy (label)
- Puis feuilletez pour trouver "dragon"
- Plus simple, moins d'espace

## Architecture de Loki

### Composants

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Cluster MicroK8s                          â”‚
â”‚                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ Pod 1  â”‚  â”‚ Pod 2  â”‚  â”‚ Pod 3  â”‚                â”‚
â”‚  â”‚ logs   â”‚  â”‚ logs   â”‚  â”‚ logs   â”‚                â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                â”‚
â”‚      â”‚           â”‚           â”‚                     â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                  â†“                                 â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚        â”‚  Promtail         â”‚ â† DaemonSet           â”‚
â”‚        â”‚  (Agent)          â”‚   (collecte logs)     â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      Loki       â”‚ â† StatefulSet
         â”‚   (Stockage)    â”‚   (agrÃ¨ge et stocke)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    Grafana      â”‚ â† Interface de requÃªte
         â”‚  (Visualisation)â”‚   et visualisation
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†‘
              â”‚ Aussi connectÃ© Ã 
              â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Prometheus    â”‚ â† MÃ©triques
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Les Composants DÃ©taillÃ©s

#### 1. Promtail (Agent de Collecte)

**RÃ´le** : Collecter les logs et les envoyer Ã  Loki

**CaractÃ©ristiques** :
- DÃ©ployÃ© en **DaemonSet** (un pod par nÅ“ud)
- Lit les logs des containers
- Extrait et ajoute des **labels** (namespace, pod, app, etc.)
- Parsing des logs pour enrichir les labels
- LÃ©ger : ~40-80 MB de RAM

**Similaire Ã ** : Fluentd dans la stack EFK

#### 2. Loki (Serveur Central)

**RÃ´le** : Recevoir, indexer (labels uniquement) et stocker les logs

**CaractÃ©ristiques** :
- DÃ©ployÃ© en **StatefulSet** (besoin de stockage persistant)
- Index seulement les labels (pas le contenu)
- Stockage en chunks compressÃ©s
- API pour requÃªter les logs
- ~200-500 MB de RAM

**Similaire Ã ** : Elasticsearch dans la stack EFK (mais beaucoup plus lÃ©ger)

#### 3. Grafana (Interface)

**RÃ´le** : Visualiser les logs ET les mÃ©triques dans la mÃªme interface

**CaractÃ©ristiques** :
- Datasource Loki intÃ©grÃ©e nativement
- Explore UI pour naviguer dans les logs
- Dashboards unifiÃ©s logs + mÃ©triques
- CorrÃ©lation automatique

**Vous l'avez dÃ©jÃ ** : Si vous avez suivi le chapitre 13 !

## DÃ©ploiement de Loki dans MicroK8s

### Option 1 : Via l'Addon MicroK8s (RecommandÃ©)

MicroK8s propose un addon pour Loki qui simplifie grandement l'installation :

```bash
# Activer l'addon observability (inclut Loki)
microk8s enable observability

# Cet addon installe automatiquement :
# - Prometheus
# - Grafana
# - Loki
# - Promtail
```

**Avantages** :
- âœ… Installation en une commande
- âœ… Configuration prÃ©-optimisÃ©e pour MicroK8s
- âœ… IntÃ©gration automatique dans Grafana

**InconvÃ©nient** :
- âš ï¸ Moins de contrÃ´le sur la configuration

### Option 2 : DÃ©ploiement Manuel (Plus de ContrÃ´le)

Si vous voulez plus de contrÃ´le ou si vous avez dÃ©jÃ  Prometheus/Grafana :

#### CrÃ©er le Namespace

```bash
microk8s kubectl create namespace monitoring
# Ou rÃ©utilisez votre namespace existant
```

#### 1. DÃ©ployer Loki

**ConfigMap Loki**

Fichier `loki-config.yaml` :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-config
  namespace: monitoring
data:
  loki.yaml: |
    auth_enabled: false

    server:
      http_listen_port: 3100
      grpc_listen_port: 9096

    common:
      path_prefix: /loki
      storage:
        filesystem:
          chunks_directory: /loki/chunks
          rules_directory: /loki/rules
      replication_factor: 1
      ring:
        instance_addr: 127.0.0.1
        kvstore:
          store: inmemory

    schema_config:
      configs:
        - from: 2020-10-24
          store: boltdb-shipper
          object_store: filesystem
          schema: v11
          index:
            prefix: index_
            period: 24h

    limits_config:
      retention_period: 168h  # 7 jours
      reject_old_samples: true
      reject_old_samples_max_age: 168h

    chunk_store_config:
      max_look_back_period: 168h

    table_manager:
      retention_deletes_enabled: true
      retention_period: 168h
```

**Points importants** :
- `retention_period: 168h` : Garde les logs 7 jours
- `filesystem` : Stockage local (pour un lab)
- `replication_factor: 1` : Une seule instance

**Service Loki**

Fichier `loki-service.yaml` :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: loki
  namespace: monitoring
  labels:
    app: loki
spec:
  selector:
    app: loki
  ports:
  - name: http
    port: 3100
    targetPort: 3100
  type: ClusterIP
```

**StatefulSet Loki**

Fichier `loki-statefulset.yaml` :

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: loki
  namespace: monitoring
spec:
  serviceName: loki
  replicas: 1
  selector:
    matchLabels:
      app: loki
  template:
    metadata:
      labels:
        app: loki
    spec:
      containers:
      - name: loki
        image: grafana/loki:2.9.3
        args:
        - -config.file=/etc/loki/loki.yaml
        ports:
        - name: http
          containerPort: 3100
        - name: grpc
          containerPort: 9096
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        volumeMounts:
        - name: config
          mountPath: /etc/loki
        - name: storage
          mountPath: /loki
      volumes:
      - name: config
        configMap:
          name: loki-config
  volumeClaimTemplates:
  - metadata:
      name: storage
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: "microk8s-hostpath"
      resources:
        requests:
          storage: 10Gi
```

**DÃ©ployer Loki** :

```bash
microk8s kubectl apply -f loki-config.yaml
microk8s kubectl apply -f loki-service.yaml
microk8s kubectl apply -f loki-statefulset.yaml

# VÃ©rifier
microk8s kubectl get pods -n monitoring -l app=loki
microk8s kubectl logs -n monitoring loki-0
```

#### 2. DÃ©ployer Promtail

**ConfigMap Promtail**

Fichier `promtail-config.yaml` :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: promtail-config
  namespace: monitoring
data:
  promtail.yaml: |
    server:
      http_listen_port: 9080
      grpc_listen_port: 0

    positions:
      filename: /tmp/positions.yaml

    clients:
      - url: http://loki.monitoring.svc.cluster.local:3100/loki/api/v1/push

    scrape_configs:
      # Logs des pods
      - job_name: kubernetes-pods
        kubernetes_sd_configs:
          - role: pod
        pipeline_stages:
          - cri: {}
        relabel_configs:
          # Copier les labels Kubernetes
          - source_labels:
              - __meta_kubernetes_pod_node_name
            target_label: __host__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - action: replace
            replacement: $1
            separator: /
            source_labels:
              - __meta_kubernetes_namespace
              - __meta_kubernetes_pod_name
            target_label: job
          - action: replace
            source_labels:
              - __meta_kubernetes_namespace
            target_label: namespace
          - action: replace
            source_labels:
              - __meta_kubernetes_pod_name
            target_label: pod
          - action: replace
            source_labels:
              - __meta_kubernetes_pod_container_name
            target_label: container
          - replacement: /var/log/pods/*$1/*.log
            separator: /
            source_labels:
              - __meta_kubernetes_pod_uid
              - __meta_kubernetes_pod_container_name
            target_label: __path__
```

**ServiceAccount et RBAC**

Fichier `promtail-rbac.yaml` :

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: promtail
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: promtail
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs:
  - get
  - watch
  - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: promtail
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: promtail
subjects:
- kind: ServiceAccount
  name: promtail
  namespace: monitoring
```

**DaemonSet Promtail**

Fichier `promtail-daemonset.yaml` :

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: promtail
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: promtail
  template:
    metadata:
      labels:
        app: promtail
    spec:
      serviceAccountName: promtail
      containers:
      - name: promtail
        image: grafana/promtail:2.9.3
        args:
        - -config.file=/etc/promtail/promtail.yaml
        env:
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
        volumeMounts:
        - name: config
          mountPath: /etc/promtail
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      volumes:
      - name: config
        configMap:
          name: promtail-config
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
```

**DÃ©ployer Promtail** :

```bash
microk8s kubectl apply -f promtail-config.yaml
microk8s kubectl apply -f promtail-rbac.yaml
microk8s kubectl apply -f promtail-daemonset.yaml

# VÃ©rifier (un pod par nÅ“ud)
microk8s kubectl get daemonset -n monitoring
microk8s kubectl get pods -n monitoring -l app=promtail
microk8s kubectl logs -n monitoring -l app=promtail --tail=20
```

#### 3. Configurer Grafana

**Ajouter Loki comme Datasource**

Si Grafana est dÃ©jÃ  dÃ©ployÃ© (chapitre 13) :

1. Connectez-vous Ã  Grafana
2. **Configuration** â†’ **Data Sources**
3. **Add data source**
4. SÃ©lectionnez **Loki**
5. Configurez :
   - **Name** : Loki
   - **URL** : `http://loki.monitoring.svc.cluster.local:3100`
   - **Access** : Server (default)
6. **Save & Test**

**Via ConfigMap (automatique)** :

Fichier `grafana-datasource-loki.yaml` :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasource-loki
  namespace: monitoring
  labels:
    grafana_datasource: "1"
data:
  loki-datasource.yaml: |
    apiVersion: 1
    datasources:
    - name: Loki
      type: loki
      access: proxy
      url: http://loki.monitoring.svc.cluster.local:3100
      isDefault: false
      editable: true
```

```bash
microk8s kubectl apply -f grafana-datasource-loki.yaml

# RedÃ©marrez Grafana pour charger la datasource
microk8s kubectl rollout restart deployment grafana -n monitoring
```

### VÃ©rification ComplÃ¨te

```bash
# Tous les composants doivent Ãªtre UP
microk8s kubectl get all -n monitoring

# Tester Loki directement
microk8s kubectl port-forward -n monitoring svc/loki 3100:3100

# Dans un autre terminal
curl http://localhost:3100/ready
# Devrait retourner "ready"

curl http://localhost:3100/metrics
# Devrait afficher les mÃ©triques Loki
```

## LogQL : Le Langage de RequÃªte de Loki

LogQL est le langage pour requÃªter Loki, similaire Ã  PromQL pour Prometheus.

### Structure d'une RequÃªte

```
{label="value"} |= "search text" | filter | aggregation
```

**Composants** :
1. **Log Stream Selector** : `{label="value"}` - SÃ©lectionne les flux de logs
2. **Filter Expression** : `|= "text"` - Filtre sur le contenu
3. **Parser** : `| json` - Parse le contenu
4. **Aggregation** : `| count_over_time()` - AgrÃ¨ge les rÃ©sultats

### SÃ©lecteurs de Flux (Log Stream Selectors)

**Syntaxe** : `{label=value}`

**Exemples** :

```logql
# Tous les logs du namespace production
{namespace="production"}

# Logs d'une application spÃ©cifique
{app="payment-service"}

# Combiner plusieurs labels
{namespace="production", app="payment-service"}

# Logs d'un pod spÃ©cifique
{pod="payment-service-7d8c9-x4b2k"}

# Utiliser les labels Kubernetes
{namespace="production", container="api"}
```

### OpÃ©rateurs de Filtrage

#### `|=` : Contient (case-sensitive)

```logql
# Logs contenant "error"
{app="payment"} |= "error"
```

#### `!=` : Ne contient pas

```logql
# Logs ne contenant pas "debug"
{app="payment"} != "debug"
```

#### `|~` : Regex correspond

```logql
# Logs correspondant au pattern ERROR|WARN
{app="payment"} |~ "ERROR|WARN"
```

#### `!~` : Regex ne correspond pas

```logql
# Logs ne correspondant pas au pattern DEBUG|TRACE
{app="payment"} !~ "DEBUG|TRACE"
```

#### ChaÃ®ner les filtres

```logql
# Logs contenant "error" ET "database"
{app="payment"} |= "error" |= "database"

# Logs contenant "error" mais pas "timeout"
{app="payment"} |= "error" != "timeout"
```

### Parsers

Si vos logs sont structurÃ©s (JSON, logfmt), vous pouvez les parser :

#### JSON Parser

```logql
{app="payment"} | json
```

Exemple de log JSON :
```json
{"level":"ERROR","message":"Payment failed","user_id":"12345"}
```

AprÃ¨s parsing, vous pouvez utiliser les champs :
```logql
{app="payment"} | json | level="ERROR"
```

#### Logfmt Parser

```logql
{app="payment"} | logfmt
```

Exemple de log logfmt :
```
level=error message="Payment failed" user_id=12345
```

#### Pattern Parser

Pour parser des logs non structurÃ©s :

```logql
{app="payment"}
| pattern `<timestamp> <level> <message>`
| level = "ERROR"
```

### AgrÃ©gations et MÃ©triques

#### count_over_time

Compte les logs sur une pÃ©riode :

```logql
# Nombre de logs par minute
count_over_time({app="payment"}[1m])

# Logs d'erreur par minute
count_over_time({app="payment"} |= "error"[1m])
```

#### rate

Taux de logs par seconde :

```logql
# Logs par seconde
rate({app="payment"}[5m])

# Erreurs par seconde
rate({app="payment"} |= "error"[5m])
```

#### bytes_over_time

Volume de logs en bytes :

```logql
bytes_over_time({app="payment"}[1h])
```

#### sum by

Grouper et sommer :

```logql
# Nombre de logs par namespace
sum by (namespace) (count_over_time({job="kubernetes-pods"}[1m]))

# Erreurs par application
sum by (app) (count_over_time({namespace="production"} |= "error"[5m]))
```

## CorrÃ©lation Logs-MÃ©triques : Le CÅ“ur du Sujet

La **vraie magie** de Loki avec Grafana, c'est la capacitÃ© Ã  **corrÃ©ler logs et mÃ©triques** dans la mÃªme interface.

### Pourquoi la CorrÃ©lation est Importante

**ScÃ©nario typique** :

1. Vous voyez une **mÃ©trique anormale** dans Prometheus :
   ```promql
   # Latence API soudainement Ã  5 secondes
   http_request_duration_seconds_p95 > 5
   ```

2. Vous vous demandez **"Pourquoi ?"**

3. Avec la corrÃ©lation, vous pouvez **immÃ©diatement voir les logs** au moment exact oÃ¹ la mÃ©trique a dÃ©viÃ©

### MÃ©thodes de CorrÃ©lation

#### 1. Split View dans Grafana Explore

**Grafana Explore** permet de visualiser logs ET mÃ©triques cÃ´te Ã  cÃ´te.

**Comment faire** :

1. Ouvrez **Grafana** â†’ **Explore**
2. En haut Ã  droite, cliquez sur **Split** (icÃ´ne de sÃ©paration)
3. Panneau gauche : SÃ©lectionnez datasource **Prometheus**
4. Panneau droit : SÃ©lectionnez datasource **Loki**

**Panneau Prometheus (gauche)** :
```promql
rate(http_requests_total{app="payment"}[5m])
```

**Panneau Loki (droit)** :
```logql
{app="payment"} |= "error"
```

**Synchronisation temporelle** :
- Les deux panneaux partagent le mÃªme time range
- Quand vous zoomez sur le graphique de mÃ©triques, les logs se mettent Ã  jour automatiquement
- Vous voyez **exactement** quels logs correspondent Ã  la spike de mÃ©triques

#### 2. Dashboards UnifiÃ©s

CrÃ©ez des dashboards qui affichent logs ET mÃ©triques :

**Exemple de dashboard** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [Payment Service Overview]                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  ğŸ“Š MÃ©triques                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Request Rate    [graph]                â”‚ â”‚
â”‚  â”‚ Error Rate      [graph]                â”‚ â”‚
â”‚  â”‚ Latency P95     [graph]                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                             â”‚
â”‚  ğŸ“‹ Logs                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Recent Errors   [log panel]            â”‚ â”‚
â”‚  â”‚ - 14:32:15 ERROR Payment timeout       â”‚ â”‚
â”‚  â”‚ - 14:31:42 ERROR DB connection lost    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Configuration d'un panneau de logs** :

1. **Add panel** â†’ **Logs**
2. **Data source** : Loki
3. **Query** :
   ```logql
   {app="payment-service"} |~ "ERROR|WARN"
   ```
4. **Options** :
   - **Show time** : Yes
   - **Order** : Newest first
   - **Limit** : 100

#### 3. Liens de Navigation

CrÃ©ez des **liens cliquables** entre panneaux de mÃ©triques et logs.

**Dans un panneau de mÃ©triques** :

1. **Panel settings** â†’ **Links**
2. **Add link** :
   - **Title** : "View Logs"
   - **URL** : Template vers Explore avec requÃªte Loki
   ```
   /explore?left={"datasource":"Loki","queries":[{"expr":"{app=\"payment-service\"}","refId":"A"}],"range":{"from":"now-5m","to":"now"}}
   ```

Maintenant, quand vous voyez une anomalie dans les mÃ©triques, un clic vous amÃ¨ne directement aux logs !

#### 4. Utiliser les MÃªmes Labels

**ClÃ© de la corrÃ©lation** : Utilisez les **mÃªmes labels** dans Prometheus et Loki.

**Dans votre application** :

```python
# MÃ©trique Prometheus
http_requests_total.labels(
    app="payment-service",
    namespace="production",
    method="POST"
).inc()

# Log avec les mÃªmes infos
logger.error(
    "Payment failed",
    extra={
        'app': 'payment-service',
        'namespace': 'production',
        'method': 'POST'
    }
)
```

**Promtail** extrait automatiquement `app` et `namespace` des labels Kubernetes.

**RÃ©sultat** :

```promql
# MÃ©trique Prometheus
http_requests_total{app="payment-service", namespace="production"}

# Logs Loki
{app="payment-service", namespace="production"}
```

MÃªme structure de labels = CorrÃ©lation facile ! ğŸ¯

### Exemple Pratique de CorrÃ©lation

#### ScÃ©nario : Debugging d'une DÃ©gradation de Performance

**Ã‰tape 1 : Alerte Prometheus**

Une alerte se dÃ©clenche :
```
Alert: HighLatency
Expr: http_request_duration_seconds_p95 > 2
For: 5m
```

**Ã‰tape 2 : Ouvrir Grafana Explore en Split View**

Panneau gauche (Prometheus) :
```promql
http_request_duration_seconds_p95{app="payment-service"}
```

Vous voyez un pic Ã  14:32.

**Ã‰tape 3 : RequÃªter les Logs au MÃªme Moment**

Panneau droit (Loki) :
```logql
{app="payment-service"} |~ "error|timeout|slow"
```

Time range : 14:25 - 14:35

**Ã‰tape 4 : Analyse des Logs**

Vous trouvez dans les logs :
```
14:32:12 WARN  Database query took 3.2s
14:32:15 ERROR Connection pool exhausted
14:32:18 ERROR Payment processing timeout after 5s
14:32:21 WARN  Database query took 4.1s
```

**Conclusion** : ProblÃ¨me de base de donnÃ©es !

**Ã‰tape 5 : Approfondir**

RequÃªte Loki plus prÃ©cise :
```logql
{app="payment-service"} |= "database" |~ "slow|timeout|error"
```

AgrÃ©gation pour voir l'ampleur :
```logql
sum by (level) (count_over_time({app="payment-service"} |= "database"[5m]))
```

**RÃ©sultat** :
- CorrÃ©lation claire entre le spike de latence et les erreurs DB
- Investigation en quelques minutes au lieu d'heures
- Vous savez maintenant oÃ¹ chercher (pool de connexions DB)

### Patterns de CorrÃ©lation Courants

#### Pattern 1 : Pic de Trafic â†’ Logs d'Erreur

**MÃ©trique** :
```promql
rate(http_requests_total[5m])
```

**Logs associÃ©s** :
```logql
{namespace="production"} |~ "ERROR|FAIL"
```

**Question** : Le pic de trafic cause-t-il plus d'erreurs ?

#### Pattern 2 : Erreur Rate â†’ Logs SpÃ©cifiques

**MÃ©trique** :
```promql
rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])
```

**Logs associÃ©s** :
```logql
{app="api"} | json | status >= 500
```

**Question** : Quels sont les messages d'erreur exacts ?

#### Pattern 3 : Resource Usage â†’ Application Logs

**MÃ©trique** :
```promql
container_memory_usage_bytes{pod=~"payment.*"}
```

**Logs associÃ©s** :
```logql
{app="payment-service"} |~ "OutOfMemory|allocation failed|GC"
```

**Question** : Y a-t-il des logs indiquant un problÃ¨me de mÃ©moire ?

#### Pattern 4 : DÃ©ploiement â†’ Impact

**MÃ©trique** :
```promql
# Avant/aprÃ¨s dÃ©ploiement
rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])
```

**Logs associÃ©s** :
```logql
{app="api"} |= "started" or "deployment"
```

**Annotation de dÃ©ploiement** dans Grafana pour marquer le moment du dÃ©ploiement.

**Question** : Le nouveau dÃ©ploiement a-t-il dÃ©gradÃ© les performances ?

## Labels DÃ©rivÃ©s : Enrichir les Logs

Loki permet de crÃ©er des **labels dÃ©rivÃ©s** Ã  partir du contenu des logs.

### Exemple : Extraire le Niveau de Log

Si vos logs ont un format :
```
2025-10-25 14:32:15 ERROR Payment failed
```

Configuration Promtail pour extraire `level` :

```yaml
pipeline_stages:
  - regex:
      expression: '^.* (?P<level>\w+) .*$'
  - labels:
      level:
```

Maintenant vous pouvez requÃªter :
```logql
{app="payment", level="ERROR"}
```

Au lieu de :
```logql
{app="payment"} |~ "ERROR"
```

**Avantage** : Le niveau devient un label indexÃ© = requÃªtes plus rapides.

### Exemple : Extraire le Status Code HTTP

Log :
```
POST /api/order HTTP/1.1 200 OK 0.123s
```

Pipeline Promtail :
```yaml
pipeline_stages:
  - regex:
      expression: '.* (?P<status_code>\d{3}) .*'
  - labels:
      status_code:
```

RequÃªte :
```logql
{app="api", status_code=~"5.."}  # Toutes les 5xx
```

**Attention** : Ne crÃ©ez pas trop de labels dÃ©rivÃ©s (risque de haute cardinalitÃ©).

## Grafana : Visualisations AvancÃ©es

### Log Panel avec DÃ©duplication

Parfois, les mÃªmes erreurs se rÃ©pÃ¨tent. Le panneau Logs de Grafana peut les dÃ©dupliquer :

**Options du panneau** :
- **Deduplication** : Signature
- Grafana regroupe les logs identiques et affiche le compte

**Affichage** :
```
[12x] 14:32:15 ERROR Payment timeout (repeated 12 times)
```

### Annotations Automatiques

Utilisez les logs pour crÃ©er des **annotations** automatiques sur les graphiques de mÃ©triques.

**Exemple** : Marquer chaque dÃ©ploiement

**RequÃªte Loki** :
```logql
{app="deployment-tracker"} |= "deployed version"
```

**Configuration** :
1. **Dashboard settings** â†’ **Annotations**
2. **Add annotation query**
3. **Data source** : Loki
4. **Query** : `{app="deployment-tracker"} |= "deployed"`

Maintenant, chaque dÃ©ploiement apparaÃ®t comme une ligne verticale sur vos graphiques !

### Alerting sur les Logs

Grafana peut crÃ©er des alertes basÃ©es sur les logs Loki :

**Exemple** : Alerte si plus de 10 erreurs par minute

**RequÃªte** :
```logql
sum(rate({app="payment"} |= "ERROR"[1m])) > 10
```

**Alert rule** :
1. **Alerting** â†’ **Alert rules** â†’ **New alert rule**
2. **Query** : La requÃªte LogQL ci-dessus
3. **Condition** : WHEN last() IS ABOVE 10
4. **Notification** : Slack, email, etc.

### Live Tail

**Grafana Explore** offre un mode "tail" en temps rÃ©el :

1. **Explore** â†’ **Loki**
2. Entrez votre requÃªte
3. Cliquez sur **Live** (en haut Ã  droite)

Les logs arrivent en **streaming en temps rÃ©el** (comme `tail -f`).

**Utile pour** :
- DÃ©boguer en live
- Suivre les dÃ©ploiements
- Monitoring actif pendant un incident

## Cas d'Usage Pratiques

### 1. Dashboard de Monitoring Complet

**Panels** :

1. **Request Rate** (Prometheus)
   ```promql
   rate(http_requests_total[5m])
   ```

2. **Error Rate** (Prometheus)
   ```promql
   rate(http_requests_total{status=~"5.."}[5m])
   ```

3. **Recent Errors** (Loki)
   ```logql
   {app="api"} |~ "ERROR|FAIL" | json | line_format "{{.timestamp}} {{.level}} {{.message}}"
   ```

4. **Latency P95** (Prometheus)
   ```promql
   histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
   ```

5. **Slow Queries** (Loki)
   ```logql
   {app="api"} |= "slow query" | logfmt | duration > 1000
   ```

### 2. Dashboard d'Investigation d'Incident

**Variables de dashboard** :
- `$namespace` : Liste des namespaces
- `$app` : Liste des applications
- `$timerange` : PÃ©riode d'investigation

**Panels** :

1. **Timeline des Ã‰vÃ©nements**
   - MÃ©triques + Logs + Annotations de dÃ©ploiement

2. **Volume de Logs par Niveau**
   ```logql
   sum by (level) (count_over_time({namespace="$namespace", app="$app"}[1m]))
   ```

3. **Top 10 Messages d'Erreur**
   ```logql
   topk(10, sum by (message) (count_over_time({namespace="$namespace", app="$app"} |= "ERROR"[$timerange])))
   ```

4. **Logs DÃ©taillÃ©s Filtrables**
   ```logql
   {namespace="$namespace", app="$app"} |~ "ERROR|WARN"
   ```

### 3. Dashboard de SantÃ© Application

Pour chaque application critique :

**Section MÃ©triques** :
- Throughput
- Latency
- Error rate
- Resource usage (CPU, memory)

**Section Logs** :
- Recent errors (last 15 min)
- Warnings count
- Critical events

**Lien vers Explore** pour investigation approfondie

## Performance et Optimisation

### Limiter le Volume de Logs

**Dans Promtail** : Filtrez les logs non essentiels avant de les envoyer Ã  Loki.

```yaml
scrape_configs:
  - job_name: kubernetes-pods
    pipeline_stages:
      # Ne pas envoyer les logs DEBUG en production
      - match:
          selector: '{namespace="production"}'
          stages:
            - drop:
                expression: ".*DEBUG.*"

      # Limiter la taille des logs
      - limit:
          rate: 1000
          burst: 2000
```

### CardinalitÃ© des Labels

**RÃ¨gle d'or** : Gardez la cardinalitÃ© des labels **basse**.

**âŒ Mauvais** :
```yaml
# CrÃ©era des millions de stream
- labels:
    user_id:  # Des millions d'utilisateurs
    request_id:  # Chaque requÃªte unique
```

**âœ… Bon** :
```yaml
# CardinalitÃ© contrÃ´lÃ©e
- labels:
    app:
    namespace:
    level:
    pod:
```

**Limite recommandÃ©e** : < 10 000 streams par pod.

### RÃ©tention et Compaction

**Configuration Loki** :

```yaml
limits_config:
  retention_period: 168h  # 7 jours

compactor:
  working_directory: /loki/compactor
  shared_store: filesystem
  retention_enabled: true
```

**Ajustez selon vos besoins** :
- Lab/dev : 3-7 jours
- Production : 30-90 jours
- Logs critiques : 1 an+

### Queries Efficaces

**âŒ RequÃªte inefficace** :
```logql
{namespace="production"} |~ ".*"  # Trop large, lit tout
```

**âœ… RequÃªte efficace** :
```logql
{namespace="production", app="payment"} |= "error"  # CiblÃ©
```

**Tips** :
- Utilisez autant de labels que possible dans le sÃ©lecteur
- Filtrez tÃ´t avec `|=` ou `|~`
- Limitez le time range quand possible

## Comparaison Finale : Loki vs EFK

| CritÃ¨re | Loki | EFK |
|---------|------|-----|
| **Ressources** | âœ… TrÃ¨s lÃ©ger | âŒ Lourd |
| **Setup** | âœ… Simple | âš ï¸ Complexe |
| **Recherche full-text** | âš ï¸ Plus lent | âœ… TrÃ¨s rapide |
| **Recherche par labels** | âœ… TrÃ¨s rapide | âš ï¸ Moyen |
| **IntÃ©gration Grafana** | âœ… Native | âš ï¸ Possible |
| **CorrÃ©lation mÃ©triques** | âœ… Excellente | âŒ Manuelle |
| **CoÃ»t stockage** | âœ… Faible | âŒ Ã‰levÃ© |
| **Courbe apprentissage** | âœ… Facile (si PromQL) | âš ï¸ Moyenne |
| **Cas d'usage** | Lab, K8s observability | Production complexe |

## Bonnes Pratiques avec Loki

### 1. Logs StructurÃ©s

**PrÃ©fÃ©rez JSON** :
```json
{"level":"ERROR","msg":"Payment failed","duration":3.2,"user_id":"12345"}
```

Puis parsez avec `| json` pour exploiter tous les champs.

### 2. Labels AppropriÃ©s

**Bons labels** :
- `namespace`, `app`, `pod`, `container` (automatiques via K8s)
- `level` (ERROR, WARN, INFO) si extrait
- `environment` (prod, staging, dev)

**Mauvais labels** :
- Toute valeur Ã  haute cardinalitÃ©
- DonnÃ©es temporelles
- IDs uniques

### 3. Utilisez les Filtres

**Filtrez tÃ´t** pour de meilleures performances :
```logql
{app="payment"} |= "error" != "ignore" |~ "timeout|failed"
```

### 4. Nommage CohÃ©rent avec Prometheus

Utilisez les mÃªmes noms de labels :
```
Prometheus: http_requests_total{app="payment", namespace="prod"}
Loki: {app="payment", namespace="prod"}
```

### 5. Dashboards Template

CrÃ©ez des **dashboard templates** rÃ©utilisables :
- Un template pour chaque type d'application (API, worker, frontend)
- Variables pour `$namespace`, `$app`
- Dupliquez et adaptez

## Ce Qu'il Faut Retenir

ğŸš€ **Loki est idÃ©al pour MicroK8s** :
- LÃ©ger : 10x moins de ressources qu'Elasticsearch
- Simple : 2 composants (Loki + Promtail)
- IntÃ©grÃ© : Natif dans Grafana

ğŸ“Š **CorrÃ©lation logs-mÃ©triques** :
- MÃªme interface (Grafana)
- Split view pour voir logs + mÃ©triques cÃ´te Ã  cÃ´te
- Dashboards unifiÃ©s
- MÃªmes labels = corrÃ©lation facile

ğŸ” **LogQL** :
- Similaire Ã  PromQL
- SÃ©lecteurs de labels + filtres de contenu
- Parsers pour logs structurÃ©s
- AgrÃ©gations pour analytics

ğŸ¯ **Labels sont clÃ©s** :
- IndexÃ©s = recherche rapide
- Mais attention Ã  la cardinalitÃ©
- CohÃ©rence avec Prometheus

ğŸ’¡ **Utilisez les deux piliers ensemble** :
- Prometheus : MÃ©triques numÃ©riques
- Loki : Logs textuels
- Correlation = Debugging ultra-rapide

âš™ï¸ **Configuration optimale** :
- Promtail en DaemonSet
- Loki en StatefulSet avec stockage persistant
- RÃ©tention adaptÃ©e (7-30 jours)
- Filtrage des logs non essentiels

ğŸ“ˆ **Prochaine Ã©tape** :
- Section 15.5 : Tracing distribuÃ© avec Jaeger (le 3Ã¨me pilier)
- Vous aurez alors les trois piliers complets de l'observabilitÃ© !

Avec Loki, vous avez maintenant une solution lÃ©gÃ¨re et puissante pour centraliser vos logs et les corrÃ©ler avec vos mÃ©triques Prometheus. L'observabilitÃ© de votre cluster MicroK8s est dÃ©sormais de niveau professionnel ! ğŸ‰

â­ï¸ [Tracing distribuÃ© avec Jaeger](/15-observabilite-avancee/05-tracing-distribue-avec-jaeger.md)
