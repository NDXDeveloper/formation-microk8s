üîù Retour au [Sommaire](/SOMMAIRE.md)

# 22.4 Snapshots de volumes

## Introduction

Les **snapshots de volumes** sont des copies instantan√©es de vos volumes persistants √† un moment pr√©cis. Imaginez qu'ils fonctionnent comme une "photo" de l'√©tat d'un disque dur √† un instant T. Cette section explore comment cr√©er, g√©rer et restaurer des snapshots de volumes dans le contexte de MicroK8s.

### Analogie pour comprendre les snapshots

Pensez √† un snapshot comme √† une **machine √† voyager dans le temps** pour vos donn√©es :

- **Photo traditionnelle** (backup par fichier) : Vous prenez en photo chaque objet de votre maison, un par un. Cela prend du temps, mais vous pouvez choisir exactement ce que vous photographiez.

- **Snapshot** : Vous figez instantan√©ment toute la maison dans le temps. En un clic, tout est captur√© simultan√©ment. C'est beaucoup plus rapide, mais vous capturez tout ou rien.

Les snapshots sont particuli√®rement utiles pour les applications avec beaucoup de donn√©es, comme les bases de donn√©es, o√π la vitesse et la coh√©rence sont cruciales.

## Diff√©rence entre snapshots et backups par fichier

Comprendre la diff√©rence est essentiel pour choisir la bonne approche.

### Snapshots de volumes (natifs)

**Comment √ßa fonctionne** :
- Le syst√®me de stockage sous-jacent cr√©e une "image" du volume
- C'est quasi-instantan√© (quelques secondes)
- Utilise des techniques comme le Copy-on-Write (CoW)
- G√©r√© au niveau du syst√®me de stockage, pas au niveau fichier

**Avantages** :
- ‚ö° Extr√™mement rapide (secondes, m√™me pour des To)
- üéØ Coh√©rence garantie (tout est captur√© au m√™me instant)
- üíæ √âconome en espace (seuls les changements sont stock√©s)
- üîÑ Restauration tr√®s rapide

**Inconv√©nients** :
- üîí N√©cessite un support au niveau du stockage
- üí∞ Souvent limit√© aux solutions cloud (AWS EBS, GCP Disk, Azure)
- üè† Difficile √† impl√©menter sur du stockage local simple

### Backups par fichier (Restic/Kopia)

**Comment √ßa fonctionne** :
- Copie fichier par fichier le contenu du volume
- Peut prendre plusieurs minutes √† heures selon la taille
- Fonctionne avec n'importe quel type de stockage

**Avantages** :
- üåç Universel (fonctionne partout)
- üéØ S√©lectif (peut exclure certains fichiers)
- üîê Chiffrement natif
- üíæ D√©duplication (√©conomie d'espace)

**Inconv√©nients** :
- üêå Plus lent (proportionnel √† la taille des donn√©es)
- ‚öôÔ∏è Utilise plus de ressources CPU/RAM
- ‚è±Ô∏è Moins de garantie de coh√©rence instantan√©e

### Tableau comparatif

| Crit√®re | Snapshots natifs | Restic/Kopia |
|---------|------------------|--------------|
| Vitesse de cr√©ation | Secondes | Minutes √† heures |
| Vitesse de restauration | Secondes √† minutes | Minutes √† heures |
| Espace requis | Optimis√© (CoW) | D√©duplication |
| Compatibilit√© | Limit√©e (cloud) | Universelle |
| Coh√©rence | Parfaite | Bonne (avec hooks) |
| Co√ªt | Variable (cloud) | Gratuit |
| Complexit√© | Simple | Mod√©r√©e |

### Quelle approche choisir ?

**Utilisez les snapshots natifs si** :
- Vous √™tes sur un cloud provider (AWS, GCP, Azure)
- Vous avez de gros volumes (> 100 GB)
- La vitesse est critique
- Votre stockage supporte les snapshots

**Utilisez Restic/Kopia si** :
- Vous √™tes sur MicroK8s avec stockage local (hostpath)
- Vous voulez du chiffrement de bout en bout
- Vous voulez de la d√©duplication
- Vous n'avez pas de support de snapshot natif

**Pour MicroK8s** : Dans la plupart des cas, vous utiliserez **Restic/Kopia** car le stockage par d√©faut (`hostpath-storage`) ne supporte pas les snapshots natifs.

## Comment fonctionnent les snapshots

### Principe du Copy-on-Write (CoW)

La plupart des syst√®mes de snapshot utilisent la technique **Copy-on-Write** :

1. **Cr√©ation du snapshot** :
   - Le syst√®me marque tous les blocs actuels comme "snapshot"
   - Aucune copie r√©elle n'est faite (instantan√© !)

2. **Modifications ult√©rieures** :
   - Quand un bloc est modifi√©, l'ancienne version est copi√©e dans le snapshot
   - La nouvelle version est √©crite normalement

3. **R√©sultat** :
   - Le snapshot ne contient que les diff√©rences
   - Tr√®s √©conome en espace

**Illustration** :

```
√âtat initial (100 GB):
Volume: [A][B][C][D][E]...

Cr√©ation snapshot (0 GB suppl√©mentaire):
Volume:    [A][B][C][D][E]...
Snapshot:  -> pointe vers Volume

Modification de B et D (2 GB):
Volume:    [A][B'][C][D'][E]...
Snapshot:  [B][D] (stocke les anciennes versions)

Espace utilis√©: 102 GB au lieu de 200 GB
```

### Snapshots incr√©mentaux vs complets

**Snapshots complets** :
- Capture tout l'√©tat du volume
- Premier snapshot toujours complet
- Taille = taille des donn√©es utilis√©es

**Snapshots incr√©mentaux** :
- Ne stockent que les diff√©rences depuis le dernier snapshot
- Beaucoup plus compacts
- D√©pendent du snapshot pr√©c√©dent

La plupart des syst√®mes font automatiquement de l'incr√©mental.

## Snapshots dans Kubernetes

Kubernetes a standardis√© la gestion des snapshots avec les **VolumeSnapshots**.

### Architecture des VolumeSnapshots

Kubernetes utilise trois ressources principales :

1. **VolumeSnapshot** : La demande de snapshot (comme un PVC pour les volumes)
2. **VolumeSnapshotClass** : Le "type" de snapshot (comme StorageClass)
3. **VolumeSnapshotContent** : Le snapshot r√©el (comme un PV)

**Analogie** :
- **VolumeSnapshot** = "Je veux une photo de ce volume"
- **VolumeSnapshotClass** = "Utilise cet appareil photo"
- **VolumeSnapshotContent** = La photo elle-m√™me

### Pr√©requis pour les snapshots Kubernetes

Pour utiliser les VolumeSnapshots natifs, il faut :

1. **Un CSI driver** qui supporte les snapshots
2. **VolumeSnapshot CRDs** install√©s dans le cluster
3. **Snapshot Controller** en cours d'ex√©cution
4. **Stockage sous-jacent** qui supporte les snapshots

### Limitations avec MicroK8s hostpath

Par d√©faut, MicroK8s utilise `hostpath-storage` qui **ne supporte pas** les snapshots natifs car :
- Pas de CSI driver avec support snapshot
- Stockage local simple (pas de CoW)
- Pas de syst√®me de fichiers avanc√© (ZFS, Btrfs) configur√©

**Solutions pour MicroK8s** :
1. Utiliser Restic/Kopia avec Velero (recommand√©)
2. Installer un CSI driver compatible (OpenEBS, Longhorn)
3. Utiliser un syst√®me de fichiers avec snapshots (ZFS, Btrfs)

## Installer le support des snapshots sur MicroK8s

Si vous voulez vraiment utiliser des snapshots natifs avec MicroK8s, voici les options.

### Option 1 : OpenEBS (recommand√©)

**OpenEBS** est une solution de stockage cloud-native qui supporte les snapshots.

#### Installation d'OpenEBS

```bash
# Activer l'addon OpenEBS (si disponible)
microk8s enable openebs

# Ou installer manuellement avec Helm
microk8s helm3 repo add openebs https://openebs.github.io/charts
microk8s helm3 repo update
microk8s helm3 install openebs openebs/openebs -n openebs --create-namespace
```

#### Installer les snapshot CRDs

```bash
# Installer les CRDs VolumeSnapshot
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml

# Installer le Snapshot Controller
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml
```

#### V√©rifier l'installation

```bash
# V√©rifier les CRDs
kubectl get crd | grep snapshot

# V√©rifier le Snapshot Controller
kubectl get pods -n kube-system | grep snapshot-controller

# V√©rifier OpenEBS
kubectl get pods -n openebs
```

#### Cr√©er une VolumeSnapshotClass pour OpenEBS

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: openebs-snapshot-class
driver: cstor.csi.openebs.io
deletionPolicy: Delete
```

Appliquer :

```bash
kubectl apply -f openebs-snapshot-class.yaml
```

### Option 2 : Longhorn

**Longhorn** est une autre solution de stockage distribu√©e qui supporte les snapshots.

```bash
# Installer Longhorn avec Helm
microk8s helm3 repo add longhorn https://charts.longhorn.io
microk8s helm3 repo update
microk8s helm3 install longhorn longhorn/longhorn \
  --namespace longhorn-system \
  --create-namespace
```

Les CRDs de snapshot et le controller doivent √™tre install√©s de la m√™me mani√®re qu'avec OpenEBS.

### Option 3 : ZFS sur le syst√®me h√¥te

Si vous √™tes √† l'aise avec Linux, vous pouvez utiliser ZFS :

```bash
# Installer ZFS (Ubuntu)
sudo apt install zfsutils-linux

# Cr√©er un pool ZFS
sudo zpool create mypool /dev/sdX

# Cr√©er un filesystem ZFS pour MicroK8s
sudo zfs create mypool/microk8s

# Monter pour MicroK8s
sudo zfs set mountpoint=/var/snap/microk8s/common/volumes mypool/microk8s
```

Ensuite, utilisez des scripts pour cr√©er des snapshots ZFS manuellement.

### Option 4 : Rester avec Restic/Kopia (le plus simple)

Pour un lab MicroK8s, **la solution la plus pragmatique** reste d'utiliser Restic/Kopia via Velero, comme vu dans les sections pr√©c√©dentes. C'est :
- Plus simple √† mettre en place
- Fonctionne imm√©diatement
- Ne n√©cessite pas de stockage sp√©cial
- Suffisant pour la plupart des besoins de lab

## Cr√©er des snapshots de volumes

### Avec VolumeSnapshot natif (si support√©)

Supposons que vous avez install√© OpenEBS et les CRDs.

#### Cr√©er un snapshot d'un PVC existant

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: pvc-snapshot-demo
  namespace: default
spec:
  volumeSnapshotClassName: openebs-snapshot-class
  source:
    persistentVolumeClaimName: data-pvc
```

Appliquer :

```bash
kubectl apply -f volume-snapshot.yaml
```

#### V√©rifier le snapshot

```bash
# Lister les snapshots
kubectl get volumesnapshot

# D√©tails du snapshot
kubectl describe volumesnapshot pvc-snapshot-demo
```

R√©sultat attendu :

```
NAME                AGE   READY   SOURCE PVC   SOURCE SNAPSHOT   SIZE
pvc-snapshot-demo   10s   true    data-pvc                       5Gi
```

#### Cr√©er un snapshot programmatiquement

Vous pouvez automatiser avec un script :

```bash
#!/bin/bash
PVC_NAME="data-pvc"
NAMESPACE="default"
SNAPSHOT_NAME="${PVC_NAME}-$(date +%Y%m%d-%H%M%S)"

kubectl apply -f - <<EOF
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: ${SNAPSHOT_NAME}
  namespace: ${NAMESPACE}
spec:
  volumeSnapshotClassName: openebs-snapshot-class
  source:
    persistentVolumeClaimName: ${PVC_NAME}
EOF

echo "Snapshot cr√©√©: ${SNAPSHOT_NAME}"
```

### Avec Velero (approche Restic/Kopia)

Si vous utilisez Velero avec Restic/Kopia :

```bash
# Cr√©er un backup incluant les volumes
velero backup create snapshot-backup \
  --include-namespaces myapp \
  --default-volumes-to-fs-backup

# V√©rifier
velero backup describe snapshot-backup
```

Les "snapshots" sont en r√©alit√© des copies par fichier, mais du point de vue de l'utilisateur, le r√©sultat est similaire.

## Restaurer depuis un snapshot

### Restauration avec VolumeSnapshot natif

Pour restaurer depuis un snapshot, vous cr√©ez un **nouveau PVC** qui utilise le snapshot comme source.

#### Cr√©er un PVC depuis un snapshot

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: restored-pvc
  namespace: default
spec:
  dataSource:
    name: pvc-snapshot-demo
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: openebs-cstor-csi
```

Appliquer :

```bash
kubectl apply -f restored-pvc.yaml
```

#### Utiliser le PVC restaur√© dans un Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: restored-app
  namespace: default
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: restored-pvc
```

Le Pod utilisera les donn√©es telles qu'elles √©taient au moment du snapshot.

### Restauration avec Velero

```bash
# Restaurer tout le backup (incluant les volumes)
velero restore create --from-backup snapshot-backup

# Ou restaurer seulement un namespace
velero restore create --from-backup snapshot-backup \
  --include-namespaces myapp
```

Velero restaure automatiquement les PVCs avec leurs donn√©es.

## Gestion des snapshots

### Lister les snapshots

```bash
# Tous les snapshots
kubectl get volumesnapshot --all-namespaces

# Dans un namespace sp√©cifique
kubectl get volumesnapshot -n default

# Format d√©taill√©
kubectl get volumesnapshot -o wide
```

### Voir les d√©tails d'un snapshot

```bash
kubectl describe volumesnapshot pvc-snapshot-demo
```

Informations affich√©es :
- Taille du snapshot
- PVC source
- √âtat (Ready/NotReady)
- VolumeSnapshotContent associ√©
- Erreurs √©ventuelles

### Supprimer un snapshot

```bash
kubectl delete volumesnapshot pvc-snapshot-demo
```

**Attention** : Selon la `deletionPolicy` de la VolumeSnapshotClass, cela peut :
- Supprimer le snapshot sous-jacent (`Delete`)
- Conserver le snapshot mais le d√©tacher (`Retain`)

### D√©finir la politique de suppression

Dans la VolumeSnapshotClass :

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: openebs-snapshot-class
driver: cstor.csi.openebs.io
deletionPolicy: Retain  # ou Delete
```

- **Delete** : Supprime le snapshot quand la ressource est supprim√©e
- **Retain** : Conserve le snapshot (gestion manuelle n√©cessaire)

## Automatisation des snapshots

### Snapshots programm√©s avec CronJobs

Cr√©ez un CronJob qui cr√©e des snapshots r√©guli√®rement :

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: daily-volume-snapshot
  namespace: default
spec:
  schedule: "0 2 * * *"  # Tous les jours √† 2h
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: snapshot-creator
          containers:
          - name: kubectl
            image: bitnami/kubectl:latest
            command:
            - /bin/bash
            - -c
            - |
              PVC_NAME="data-pvc"
              SNAPSHOT_NAME="daily-snapshot-$(date +%Y%m%d-%H%M%S)"

              kubectl apply -f - <<EOF
              apiVersion: snapshot.storage.k8s.io/v1
              kind: VolumeSnapshot
              metadata:
                name: ${SNAPSHOT_NAME}
                namespace: default
              spec:
                volumeSnapshotClassName: openebs-snapshot-class
                source:
                  persistentVolumeClaimName: ${PVC_NAME}
              EOF

              echo "Snapshot cr√©√©: ${SNAPSHOT_NAME}"
          restartPolicy: OnFailure
```

#### Cr√©er le ServiceAccount n√©cessaire

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: snapshot-creator
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: snapshot-creator-role
  namespace: default
rules:
- apiGroups: ["snapshot.storage.k8s.io"]
  resources: ["volumesnapshots"]
  verbs: ["create", "get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: snapshot-creator-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: snapshot-creator-role
subjects:
- kind: ServiceAccount
  name: snapshot-creator
  namespace: default
```

### Nettoyage automatique des vieux snapshots

CronJob pour supprimer les snapshots de plus de 7 jours :

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cleanup-old-snapshots
  namespace: default
spec:
  schedule: "0 3 * * *"  # Tous les jours √† 3h
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: snapshot-creator
          containers:
          - name: kubectl
            image: bitnami/kubectl:latest
            command:
            - /bin/bash
            - -c
            - |
              # Supprimer les snapshots de plus de 7 jours
              CUTOFF_DATE=$(date -d '7 days ago' +%s)

              kubectl get volumesnapshot -o json | \
              jq -r '.items[] |
                select(.metadata.creationTimestamp | fromdateiso8601 < '$CUTOFF_DATE') |
                .metadata.name' | \
              while read snapshot; do
                echo "Suppression de $snapshot"
                kubectl delete volumesnapshot $snapshot
              done
          restartPolicy: OnFailure
```

### Avec Velero Schedules

Si vous utilisez Velero, les schedules cr√©ent automatiquement des "snapshots" (backups) :

```bash
velero schedule create daily-snapshot \
  --schedule="0 2 * * *" \
  --ttl 168h \
  --default-volumes-to-fs-backup
```

La gestion de la r√©tention est automatique avec le TTL.

## Snapshots pour les bases de donn√©es

Les bases de donn√©es n√©cessitent une attention particuli√®re pour garantir la coh√©rence.

### Principe : Freeze/Thaw

Pour garantir la coh√©rence :

1. **Freeze** : Figer les √©critures (flush les buffers)
2. **Snapshot** : Cr√©er le snapshot
3. **Thaw** : Reprendre les √©critures

### PostgreSQL

#### Avec des hooks dans un Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: postgres
  annotations:
    # Avant snapshot : checkpoint et freeze
    pre.hook.backup.velero.io/command: >-
      ["/bin/bash", "-c",
       "psql -U postgres -c 'CHECKPOINT;' &&
        psql -U postgres -c 'SELECT pg_start_backup(\"snapshot\");'"]

    # Apr√®s snapshot : reprendre
    post.hook.backup.velero.io/command: >-
      ["/bin/bash", "-c",
       "psql -U postgres -c 'SELECT pg_stop_backup();'"]
```

#### Script manuel pour snapshot

```bash
#!/bin/bash
POD_NAME="postgres-pod"
NAMESPACE="database"
PVC_NAME="postgres-data"

# Freeze PostgreSQL
kubectl exec -n $NAMESPACE $POD_NAME -- \
  psql -U postgres -c "SELECT pg_start_backup('snapshot');"

# Cr√©er le snapshot
kubectl apply -f - <<EOF
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: postgres-snapshot-$(date +%Y%m%d-%H%M%S)
  namespace: $NAMESPACE
spec:
  volumeSnapshotClassName: openebs-snapshot-class
  source:
    persistentVolumeClaimName: $PVC_NAME
EOF

# Attendre que le snapshot soit pr√™t
sleep 5

# Thaw PostgreSQL
kubectl exec -n $NAMESPACE $POD_NAME -- \
  psql -U postgres -c "SELECT pg_stop_backup();"

echo "Snapshot PostgreSQL cr√©√© avec succ√®s"
```

### MySQL/MariaDB

```bash
#!/bin/bash
POD_NAME="mysql-pod"
NAMESPACE="database"

# Flush et lock
kubectl exec -n $NAMESPACE $POD_NAME -- \
  mysql -u root -p$MYSQL_ROOT_PASSWORD -e "FLUSH TABLES WITH READ LOCK;"

# Cr√©er snapshot
# ... (m√™me logique que PostgreSQL)

# Unlock
kubectl exec -n $NAMESPACE $POD_NAME -- \
  mysql -u root -p$MYSQL_ROOT_PASSWORD -e "UNLOCK TABLES;"
```

### MongoDB

```bash
#!/bin/bash
POD_NAME="mongodb-pod"
NAMESPACE="database"

# Forcer un sync
kubectl exec -n $NAMESPACE $POD_NAME -- \
  mongo admin -u root -p$MONGO_ROOT_PASSWORD --eval "db.fsyncLock()"

# Cr√©er snapshot
# ...

# Unlock
kubectl exec -n $NAMESPACE $POD_NAME -- \
  mongo admin -u root -p$MONGO_ROOT_PASSWORD --eval "db.fsyncUnlock()"
```

## Snapshots et clonage de volumes

Les snapshots sont parfaits pour le clonage rapide d'environnements.

### Cr√©er un environnement de staging depuis production

#### √âtape 1 : Cr√©er un snapshot de production

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: prod-snapshot
  namespace: production
spec:
  volumeSnapshotClassName: openebs-snapshot-class
  source:
    persistentVolumeClaimName: production-data
```

#### √âtape 2 : Cr√©er un PVC de staging depuis le snapshot

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: staging-data
  namespace: staging
spec:
  dataSource:
    name: prod-snapshot
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
    # Note: Le snapshot peut √™tre dans un autre namespace
    # mais cela d√©pend du CSI driver
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: openebs-cstor-csi
```

#### √âtape 3 : D√©ployer l'application de staging

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: staging-app
  namespace: staging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
      env: staging
  template:
    metadata:
      labels:
        app: myapp
        env: staging
    spec:
      containers:
      - name: app
        image: myapp:latest
        volumeMounts:
        - name: data
          mountPath: /data
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: staging-data
```

### Clonage pour les d√©veloppeurs

Cr√©er rapidement un environnement de d√©veloppement :

```bash
#!/bin/bash
# Script pour cloner un environnement

SOURCE_NS="production"
SOURCE_PVC="app-data"
TARGET_NS="dev-john"
TARGET_PVC="app-data"

# Cr√©er le namespace de dev
kubectl create namespace $TARGET_NS

# Cr√©er snapshot
SNAPSHOT_NAME="clone-$(date +%Y%m%d-%H%M%S)"
kubectl apply -f - <<EOF
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: ${SNAPSHOT_NAME}
  namespace: ${SOURCE_NS}
spec:
  volumeSnapshotClassName: openebs-snapshot-class
  source:
    persistentVolumeClaimName: ${SOURCE_PVC}
EOF

# Attendre que le snapshot soit pr√™t
echo "Attente du snapshot..."
kubectl wait --for=condition=ready volumesnapshot/${SNAPSHOT_NAME} -n ${SOURCE_NS} --timeout=300s

# Cr√©er PVC dans le namespace dev
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ${TARGET_PVC}
  namespace: ${TARGET_NS}
spec:
  dataSource:
    name: ${SNAPSHOT_NAME}
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: openebs-cstor-csi
EOF

echo "Environnement de dev ${TARGET_NS} cr√©√© avec succ√®s"
```

## Monitoring et m√©triques des snapshots

### Surveiller l'√©tat des snapshots

Script de surveillance basique :

```bash
#!/bin/bash
# V√©rifier l'√©tat de tous les snapshots

echo "=== √âtat des VolumeSnapshots ==="
kubectl get volumesnapshot --all-namespaces -o json | \
jq -r '.items[] |
  "\(.metadata.namespace)/\(.metadata.name): \(.status.readyToUse) (Age: \(.metadata.creationTimestamp))"'

echo ""
echo "=== Snapshots non pr√™ts ==="
kubectl get volumesnapshot --all-namespaces -o json | \
jq -r '.items[] |
  select(.status.readyToUse != true) |
  "\(.metadata.namespace)/\(.metadata.name): \(.status)"'
```

### M√©triques Prometheus

Si vous utilisez le Snapshot Controller avec Prometheus :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: snapshot-controller-metrics
  namespace: kube-system
  labels:
    app: snapshot-controller
spec:
  selector:
    app: snapshot-controller
  ports:
  - name: metrics
    port: 8080
    targetPort: 8080
```

M√©triques disponibles :
- `snapshot_controller_operation_total_seconds` : Dur√©e des op√©rations
- `snapshot_controller_operations_total` : Nombre d'op√©rations
- `snapshot_controller_volumesnapshot_count` : Nombre de snapshots

### Alertes recommand√©es

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: volume-snapshot-alerts
  namespace: monitoring
spec:
  groups:
  - name: snapshots
    interval: 30s
    rules:
    - alert: VolumeSnapshotFailed
      expr: |
        snapshot_controller_operations_total{status="failed"} > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Volume snapshot operation failed"
        description: "A volume snapshot operation has failed"

    - alert: VolumeSnapshotStuck
      expr: |
        time() - snapshot_controller_operation_total_seconds > 3600
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "Volume snapshot stuck"
        description: "A snapshot operation has been running for over 1 hour"
```

## Limitations et consid√©rations

### Limitations g√©n√©rales

1. **D√©pendance au stockage** :
   - N√©cessite un CSI driver compatible
   - Pas universel comme Restic/Kopia

2. **Performance** :
   - M√™me si "instantan√©", il y a un overhead
   - Impact possible sur les performances du stockage

3. **Espace disque** :
   - Les snapshots consomment de l'espace
   - Plus il y a de modifications, plus ils prennent de place

4. **Cha√Æne de d√©pendances** :
   - Les snapshots incr√©mentaux d√©pendent des pr√©c√©dents
   - Supprimer un snapshot parent peut causer des probl√®mes

### Limitations sp√©cifiques MicroK8s

1. **Hostpath par d√©faut** :
   - Pas de support natif des snapshots
   - N√©cessite installation d'OpenEBS/Longhorn

2. **Ressources limit√©es** :
   - OpenEBS peut √™tre gourmand en ressources pour un lab
   - Restic/Kopia est souvent plus adapt√©

3. **Complexit√©** :
   - Setup plus complexe que Velero seul
   - Courbe d'apprentissage suppl√©mentaire

### Quand ne pas utiliser les snapshots natifs

**√âvitez les snapshots natifs si** :
- Vous avez un simple lab MicroK8s sur un laptop
- Vous manquez de ressources (< 8 GB RAM)
- Vous voulez la simplicit√©
- Vous n'avez pas de stockage avanc√©

**Privil√©giez Restic/Kopia si** :
- Vous d√©butez avec Kubernetes
- Vous voulez une solution qui marche partout
- Vous voulez du chiffrement int√©gr√©
- Vous voulez de la d√©duplication

## Comparaison des solutions de snapshot pour MicroK8s

### Tableau r√©capitulatif

| Solution | Complexit√© | Performance | Ressources | Universel | Recommand√© pour |
|----------|-----------|-------------|------------|-----------|-----------------|
| Hostpath + Restic | ‚≠ê Faible | ‚≠ê‚≠ê Moyenne | ‚≠ê‚≠ê‚≠ê Faible | ‚úÖ Oui | Labs, d√©butants |
| OpenEBS | ‚≠ê‚≠ê‚≠ê √âlev√©e | ‚≠ê‚≠ê‚≠ê Bonne | ‚≠ê‚≠ê Moyenne | ‚ùå Non | Production, apprentissage avanc√© |
| Longhorn | ‚≠ê‚≠ê‚≠ê √âlev√©e | ‚≠ê‚≠ê‚≠ê Bonne | ‚≠ê‚≠ê Moyenne | ‚ùå Non | Multi-node, HA |
| ZFS manuel | ‚≠ê‚≠ê‚≠ê‚≠ê Tr√®s √©lev√©e | ‚≠ê‚≠ê‚≠ê‚≠ê Excellente | ‚≠ê‚≠ê‚≠ê Faible | ‚ùå Non | Experts Linux |
| Cloud provider | ‚≠ê‚≠ê Moyenne | ‚≠ê‚≠ê‚≠ê‚≠ê Excellente | ‚≠ê‚≠ê‚≠ê Faible | ‚ùå Non | Production cloud |

### Recommandation finale pour MicroK8s

Pour un **lab personnel** :
```
Hostpath + Restic/Kopia via Velero
```

Pour un **environnement de production l√©ger** :
```
OpenEBS avec VolumeSnapshots natifs
```

Pour un **cluster multi-node** :
```
Longhorn avec snapshots et r√©plication
```

Pour un **apprentissage approfondi** :
```
OpenEBS ou ZFS pour comprendre les m√©canismes
```

## Scripts et automatisation utiles

### Script complet de gestion des snapshots

```bash
#!/bin/bash
# snapshot-manager.sh - Outil de gestion des snapshots

ACTION=$1
PVC_NAME=$2
NAMESPACE=${3:-default}
SNAPSHOT_CLASS=${4:-openebs-snapshot-class}

case $ACTION in
  create)
    SNAPSHOT_NAME="${PVC_NAME}-$(date +%Y%m%d-%H%M%S)"
    echo "Cr√©ation du snapshot: $SNAPSHOT_NAME"

    kubectl apply -f - <<EOF
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: ${SNAPSHOT_NAME}
  namespace: ${NAMESPACE}
spec:
  volumeSnapshotClassName: ${SNAPSHOT_CLASS}
  source:
    persistentVolumeClaimName: ${PVC_NAME}
EOF

    echo "Attente de la cr√©ation..."
    kubectl wait --for=condition=ready volumesnapshot/${SNAPSHOT_NAME} \
      -n ${NAMESPACE} --timeout=300s
    echo "Snapshot cr√©√©: ${SNAPSHOT_NAME}"
    ;;

  list)
    echo "=== Snapshots dans ${NAMESPACE} ==="
    kubectl get volumesnapshot -n ${NAMESPACE}
    ;;

  restore)
    SNAPSHOT_NAME=$PVC_NAME  # Dans ce cas, c'est le nom du snapshot
    NEW_PVC="${SNAPSHOT_NAME}-restored"
    echo "Restauration depuis ${SNAPSHOT_NAME} vers ${NEW_PVC}"

    kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ${NEW_PVC}
  namespace: ${NAMESPACE}
spec:
  dataSource:
    name: ${SNAPSHOT_NAME}
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: openebs-cstor-csi
EOF

    echo "PVC restaur√©: ${NEW_PVC}"
    ;;

  delete)
    echo "Suppression du snapshot: ${PVC_NAME}"
    kubectl delete volumesnapshot ${PVC_NAME} -n ${NAMESPACE}
    ;;

  cleanup)
    DAYS=${PVC_NAME:-7}
    echo "Suppression des snapshots de plus de ${DAYS} jours..."

    CUTOFF=$(date -d "${DAYS} days ago" +%s)
    kubectl get volumesnapshot -n ${NAMESPACE} -o json | \
    jq -r ".items[] |
      select(.metadata.creationTimestamp | fromdateiso8601 < ${CUTOFF}) |
      .metadata.name" | \
    while read snap; do
      echo "Suppression: $snap"
      kubectl delete volumesnapshot $snap -n ${NAMESPACE}
    done
    ;;

  *)
    echo "Usage: $0 {create|list|restore|delete|cleanup} PVC_NAME [NAMESPACE] [SNAPSHOT_CLASS]"
    echo ""
    echo "Actions:"
    echo "  create  - Cr√©er un snapshot d'un PVC"
    echo "  list    - Lister les snapshots"
    echo "  restore - Restaurer un PVC depuis un snapshot"
    echo "  delete  - Supprimer un snapshot"
    echo "  cleanup - Supprimer les snapshots de plus de N jours"
    exit 1
    ;;
esac
```

Utilisation :

```bash
# Cr√©er un snapshot
./snapshot-manager.sh create my-pvc production

# Lister les snapshots
./snapshot-manager.sh list production

# Restaurer
./snapshot-manager.sh restore my-pvc-20250115-120000 production

# Nettoyer (snapshots > 7 jours)
./snapshot-manager.sh cleanup 7 production
```

## Conclusion

Les snapshots de volumes sont un outil puissant pour la protection des donn√©es dans Kubernetes, mais leur utilisation avec MicroK8s n√©cessite une r√©flexion sur vos besoins r√©els.

### Points cl√©s √† retenir

‚úÖ **Snapshots natifs** : Rapides et efficaces, mais n√©cessitent un stockage compatible
‚úÖ **Restic/Kopia** : Universel et simple, id√©al pour MicroK8s avec hostpath
‚úÖ **OpenEBS/Longhorn** : Solutions pour avoir des snapshots natifs sur MicroK8s
‚úÖ **Coh√©rence** : Utilisez des hooks pour les bases de donn√©es
‚úÖ **Automatisation** : CronJobs ou Velero Schedules pour r√©gularit√©
‚úÖ **Clonage** : Parfait pour cr√©er des environnements de test
‚úÖ **Monitoring** : Surveillez l'√©tat et l'espace disque

### Choix recommand√© selon le contexte

**Pour d√©buter et labs simples** :
```
‚Üí Velero + Restic/Kopia
```

**Pour apprendre les snapshots Kubernetes** :
```
‚Üí OpenEBS + VolumeSnapshots CRDs
```

**Pour une production MicroK8s** :
```
‚Üí Longhorn (multi-node) ou OpenEBS (single-node)
```

### Prochaines √©tapes

Dans la section suivante (22.5), nous verrons comment mettre en place des tests de restauration complets et √©laborer un v√©ritable plan de reprise d'activit√© (DRP) pour votre cluster MicroK8s.

---

**Checklist snapshots de volumes** :

- [ ] Comprendre la diff√©rence snapshots vs backups par fichier
- [ ] √âvaluer si le stockage supporte les snapshots natifs
- [ ] D√©cider : Restic/Kopia ou snapshots natifs ?
- [ ] Installer les composants n√©cessaires (OpenEBS/CRDs)
- [ ] Cr√©er une VolumeSnapshotClass
- [ ] Tester la cr√©ation d'un snapshot
- [ ] Tester la restauration depuis un snapshot
- [ ] Automatiser avec CronJobs ou Schedules
- [ ] Impl√©menter des hooks pour bases de donn√©es
- [ ] Mettre en place le monitoring
- [ ] Documenter la proc√©dure

‚è≠Ô∏è [Export/Import de configurations](/22-sauvegarde-et-restauration/05-export-import-de-configurations.md)
